"""
ë²•ì œì²˜ ë²•ë ¹ ìˆ˜ì§‘ê¸° - ë‹¤ì–‘í•œ ë²•ë¥  ë°ì´í„° ì§€ì› ë²„ì „ (v7.0)
- ìì¹˜ë²•ê·œ, íŒë¡€, í—Œì¬ê²°ì •ë¡€, ë²•ë ¹í•´ì„ë¡€, í–‰ì •ì‹¬íŒë¡€, ì¡°ì•½ ê²€ìƒ‰ ì§€ì›
- PDF ë‹¤ìš´ë¡œë“œ ë¡œì§ ì œê±°, OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ê¸°ëŠ¥ ì¶”ê°€
- ì´ˆê¸°í™” ì‹œ ê¸°ê´€ì½”ë“œ/APIí‚¤ ìœ ì§€
- ë³„í‘œ/ë³„ì²¨ í…ìŠ¤íŠ¸ ë‚´ìš© ìë™ ìˆ˜ì§‘
"""

import streamlit as st
import requests
import xml.etree.ElementTree as ET
import json
import time
import re
import os
from datetime import datetime
from io import BytesIO
import zipfile
import pandas as pd
import PyPDF2
import pdfplumber
from typing import List, Set, Dict, Optional, Tuple, Any, cast
from dataclasses import dataclass
from functools import lru_cache
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import base64
import urllib.parse
from collections import defaultdict, deque

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ“š ë²•ì œì²˜ ë²•ë ¹ ìˆ˜ì§‘ê¸°",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ===== ì„¤ì • í´ë˜ìŠ¤ =====
@dataclass
class APIConfig:
    """API ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    # ë²•ì œì²˜ API ì—”ë“œí¬ì¸íŠ¸
    LAW_SEARCH_URL = "https://www.law.go.kr/DRF/lawSearch.do"  # ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰
    LAW_DETAIL_URL = "https://www.law.go.kr/DRF/lawService.do"  # ë²•ë ¹ ìƒì„¸
    ADMIN_RULE_SEARCH_URL = "https://www.law.go.kr/DRF/lawSearch.do"  # í–‰ì •ê·œì¹™ ê²€ìƒ‰
    ADMIN_RULE_DETAIL_URL = "https://www.law.go.kr/DRF/lawService.do"  # í–‰ì •ê·œì¹™ë„ ë™ì¼ ì„œë¹„ìŠ¤ ì‚¬ìš©
    
    # API ì„¤ì •
    DEFAULT_DELAY = 0.3  # API í˜¸ì¶œ ê°„ê²© (ì´ˆ)
    MAX_RETRIES = 3      # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    TIMEOUT = 30         # íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    MAX_CONCURRENT = 5   # ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜
    
    # í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜
    RESULTS_PER_PAGE = 100


class LawPatterns:
    """ë²•ë ¹ëª… ì¶”ì¶œ íŒ¨í„´ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ - ê°œì„ ëœ ë²„ì „"""
    
    # ì œì™¸ í‚¤ì›Œë“œ (ìˆ˜ì •: í‚¤ì›Œë“œë§Œ ë‚¨ê¹€)
    EXCLUDE_KEYWORDS = {
        'ìƒí•˜ìœ„ë²•', 'ê´€ë ¨ë²•ë ¹', 'ìƒìœ„ë²•', 'í•˜ìœ„ë²•', 'ì„ íƒëœ'
    }
    
    # ë²•ë ¹ íƒ€ì…
    LAW_TYPES = {
        'ë²•', 'ë²•ë¥ ', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™', 'ê·œì •', 'ê·œì¹™', 'ì„¸ì¹™', 
        'ê³ ì‹œ', 'í›ˆë ¹', 'ì˜ˆê·œ', 'ì§€ì¹¨', 'ë¶„ë¥˜', 'ì—…ë¬´ê·œì •', 'ê°ë…ê·œì •'
    }
    
    # í–‰ì •ê·œì¹™ í‚¤ì›Œë“œ
    ADMIN_KEYWORDS = {
        'ê·œì •', 'ê³ ì‹œ', 'í›ˆë ¹', 'ì˜ˆê·œ', 'ì§€ì¹¨', 'ì„¸ì¹™', 'ê¸°ì¤€', 'ìš”ë ¹', 'ì§€ì‹œ'
    }
    
    # ì œê±°í•  ì ‘ë‘ì–´ íŒ¨í„´
    PREFIX_PATTERNS = [
        r'^í–‰ì •ê·œì¹™\s*',
        r'^ë²•ë ¹\s*',
        r'^\d{8}\s*',  # ë‚ ì§œ í˜•ì‹ (20250422 ê°™ì€)
        r'^\d+\.\s*',  # ë²ˆí˜¸ í˜•ì‹ (1. 2. ê°™ì€)
    ]
    
    # ë²•ë ¹ëª… íŒ¨í„´ (ì •ê·œí‘œí˜„ì‹) - ê°œì„ ëœ ë²„ì „
    LAW_PATTERNS = [
        # ì‹œí–‰ ë‚ ì§œ í¬í•¨ íŒ¨í„´
        r'([ê°€-í£]+(?:\s+[ê°€-í£]+)*(?:ë²•|ë²•ë¥ |ê·œì •|ê·œì¹™|ì„¸ì¹™|ë¶„ë¥˜))\s*\[ì‹œí–‰\s*\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.\]',
        # ë…ë¦½ì ì¸ ê·œì •/ì„¸ì¹™
        r'^([ê°€-í£]+(?:(?:\s+ë°\s+)|(?:\s+))?[ê°€-í£]*(?:ì—\s*ê´€í•œ\s*)?(?:ê·œì •|ì—…ë¬´ê·œì •|ê°ë…ê·œì •|ìš´ì˜ê·œì •|ê´€ë¦¬ê·œì •))(?:\s|$)',
        # ì‹œí–‰ì„¸ì¹™
        r'^([ê°€-í£]+(?:(?:\s+ë°\s+)|(?:\s+))?[ê°€-í£]*(?:ì—…ë¬´)?ì‹œí–‰ì„¸ì¹™)(?:\s|$)',
        # ë¶™ì–´ìˆëŠ” í˜•íƒœ
        r'([ê°€-í£]+(?:ê²€ì‚¬ë°ì œì¬ì—ê´€í•œ|ì—ê´€í•œ)?ê·œì •)(?:\s|$)',
        # ì¼ë°˜ ë²•ë¥ 
        r'^([ê°€-í£]+(?:\s+[ê°€-í£]+)*(?:ì—\s*ê´€í•œ\s*)?(?:íŠ¹ë³„|ê¸°ë³¸|ê´€ë¦¬|ì´‰ì§„|ì§€ì›|ìœ¡ì„±|ì§„í¥|ë³´í˜¸|ê·œì œ|ë°©ì§€)?ë²•(?:ë¥ )?)(?:\s|$)',
        # ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™
        r'^([ê°€-í£]+(?:\s+[ê°€-í£]+)*ë²•(?:ë¥ )?)\s+ì‹œí–‰ë ¹(?:\s|$)',
        r'^([ê°€-í£]+(?:\s+[ê°€-í£]+)*ë²•(?:ë¥ )?)\s+ì‹œí–‰ê·œì¹™(?:\s|$)',
        # ê³ ì‹œ/í›ˆë ¹ íŒ¨í„´ ì¶”ê°€
        r'([ê°€-í£]+(?:\s+[ê°€-í£]+)*(?:ê³ ì‹œ|í›ˆë ¹|ì˜ˆê·œ|ì§€ì¹¨))(?:\s|$)',
        # ë¶„ë¥˜ íŒ¨í„´ ì¶”ê°€
        r'([ê°€-í£]+(?:\s+)?ë¶„ë¥˜)(?:\s|$)',
    ]


# ===== íŒŒì¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ í´ë˜ìŠ¤ =====
class EnhancedLawFileExtractor:
    """ê°œì„ ëœ ë²•ë ¹ëª… ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self, use_ai: bool = False, api_key: Optional[str] = None):
        self.patterns = LawPatterns()
        self.use_ai = use_ai
        self.api_key = api_key
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def extract_from_file(self, file, file_type: str) -> List[str]:
        """íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ì¶”ì¶œ ë©”ì„œë“œ ë””ìŠ¤íŒ¨ì¹˜"""
        extractors = {
            'pdf': self._extract_from_pdf,
            'xlsx': self._extract_from_excel,
            'xls': self._extract_from_excel,
            'md': self._extract_from_markdown,
            'txt': self._extract_from_text
        }
        
        extractor = extractors.get(file_type.lower())
        if not extractor:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}")
            
        return extractor(file)
    
    def _extract_from_pdf(self, file) -> List[str]:
        """PDF íŒŒì¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        try:
            text = self._read_pdf_content(file)
            laws = self._extract_laws_from_text(text)
            
            if self.use_ai and self.api_key:
                laws = self._enhance_with_ai(text, laws)
                
            return sorted(list(laws))
            
        except Exception as e:
            self.logger.error(f"PDF ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            st.error(f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return []
    
    def _read_pdf_content(self, file) -> str:
        """PDF ë‚´ìš© ì½ê¸° - pdfplumber ìš°ì„ , ì‹¤íŒ¨ì‹œ PyPDF2"""
        text = ""
        
        # pdfplumber ì‹œë„
        try:
            with pdfplumber.open(file) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            return text
        except Exception as e:
            self.logger.warning(f"pdfplumber ì‹¤íŒ¨: {e}")
        
        # PyPDF2 í´ë°±
        try:
            file.seek(0)
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            self.logger.error(f"PyPDF2ë„ ì‹¤íŒ¨: {e}")
            raise
    
    def _extract_laws_from_text(self, text: str) -> Set[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""
        laws = set()
        
        # í…ìŠ¤íŠ¸ ì •ê·œí™”
        text = self._normalize_text(text)
        
        # ì œì™¸ í‚¤ì›Œë“œë¡œ í…ìŠ¤íŠ¸ ë¶„í•  ì²˜ë¦¬
        # 'ì—¬ì‹ ì „ë¬¸ê¸ˆìœµì—…ë²• ìƒí•˜ìœ„ë²• ì—¬ì‹ ì „ë¬¸ê¸ˆìœµì—…ë²•' -> ë¶„ë¦¬
        for exclude_keyword in self.patterns.EXCLUDE_KEYWORDS:
            text = text.replace(exclude_keyword, '\n')
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë²•ë ¹ëª… ì¶”ì¶œ
        for pattern in self.patterns.LAW_PATTERNS:
            matches = re.findall(pattern, text, re.MULTILINE | re.IGNORECASE)
            for match in matches:
                law_name = self._clean_law_name(match)
                if self._validate_law_name(law_name):
                    laws.add(law_name)
        
        # ë¼ì¸ë³„ ì¶”ê°€ ì²˜ë¦¬
        for line in text.split('\n'):
            line = line.strip()
            
            # ì œì™¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¼ì¸ì€ ë¶„í•  ì²˜ë¦¬
            contains_exclude = False
            for exclude_keyword in self.patterns.EXCLUDE_KEYWORDS:
                if exclude_keyword in line:
                    # ì œì™¸ í‚¤ì›Œë“œ ì•ë’¤ë¡œ ë¶„í• 
                    parts = line.split(exclude_keyword)
                    for part in parts:
                        part = part.strip()
                        if part and part not in self.patterns.EXCLUDE_KEYWORDS:
                            # ê° ë¶€ë¶„ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ
                            for law_type in self.patterns.LAW_TYPES:
                                if law_type in part:
                                    law_name = self._extract_law_name_from_line(part, law_type)
                                    if law_name and self._validate_law_name(law_name):
                                        laws.add(law_name)
                    contains_exclude = True
                    break
            
            if contains_exclude:
                continue
                
            # ì ‘ë‘ì–´ ì œê±°
            for prefix_pattern in self.patterns.PREFIX_PATTERNS:
                line = re.sub(prefix_pattern, '', line)
            
            if line in self.patterns.EXCLUDE_KEYWORDS:
                continue
            
            # ë²•ë ¹ íƒ€ì…ë³„ ë§¤ì¹­
            for law_type in self.patterns.LAW_TYPES:
                if law_type in line:
                    law_name = self._extract_law_name_from_line(line, law_type)
                    if law_name and self._validate_law_name(law_name):
                        laws.add(law_name)
        
        # í›„ì²˜ë¦¬
        laws = self._post_process_laws(laws)
        
        return laws
    
    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        # ì—°ì† ê³µë°± ì œê±°
        text = ' '.join(text.split())
        
        # í‘œì¤€í™”
        replacements = {
            'ì—ê´€í•œ': 'ì— ê´€í•œ',
            'ë°': ' ë° ',
            'Â·': 'Â·',  # ì¤‘ì  í†µì¼
            'ï¼Œ': ',',  # ì‰¼í‘œ í†µì¼
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
            
        return text
    
    def _clean_law_name(self, law_name: str) -> str:
        """ë²•ë ¹ëª… ì •ì œ"""
        if not isinstance(law_name, str):
            law_name = str(law_name)
        
        # ì‹œí–‰ ì •ë³´ ì œê±°
        law_name = re.sub(r'\s*\[ì‹œí–‰[^\]]+\]', '', law_name)
        
        # ì ‘ë‘ì–´ ì œê±°
        for prefix_pattern in self.patterns.PREFIX_PATTERNS:
            law_name = re.sub(prefix_pattern, '', law_name)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        law_name = law_name.strip()
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        law_name = ' '.join(law_name.split())
        
        # ë¶™ì–´ìˆëŠ” í˜•íƒœ ì •ê·œí™”
        law_name = re.sub(r'ê²€ì‚¬ë°', 'ê²€ì‚¬ ë° ', law_name)
        law_name = re.sub(r'ì—ê´€í•œ', 'ì— ê´€í•œ ', law_name)
        
        return law_name
    
    def _extract_law_name_from_line(self, line: str, law_type: str) -> Optional[str]:
        """ë¼ì¸ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        # ë²•ë ¹ íƒ€ì… ìœ„ì¹˜ ì°¾ê¸°
        type_pos = line.find(law_type)
        if type_pos == -1:
            return None
            
        # ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° (í•œê¸€ë¡œ ì‹œì‘)
        start = 0
        for i in range(type_pos - 1, -1, -1):
            if not (line[i].isalnum() or line[i] in ' Â·ë°ê´€í•œì˜ì—'):
                start = i + 1
                break
        
        # ë ìœ„ì¹˜ëŠ” ë²•ë ¹ íƒ€ì… ë’¤
        end = type_pos + len(law_type)
        
        # ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ ì²˜ë¦¬
        if end < len(line) - 3:
            next_chars = line[end:end+4]
            if 'ì‹œí–‰ë ¹' in next_chars or 'ì‹œí–‰ê·œì¹™' in next_chars:
                space_pos = line.find(' ', end)
                if space_pos != -1:
                    end = space_pos
                else:
                    end = len(line)
        
        return line[start:end].strip()
    
    def _validate_law_name(self, law_name: str) -> bool:
        """ë²•ë ¹ëª… ìœ íš¨ì„± ê²€ì¦"""
        # ê¸¸ì´ ì²´í¬
        if len(law_name) < 3 or len(law_name) > 100:
            return False
            
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        if law_name in self.patterns.EXCLUDE_KEYWORDS:
            return False
            
        # í•œê¸€ í¬í•¨ ì²´í¬
        if not re.search(r'[ê°€-í£]', law_name):
            return False
            
        # ë²•ë ¹ íƒ€ì… í¬í•¨ ì²´í¬
        if not any(law_type in law_name for law_type in self.patterns.LAW_TYPES):
            return False
            
        # ì ‘ë‘ì–´ê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš° ì œê±°
        if any(pattern in law_name for pattern in ['í–‰ì •ê·œì¹™', 'ë²•ë ¹']):
            return False
            
        return True
    
    def _post_process_laws(self, laws: Set[str]) -> Set[str]:
        """ë²•ë ¹ëª… í›„ì²˜ë¦¬ - ì¤‘ë³µ ì œê±°, ì •ê·œí™”"""
        processed = set()
        
        # ì •ë ¬í•˜ì—¬ ê¸´ ê²ƒë¶€í„° ì²˜ë¦¬
        sorted_laws = sorted(laws, key=len, reverse=True)
        
        for law in sorted_laws:
            # ë¶€ë¶„ ë¬¸ìì—´ ì²´í¬
            is_substring = False
            for existing in processed:
                if law in existing and law != existing:
                    is_substring = True
                    break
                    
            if not is_substring:
                # ìµœì¢… ì •ê·œí™”
                law = law.strip()
                law = ' '.join(law.split())  # ì—°ì† ê³µë°± ì œê±°
                
                # ì ‘ë‘ì–´ ìµœì¢… ì œê±°
                for prefix_pattern in self.patterns.PREFIX_PATTERNS:
                    law = re.sub(prefix_pattern, '', law)
                
                processed.add(law)
                
        return processed
    
    def _enhance_with_ai(self, text: str, laws: Set[str]) -> Set[str]:
        """AIë¥¼ í™œìš©í•œ ë²•ë ¹ëª… ì¶”ì¶œ ê°œì„  - ê°•í™”ëœ ë²„ì „"""
        try:
            # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬
            try:
                from openai import OpenAI
            except ImportError:
                self.logger.warning("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return laws
            
            # API í‚¤ ìœ íš¨ì„± ê²€ì¦ ê°œì„ 
            if not self.api_key:
                self.logger.warning("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return laws
            
            # API í‚¤ ì •ë¦¬ (ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ì í™•ì¸)
            cleaned_key = self.api_key.strip()
            
            # API í‚¤ í˜•ì‹ ê²€ì¦
            if not (cleaned_key.startswith('sk-') or cleaned_key.startswith('sess-')):
                self.logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ í˜•ì‹")
                return laws
            
            self.logger.info(f"OpenAI API í‚¤ ì‚¬ìš© ì¤‘: {cleaned_key[:10]}...")
            
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± (í‚¤ ì¬ì„¤ì •)
            client = OpenAI(
                api_key=cleaned_key,
                max_retries=2,
                timeout=30.0
            )
            
            # API í‚¤ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ í˜¸ì¶œ
            try:
                # í…ìŠ¤íŠ¸ ìƒ˜í”Œë§ (í† í° ì œí•œ)
                sample = text[:3000]
                
                # í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ê°•í™”ëœ ë²„ì „
                prompt = self._create_enhanced_ai_prompt(sample, laws, text)
                
                # API í˜¸ì¶œ - ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ
                try:
                    response = client.chat.completions.create(
                        model="gpt-5",
                        messages=[
                            {"role": "system", "content": "í•œêµ­ ë²•ë ¹ ì „ë¬¸ê°€. ë²•ë ¹ì²´ê³„ë„ì—ì„œ ë²•ë ¹ëª…ì„ ì •í™•íˆ ì¶”ì¶œí•˜ê³ , íŠ¹ìˆ˜ë¬¸ì ë³€í™˜ê³¼ ì‚¬ìš©ì ì˜ë„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤."},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.1,
                        max_tokens=1500
                    )

                    # ì‘ë‹µ íŒŒì‹±
                    ai_laws = self._parse_ai_response_enhanced(response.choices[0].message.content)

                    self.logger.info(f"AIê°€ ì¶”ê°€ë¡œ {len(ai_laws - laws)}ê°œì˜ ë²•ë ¹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

                    # ê²°ê³¼ ë³‘í•©
                    return laws.union(ai_laws)

                except Exception as chat_error:
                    # gpt-5 ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
                    try:
                        response = client.chat.completions.create(
                            model="gpt-5",
                            messages=[
                                {"role": "system", "content": "í•œêµ­ ë²•ë ¹ ì „ë¬¸ê°€. ë²•ë ¹ì²´ê³„ë„ì—ì„œ ë²•ë ¹ëª…ì„ ì •í™•íˆ ì¶”ì¶œí•˜ê³ , íŠ¹ìˆ˜ë¬¸ì ë³€í™˜ê³¼ ì‚¬ìš©ì ì˜ë„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤."},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.1,
                            max_tokens=1500
                        )

                        ai_laws = self._parse_ai_response_enhanced(response.choices[0].message.content)
                        self.logger.info(f"GPT-5ë¡œ {len(ai_laws - laws)}ê°œì˜ ë²•ë ¹ì„ ì¶”ê°€ë¡œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                        return laws.union(ai_laws)

                    except:
                        self.logger.warning("AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¶”ì¶œë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                        return laws
                
            except Exception as api_error:
                error_msg = str(api_error)
                if "401" in error_msg or "Incorrect API key" in error_msg:
                    self.logger.error("API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    st.error("âš ï¸ OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ í‚¤ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                elif "insufficient_quota" in error_msg:
                    self.logger.warning("API ì‚¬ìš©ëŸ‰ í•œë„ ì´ˆê³¼")
                    st.warning("âš ï¸ OpenAI API ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                else:
                    self.logger.error(f"OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {error_msg}")
                return laws
            
        except Exception as e:
            self.logger.error(f"AI ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return laws
    
    def _create_enhanced_ai_prompt(self, sample: str, existing_laws: Set[str], full_text: str) -> str:
        """ê°•í™”ëœ AI í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        doc_structure = self._analyze_document_structure(full_text)
        
        return f"""ë‹¹ì‹ ì€ í•œêµ­ ë²•ë ¹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë²•ë ¹ì²´ê³„ë„ ë¬¸ì„œì—ì„œ ë²•ë ¹ëª…ì„ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.

ì¤‘ìš” ê·œì¹™:
1. ë²•ì œì²˜ ê³µì‹ ëª…ì¹­ ì‚¬ìš©
2. íŠ¹ìˆ˜ë¬¸ì ë³€í™˜: * â†’ Â·, ï¼Š â†’ Â·
3. "ìƒí•˜ìœ„ë²•", "ê´€ë ¨ë²•ë ¹", "í–‰ì •ê·œì¹™", "ë²•ë ¹" ê°™ì€ ì¹´í…Œê³ ë¦¬ ì œëª©ì€ ì œì™¸
4. ë‚ ì§œ(ì˜ˆ: 20250422, [ì‹œí–‰ 2022.12.11.]) ì œì™¸
5. ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ì€ ë…ë¦½ëœ ë²•ë ¹ìœ¼ë¡œ ì¶”ì¶œ
6. ë¬¸ì„œì— ìˆëŠ” ëª¨ë“  ë²•ë ¹ëª…ì„ ë¹ ì§ì—†ì´ ì¶”ì¶œ

ë¬¸ì„œ êµ¬ì¡° ì •ë³´:
{doc_structure}

í…ìŠ¤íŠ¸ ìƒ˜í”Œ:
{sample}

í˜„ì¬ê¹Œì§€ ì¶”ì¶œëœ ë²•ë ¹ (ì°¸ê³ ):
{', '.join(list(existing_laws)[:10])}

ë‹¤ìŒê³¼ ê°™ì€ ë²•ë ¹ë“¤ì„ íŠ¹íˆ ì£¼ì˜í•´ì„œ ì°¾ìœ¼ì„¸ìš”:
- í–‰ì •ê·œì¹™ (ê·œì •, í›ˆë ¹, ì˜ˆê·œ, ì§€ì¹¨, ì„¸ì¹™ ë“±)
- íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ë²•ë ¹ëª… (ì˜ˆ: ì‹¬ì˜Â·ì§•ê³„ìœ„ì›íšŒ)
- ê¸´ ë²•ë ¹ëª… (ì˜ˆ: ê·¼ë¡œê¸°ì¤€ë²• ë° ê³µì¸ë…¸ë¬´ì‚¬ë²•ì— ë”°ë¥¸ ê³¼íƒœë£Œì˜ ê°€ì¤‘ì²˜ë¶„ì— ê´€í•œ ì„¸ë¶€ ì§€ì¹¨)

ë²•ë ¹ëª…ë§Œ í•œ ì¤„ì— í•˜ë‚˜ì”© ì¶œë ¥í•˜ì„¸ìš”:"""
    
    def _analyze_document_structure(self, text: str) -> str:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„"""
        lines = text.split('\n')
        structure_info = []
        
        # ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ
        category_keywords = ['ìƒí•˜ìœ„ë²•', 'ê´€ë ¨ë²•ë ¹', 'í–‰ì •ê·œì¹™', 'ë²•ë ¹']
        
        current_category = None
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ì¹´í…Œê³ ë¦¬ ê°ì§€
            for keyword in category_keywords:
                if keyword in line and len(line) < 20:  # ì§§ì€ ë¼ì¸ì—ì„œë§Œ
                    current_category = keyword
                    structure_info.append(f"[{keyword} ì„¹ì…˜ ì‹œì‘]")
                    break
            
            # ë‚ ì§œ íŒ¨í„´ ê°ì§€
            if re.search(r'\[ì‹œí–‰\s*\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.\]', line):
                structure_info.append(f"ë‚ ì§œê°€ í¬í•¨ëœ ë²•ë ¹ ë°œê²¬: {line[:50]}...")
        
        return '\n'.join(structure_info[:10])  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ
    
    def _parse_ai_response_enhanced(self, response: str) -> Set[str]:
        """ê°•í™”ëœ AI ì‘ë‹µ íŒŒì‹±"""
        laws = set()
        
        for line in response.strip().split('\n'):
            line = line.strip()
            
            # ë²ˆí˜¸, ê¸°í˜¸ ì œê±°
            line = re.sub(r'^[\d\-\.\*\â€¢\Â·]+\s*', '', line)
            line = line.strip('"\'')
            
            # ì ‘ë‘ì–´ ì œê±°
            for prefix_pattern in self.patterns.PREFIX_PATTERNS:
                line = re.sub(prefix_pattern, '', line)
            
            # íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
            line = self._normalize_law_name_for_ai(line)
            
            if line and self._validate_law_name(line):
                laws.add(line)
                self.logger.debug(f"AI ì¶”ì¶œ: {line}")
                
        return laws
    
    def _normalize_law_name_for_ai(self, law_name: str) -> str:
        """AI ì‘ë‹µì—ì„œ ë²•ë ¹ëª… ì •ê·œí™”"""
        # íŠ¹ìˆ˜ë¬¸ì ë³€í™˜
        replacements = {
            '*': 'Â·',
            'ï¼Š': 'Â·',
            'â€¤': 'Â·',
            'ï½¥': 'Â·',
            'ãƒ»': 'Â·',
            'ï¼Œ': ',',
            'ï¼': '.',
            'ï¼ˆ': '(',
            'ï¼‰': ')',
        }
        
        for old, new in replacements.items():
            law_name = law_name.replace(old, new)
        
        # ì—°ì† ê³µë°± ì œê±°
        law_name = ' '.join(law_name.split())
        
        return law_name.strip()
    
    def _extract_from_excel(self, file) -> List[str]:
        """Excel íŒŒì¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        laws = set()
        
        try:
            excel_file = pd.ExcelFile(file)
            
            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(file, sheet_name=sheet_name)
                
                # ëª¨ë“  ì…€ì˜ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                text = self._collect_excel_text(df)
                
                # ë²•ë ¹ëª… ì¶”ì¶œ
                sheet_laws = self._extract_laws_from_text(text)
                laws.update(sheet_laws)
                
        except Exception as e:
            self.logger.error(f"Excel ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            st.error(f"Excel íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
        return sorted(list(laws))
    
    def _collect_excel_text(self, df: pd.DataFrame) -> str:
        """DataFrameì—ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        texts = []
        
        for column in df.columns:
            for value in df[column].dropna():
                if isinstance(value, str):
                    texts.append(value)
                    
        return '\n'.join(texts)
    
    def _extract_from_markdown(self, file) -> List[str]:
        """Markdown íŒŒì¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        try:
            content = file.read().decode('utf-8')
            laws = self._extract_laws_from_text(content)
            return sorted(list(laws))
        except Exception as e:
            self.logger.error(f"Markdown ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_from_text(self, file) -> List[str]:
        """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        try:
            content = file.read().decode('utf-8')
            laws = self._extract_laws_from_text(content)
            return sorted(list(laws))
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []


# ===== ë²•ë ¹ ìˆ˜ì§‘ API í´ë˜ìŠ¤ =====
class LawCollectorAPI:
    """ê°œì„ ëœ ë²•ë ¹ ìˆ˜ì§‘ API í´ë˜ìŠ¤ - ì •í™•í•œ ê²€ìƒ‰ ëª¨ë“œ ì¶”ê°€"""
    
    def __init__(self, oc_code: str):
        self.oc_code = oc_code
        self.config = APIConfig()
        self.logger = logging.getLogger(self.__class__.__name__)
        self.session = self._create_session()
        self._cache = {}  # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ
        self.patterns = LawPatterns()
        
    @lru_cache(maxsize=128)
    def _get_cached_search_result(self, law_name: str) -> Optional[str]:
        """ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜"""
        return None  # ì‹¤ì œ êµ¬í˜„ì‹œ ìºì‹œ ë¡œì§ ì¶”ê°€
        
    def _create_session(self) -> requests.Session:
        """ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ì…˜ ìƒì„±"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # SSL ì¸ì¦ì„œ ê²€ì¦ í™œì„±í™” (ë³´ì•ˆ ê°•í™”)
        session.verify = True
        
        # ì¬ì‹œë„ ì„¤ì •
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        retry_strategy = Retry(
            total=self.config.MAX_RETRIES,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session
    
    def search_laws(self, law_names: List[str], 
                   progress_callback=None, 
                   use_variations: bool = True) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ë²•ë ¹ì„ ë³‘ë ¬ë¡œ ê²€ìƒ‰ - ì¤‘ë³µ ì œê±° ì¶”ê°€
        
        Args:
            law_names: ê²€ìƒ‰í•  ë²•ë ¹ëª… ë¦¬ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
            use_variations: ë²•ë ¹ëª… ë³€í˜• ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
                           - True: ë„ì–´ì“°ê¸° ë“± ë³€í˜•í•˜ì—¬ ê²€ìƒ‰ (ì§ì ‘ ê²€ìƒ‰ ëª¨ë“œ)
                           - False: ì •í™•í•œ ë²•ë ¹ëª…ìœ¼ë¡œë§Œ ê²€ìƒ‰ (ë²•ë ¹ì²´ê³„ë„ ëª¨ë“œ)
        """
        results = []
        no_result_laws = []
        seen_law_ids = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set
        
        with ThreadPoolExecutor(max_workers=self.config.MAX_CONCURRENT) as executor:
            # ê²€ìƒ‰ ì‘ì—… ì œì¶œ
            if use_variations:
                # ë³€í˜• ê²€ìƒ‰ ì‚¬ìš© (ì§ì ‘ ê²€ìƒ‰ ëª¨ë“œ)
                future_to_law = {
                    executor.submit(self._search_with_variations, law_name): law_name
                    for law_name in law_names
                }
            else:
                # ì •í™•í•œ ê²€ìƒ‰ë§Œ ì‚¬ìš© (ë²•ë ¹ì²´ê³„ë„ ëª¨ë“œ)
                future_to_law = {
                    executor.submit(self._search_exact_match, law_name): law_name
                    for law_name in law_names
                }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for idx, future in enumerate(as_completed(future_to_law)):
                law_name = future_to_law[future]
                
                try:
                    result = future.result()
                    if result:
                        # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        if not isinstance(result, list):
                            result = [result] if result else []
                        
                        # ì¤‘ë³µ ì œê±°
                        for law in result:
                            if law['law_id'] not in seen_law_ids:
                                seen_law_ids.add(law['law_id'])
                                # ì›ë˜ ê²€ìƒ‰ì–´ ì €ì¥
                                law['search_query'] = law_name
                                results.append(law)
                    else:
                        no_result_laws.append(law_name)
                    
                    if progress_callback:
                        progress_callback((idx + 1) / len(law_names))
                        
                except Exception as e:
                    self.logger.error(f"{law_name} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    no_result_laws.append(law_name)
        
        # ê²€ìƒ‰ ì‹¤íŒ¨í•œ ë²•ë ¹ í‘œì‹œ
        if no_result_laws:
            with st.expander(f"âŒ ê²€ìƒ‰ë˜ì§€ ì•Šì€ ë²•ë ¹ ({len(no_result_laws)}ê°œ)"):
                for law in no_result_laws:
                    st.write(f"- {law}")
                
                # ëª¨ë“œì— ë”°ë¥¸ ë‹¤ë¥¸ ì•ˆë‚´ ë©”ì‹œì§€
                if use_variations:
                    st.info("ğŸ’¡ Tip: ê¸°ê´€ì½”ë“œë¥¼ í™•ì¸í•˜ê±°ë‚˜, ë²•ë ¹ëª…ì„ ìˆ˜ì •í•´ë³´ì„¸ìš”.")
                else:
                    st.info("ğŸ’¡ Tip: ë²•ë ¹ì²´ê³„ë„ì˜ ë²•ë ¹ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë²•ë ¹ë§Œ ê²€ìƒ‰ë©ë‹ˆë‹¤.")
                    
        return results
    
    def _search_exact_match(self, law_name: str) -> List[Dict[str, Any]]:
        """ê°œì„ ëœ ë§¤ì¹­ìœ¼ë¡œ ë²•ë ¹ ê²€ìƒ‰ - íŒŒì¼ ì—…ë¡œë“œ ëª¨ë“œìš©"""
        self.logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ê²€ìƒ‰ ëª¨ë“œ: {law_name}")
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
        normalized_name = self._normalize_law_name(law_name)
        
        # ê¸°ë³¸ ê²€ìƒ‰ + ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œë„ ê²€ìƒ‰
        all_results = []
        
        # 1. ì›ë³¸ ê·¸ëŒ€ë¡œ ê²€ìƒ‰
        results = self._search_single_law_exact(law_name)
        all_results.extend(results)
        
        # 2. ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰ (ë‹¤ë¥¸ ê²½ìš°ë§Œ)
        if normalized_name != law_name:
            normalized_results = self._search_single_law_exact(normalized_name)
            all_results.extend(normalized_results)
        
        # ì¤‘ë³µ ì œê±°
        seen_ids = set()
        unique_results = []
        
        for result in all_results:
            if result['law_id'] not in seen_ids:
                seen_ids.add(result['law_id'])
                unique_results.append(result)
        
        # ê²°ê³¼ í•„í„°ë§ - ìœ ì‚¬ë„ ê¸°ë°˜
        filtered_results = []
        for result in unique_results:
            similarity = self._calculate_similarity(law_name, result['law_name'])
            if similarity >= 0.85:  # 85% ì´ìƒ ìœ ì‚¬ë„
                filtered_results.append(result)
                self.logger.debug(f"ë§¤ì¹­ ì„±ê³µ (ìœ ì‚¬ë„ {similarity:.2f}): {result['law_name']}")
            else:
                self.logger.debug(f"ë§¤ì¹­ ì‹¤íŒ¨ (ìœ ì‚¬ë„ {similarity:.2f}): {result['law_name']} != {law_name}")
        
        return filtered_results
    
    def _normalize_law_name(self, law_name: str) -> str:
        """ë²•ë ¹ëª… ì •ê·œí™” - íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬"""
        normalized = law_name
        
        # íŠ¹ìˆ˜ë¬¸ì ë³€í™˜
        replacements = {
            '*': 'Â·',
            'ï¼Š': 'Â·',
            'â€¤': 'Â·',
            'ï½¥': 'Â·',
            'ãƒ»': 'Â·',
            'ï¼Œ': ',',
            'ï¼': '.',
            'ï¼ˆ': '(',
            'ï¼‰': ')',
            'ã€Œ': '',
            'ã€': '',
            'ã€': '',
            'ã€': '',
        }
        
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)
        
        # ì—°ì† ê³µë°± ì œê±°
        normalized = ' '.join(normalized.split())
        
        return normalized.strip()
    
    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)"""
        # ê°„ë‹¨í•œ ë¬¸ì ê¸°ë°˜ ìœ ì‚¬ë„
        str1 = self._normalize_law_name(str1.lower())
        str2 = self._normalize_law_name(str2.lower())
        
        if str1 == str2:
            return 1.0
        
        # ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
        longer = max(len(str1), len(str2))
        if longer == 0:
            return 1.0
        
        distance = self._levenshtein_distance(str1, str2)
        return (longer - distance) / longer
    
    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    def _search_single_law_exact(self, law_name: str) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ë²•ë ¹ ì •í™•í•œ ê²€ìƒ‰ - ì¼ë°˜ ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ ëª¨ë‘"""
        results = []
        
        # 1. ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰ (target=law)
        general_laws = self._search_general_law(law_name)
        results.extend(general_laws)
        
        # 2. í–‰ì •ê·œì¹™ ê²€ìƒ‰ (ë³„ë„ API)
        admin_rules = self._search_admin_rule(law_name)
        results.extend(admin_rules)
        
        # ì¤‘ë³µ ì œê±°
        unique_results = self._remove_duplicates(results)
        
        # ê²€ìƒ‰ ê²°ê³¼ ë¡œê·¸
        if unique_results:
            general_count = sum(1 for r in unique_results if not r.get('is_admin_rule'))
            admin_count = sum(1 for r in unique_results if r.get('is_admin_rule'))
            self.logger.info(f"âœ… ì •í™•í•œ ê²€ìƒ‰ ì™„ë£Œ: {law_name} - ì¼ë°˜ë²•ë ¹ {general_count}ê°œ, í–‰ì •ê·œì¹™ {admin_count}ê°œ")
        
        return unique_results
    
    def _search_with_variations(self, law_name: str) -> List[Dict[str, Any]]:
        """ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë²•ë ¹ ê²€ìƒ‰ - ê°œì„ ëœ ë²„ì „"""
        variations = self._generate_search_variations(law_name)
        all_results = []
        seen_law_ids = set()
        
        for idx, variation in enumerate(variations):
            self.logger.info(f"ê²€ìƒ‰ ë³€í˜• {idx+1}/{len(variations)}: {variation}")
            results = self.search_single_law(variation)
            
            if results:
                # ì–´ë–¤ ë³€í˜•ìœ¼ë¡œ ì°¾ì•˜ëŠ”ì§€ ê¸°ë¡
                for result in results:
                    # ì¤‘ë³µ ì œê±°
                    if result['law_id'] not in seen_law_ids:
                        seen_law_ids.add(result['law_id'])
                        result['found_with_variation'] = variation
                        result['variation_index'] = idx
                        result['search_query'] = law_name  # ì›ë˜ ê²€ìƒ‰ì–´ ë³´ì¡´
                        all_results.append(result)
        
        return all_results
    
    def _generate_search_variations(self, law_name: str) -> List[str]:
        """ë²•ë ¹ëª…ì˜ ë‹¤ì–‘í•œ ë³€í˜• ìƒì„±"""
        variations = [law_name]
        
        # ë„ì–´ì“°ê¸° ì¶”ê°€/ì œê±°
        spaced = law_name.replace('ë°', ' ë° ').replace('ì—ê´€í•œ', 'ì— ê´€í•œ')
        if spaced != law_name:
            variations.append(spaced)
        
        no_space = law_name.replace(' ', '')
        if no_space != law_name:
            variations.append(no_space)
        
        # ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ ë¶„ë¦¬
        if ' ì‹œí–‰ë ¹' in law_name:
            base = law_name.replace(' ì‹œí–‰ë ¹', '')
            variations.extend([base, f"{base}ì‹œí–‰ë ¹"])
        
        if ' ì‹œí–‰ê·œì¹™' in law_name:
            base = law_name.replace(' ì‹œí–‰ê·œì¹™', '')
            variations.extend([base, f"{base}ì‹œí–‰ê·œì¹™"])
        
        return variations[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ
    
    def search_single_law(self, law_name: str) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ë²•ë ¹ ê²€ìƒ‰ - ì¼ë°˜ ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ ëª¨ë‘"""
        results = []
        
        # 1. ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰ (target=law)
        general_laws = self._search_general_law(law_name)
        results.extend(general_laws)
        
        # 2. í–‰ì •ê·œì¹™ ê²€ìƒ‰ (ë³„ë„ API)
        admin_rules = self._search_admin_rule(law_name)
        results.extend(admin_rules)
        
        # ì¤‘ë³µ ì œê±°
        unique_results = self._remove_duplicates(results)
        
        # ê²€ìƒ‰ ê²°ê³¼ ë¡œê·¸
        if unique_results:
            general_count = sum(1 for r in unique_results if not r.get('is_admin_rule'))
            admin_count = sum(1 for r in unique_results if r.get('is_admin_rule'))
            self.logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {law_name} - ì¼ë°˜ë²•ë ¹ {general_count}ê°œ, í–‰ì •ê·œì¹™ {admin_count}ê°œ")
        
        return unique_results
    
    def _search_general_law(self, law_name: str) -> List[Dict[str, Any]]:
        """ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰"""
        params = {
            'OC': self.oc_code,
            'target': 'law',
            'type': 'XML',
            'query': law_name,
            'display': str(self.config.RESULTS_PER_PAGE),
            'page': '1'
        }
        
        try:
            self.logger.debug(f"ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰: {law_name}")
            
            response = self.session.get(
                self.config.LAW_SEARCH_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )
            
            if response.status_code != 200:
                self.logger.warning(f"ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰ ì‹¤íŒ¨: {law_name} - ìƒíƒœì½”ë“œ: {response.status_code}")
                return []
                
            # XML íŒŒì‹±
            laws = self._parse_law_search_response(response.text, law_name)
            
            if laws:
                self.logger.info(f"ì¼ë°˜ ë²•ë ¹ {len(laws)}ê°œ ë°œê²¬: {law_name}")
            
            return laws
            
        except Exception as e:
            self.logger.error(f"ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _search_admin_rule(self, law_name: str) -> List[Dict[str, Any]]:
        """í–‰ì •ê·œì¹™ ê²€ìƒ‰ - ì™„ì „ ì¬ì‘ì„±"""
        params = {
            'OC': self.oc_code,
            'target': 'admrul',
            'type': 'XML',
            'query': law_name,
            'display': '100',
            'page': '1'
        }
        
        try:
            self.logger.info(f"í–‰ì •ê·œì¹™ ê²€ìƒ‰ ì‹œì‘: {law_name}")
            self.logger.debug(f"API URL: {self.config.ADMIN_RULE_SEARCH_URL}")
            self.logger.debug(f"íŒŒë¼ë¯¸í„°: {params}")
            
            # í–‰ì •ê·œì¹™ ì „ìš© API ì‚¬ìš©
            response = self.session.get(
                self.config.ADMIN_RULE_SEARCH_URL,  # admRulSc.do
                params=params,
                timeout=self.config.TIMEOUT
            )
            
            self.logger.debug(f"ì‘ë‹µ ìƒíƒœì½”ë“œ: {response.status_code}")
            
            if response.status_code == 200:
                # í–‰ì •ê·œì¹™ ì „ìš© íŒŒì‹±
                rules = self._parse_admin_rule_search_response(response.text, law_name)
                
                if rules:
                    self.logger.info(f"âœ… í–‰ì •ê·œì¹™ {len(rules)}ê°œ ë°œê²¬: {law_name}")
                else:
                    self.logger.info(f"í–‰ì •ê·œì¹™ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {law_name}")
                
                return rules
            else:
                self.logger.warning(f"í–‰ì •ê·œì¹™ ê²€ìƒ‰ ì‹¤íŒ¨: {law_name} - ìƒíƒœì½”ë“œ: {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"í–‰ì •ê·œì¹™ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _parse_law_search_response(self, content: str, 
                                  search_query: str) -> List[Dict[str, Any]]:
        """ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹±"""
        laws = []
        
        try:
            # ì „ì²˜ë¦¬
            content = self._preprocess_xml_content(content)
            
            # XML íŒŒì‹±
            root = ET.fromstring(content.encode('utf-8'))
            
            for law_elem in root.findall('.//law'):
                law_info = {
                    'law_id': law_elem.findtext('ë²•ë ¹ID', ''),
                    'law_msn': law_elem.findtext('ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_name': law_elem.findtext('ë²•ë ¹ëª…í•œê¸€', ''),
                    'law_type': law_elem.findtext('ë²•ì¢…êµ¬ë¶„', ''),
                    'promulgation_date': law_elem.findtext('ê³µí¬ì¼ì', ''),
                    'enforcement_date': law_elem.findtext('ì‹œí–‰ì¼ì', ''),
                    'is_admin_rule': False,
                    'search_query': search_query
                }
                
                if law_info['law_id'] and law_info['law_name']:
                    laws.append(law_info)
                    
        except ET.ParseError as e:
            self.logger.error(f"ì¼ë°˜ ë²•ë ¹ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
            
        return laws
    
    def _parse_admin_rule_search_response(self, content: str, 
                                         search_query: str) -> List[Dict[str, Any]]:
        """í–‰ì •ê·œì¹™ ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹± - ì „ìš© íŒŒì„œ"""
        rules = []
        
        try:
            # ì „ì²˜ë¦¬
            content = self._preprocess_xml_content(content)
            
            # XML íŒŒì‹±
            root = ET.fromstring(content.encode('utf-8'))
            
            self.logger.debug(f"í–‰ì •ê·œì¹™ XML ë£¨íŠ¸ íƒœê·¸: {root.tag}")
            self.logger.debug(f"í•˜ìœ„ ìš”ì†Œ: {[child.tag for child in root][:5]}")
            
            # í–‰ì •ê·œì¹™ì€ admrul íƒœê·¸ ì‚¬ìš©
            for rule_elem in root.findall('.//admrul'):
                rule_info = {
                    'law_id': rule_elem.findtext('í–‰ì •ê·œì¹™ID', ''),
                    'law_msn': rule_elem.findtext('í–‰ì •ê·œì¹™ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_name': rule_elem.findtext('í–‰ì •ê·œì¹™ëª…', ''),
                    'law_type': rule_elem.findtext('í–‰ì •ê·œì¹™ì¢…ë¥˜', ''),
                    'promulgation_date': rule_elem.findtext('ë°œë ¹ì¼ì', ''),
                    'enforcement_date': rule_elem.findtext('ì‹œí–‰ì¼ì', ''),
                    'is_admin_rule': True,
                    'search_query': search_query
                }
                
                if rule_info['law_id'] and rule_info['law_name']:
                    rules.append(rule_info)
                    self.logger.debug(f"í–‰ì •ê·œì¹™ ë°œê²¬: {rule_info['law_name']}")
                    
        except ET.ParseError as e:
            self.logger.error(f"í–‰ì •ê·œì¹™ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
            self.logger.debug(f"íŒŒì‹± ì‹¤íŒ¨í•œ ë‚´ìš© ì¼ë¶€: {content[:500]}")
            
        return rules
    
    def _preprocess_xml_content(self, content: str) -> str:
        """XML ë‚´ìš© ì „ì²˜ë¦¬"""
        # BOM ì œê±°
        if content.startswith('\ufeff'):
            content = content[1:]
        
        # XML í—¤ë” í™•ì¸
        if not content.strip().startswith('<?xml'):
            content = '<?xml version="1.0" encoding="UTF-8"?>\n' + content
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        content = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', content)
        
        return content
    
    def collect_law_details(self, laws: List[Dict[str, Any]],
                           progress_callback=None,
                           expand_hierarchy: bool = False) -> Dict[str, Dict[str, Any]]:
        """ë²•ë ¹ ìƒì„¸ ì •ë³´ ë³‘ë ¬ ìˆ˜ì§‘ ë° ì„ íƒ ì‹œ ê³„ì¸µ í™•ì¥ - ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜• ì§€ì›"""
        collected: Dict[str, Dict[str, Any]] = {}

        with ThreadPoolExecutor(max_workers=self.config.MAX_CONCURRENT) as executor:
            # ìˆ˜ì§‘ ì‘ì—… ì œì¶œ - ë°ì´í„° ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì„œë“œ í˜¸ì¶œ
            future_to_law = {}
            for law in laws:
                data_type = law.get('data_type', '')

                # ìƒˆë¡œìš´ ë°ì´í„° ìœ í˜•ì¸ ê²½ìš° get_detail_by_type ì‚¬ìš©
                if data_type in ['ordinance', 'precedent', 'constitutional',
                                'interpretation', 'admin_decision', 'treaty']:
                    future = executor.submit(self.get_detail_by_type, law)
                else:
                    # ê¸°ì¡´ ë²•ë ¹/í–‰ì •ê·œì¹™
                    future = executor.submit(
                        self._get_law_detail,
                        law['law_id'],
                        law.get('law_msn', ''),
                        law['law_name'],
                        law.get('is_admin_rule', False)
                    )
                future_to_law[future] = law

            # ê²°ê³¼ ìˆ˜ì§‘
            for idx, future in enumerate(as_completed(future_to_law)):
                law = future_to_law[future]

                try:
                    detail = future.result()
                    if detail:
                        collected[law['law_id']] = detail

                    if progress_callback:
                        progress_callback((idx + 1) / len(laws))

                except Exception as e:
                    self.logger.error(f"{law['law_name']} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")

        if expand_hierarchy and collected:
            self._expand_related_laws(collected)

        return collected
    
    def _expand_related_laws(self, collected: Dict[str, Dict[str, Any]],
                             max_depth: int = 2) -> None:
        """ì„ íƒëœ ë²•ë ¹ì˜ ê´€ê³„ë¥¼ ì¶”ì í•˜ì—¬ ì‹œí–‰ë ¹Â·ì‹œí–‰ê·œì¹™Â·í–‰ì •ê·œì¹™ì„ ìë™ í™•ì¥"""
        processed_ids = set(collected.keys())
        seen_candidates: Set[Tuple[str, str]] = set()
        queue: deque[Tuple[str, int]] = deque((law_id, 0) for law_id in collected.keys())

        while queue:
            current_id, depth = queue.popleft()
            current_detail = collected.get(current_id)
            if not current_detail:
                continue

            if depth >= max_depth:
                continue

            candidates = self._generate_hierarchy_candidates(current_detail)
            for related_name in current_detail.get('related_law_names', []) or []:
                candidates.append(("ê´€ë ¨ ë²•ë ¹", related_name))

            for relation, candidate_name in candidates:
                normalized_candidate = self._normalize_law_name(candidate_name)
                if not normalized_candidate:
                    continue

                candidate_key = (relation, normalized_candidate)
                if candidate_key in seen_candidates:
                    continue
                seen_candidates.add(candidate_key)

                search_results = self._search_exact_match(candidate_name)
                for result in search_results:
                    result_id = result.get('law_id')
                    result_msn = result.get('law_msn')

                    if not result_id or result_id in processed_ids:
                        continue

                    detail = self._get_law_detail(
                        result_id,
                        result_msn,
                        result.get('law_name', candidate_name),
                        result.get('is_admin_rule', False)
                    )

                    if not detail:
                        continue

                    detail.setdefault('related_laws', [])
                    detail['parent_law_id'] = current_id
                    detail['relationship_from_parent'] = relation
                    detail['source_candidate'] = candidate_name

                    collected[result_id] = detail
                    processed_ids.add(result_id)
                    queue.append((result_id, depth + 1))

                    current_detail.setdefault('related_laws', [])
                    current_detail['related_laws'].append({
                        'law_id': result_id,
                        'law_name': detail['law_name'],
                        'relationship': relation,
                        'is_admin_rule': detail.get('is_admin_rule', False)
                    })

    def _generate_hierarchy_candidates(self, law_detail: Dict[str, Any]) -> List[Tuple[str, str]]:
        """ë²•ë ¹ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ì‹œí–‰ë ¹Â·ì‹œí–‰ê·œì¹™Â·í–‰ì •ê·œì¹™ í›„ë³´ ìƒì„±"""
        law_name = law_detail.get('law_name', '').strip()
        if not law_name:
            return []

        normalized_name = self._normalize_law_name(law_name)
        candidates: List[Tuple[str, str]] = []
        admin_base = self._prepare_admin_base(normalized_name)

        def add_candidate(relation: str, candidate: str) -> None:
            cleaned = self._normalize_law_name(candidate)
            if cleaned and cleaned != normalized_name:
                candidates.append((relation, cleaned))

        if 'ì‹œí–‰ë ¹' in normalized_name:
            base_name = normalized_name.replace(' ì‹œí–‰ë ¹', '').replace('ì‹œí–‰ë ¹', '').strip()
            if base_name:
                add_candidate('ëª¨ë²•', base_name)
                add_candidate('ì‹œí–‰ê·œì¹™', f"{base_name} ì‹œí–‰ê·œì¹™")
                add_candidate('ì‹œí–‰ì„¸ì¹™', f"{base_name} ì‹œí–‰ì„¸ì¹™")
                self._add_admin_candidates(candidates, base_name)
        elif any(suffix in normalized_name for suffix in ['ì‹œí–‰ê·œì¹™', 'ì‹œí–‰ì„¸ì¹™']):
            base_name = normalized_name
            base_name = base_name.replace(' ì‹œí–‰ê·œì¹™', '').replace('ì‹œí–‰ê·œì¹™', '')
            base_name = base_name.replace(' ì‹œí–‰ì„¸ì¹™', '').replace('ì‹œí–‰ì„¸ì¹™', '').strip()
            if base_name:
                add_candidate('ëª¨ë²•', base_name)
                add_candidate('ì‹œí–‰ë ¹', f"{base_name} ì‹œí–‰ë ¹")
                self._add_admin_candidates(candidates, base_name)
        else:
            add_candidate('ì‹œí–‰ë ¹', f"{normalized_name} ì‹œí–‰ë ¹")
            add_candidate('ì‹œí–‰ê·œì¹™', f"{normalized_name} ì‹œí–‰ê·œì¹™")
            add_candidate('ì‹œí–‰ì„¸ì¹™', f"{normalized_name} ì‹œí–‰ì„¸ì¹™")
            self._add_admin_candidates(candidates, normalized_name)

        if admin_base and admin_base != normalized_name:
            self._add_admin_candidates(candidates, admin_base)

        # ì¤‘ë³µ ì œê±°
        unique_candidates: Dict[Tuple[str, str], Tuple[str, str]] = {}
        for relation, candidate in candidates:
            key = (relation, candidate)
            unique_candidates[key] = (relation, candidate)

        return list(unique_candidates.values())

    def _add_admin_candidates(self, bucket: List[Tuple[str, str]], base_name: str) -> None:
        """í–‰ì •ê·œì¹™ ê°€ëŠ¥ì„±ì´ ë†’ì€ í›„ë³´ë¥¼ ë²„í‚·ì— ì¶”ê°€"""
        admin_base = self._prepare_admin_base(base_name)
        if not admin_base:
            return

        suffixes = [
            'ê°ë…ê·œì •',
            'ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™',
            'ì—…ë¬´ì‹œí–‰ì„¸ì¹™',
            'ê°ë…ê·œì • ì‹œí–‰ì„¸ì¹™',
            'ê°ë…ê·œì •ì‹œí–‰ì„¸ì¹™',
            'ê·œì •',
            'ê³ ì‹œ',
            'í›ˆë ¹',
            'ì˜ˆê·œ',
            'ì§€ì¹¨'
        ]

        for suffix in suffixes:
            candidate = f"{admin_base}{suffix}".strip()
            if candidate and len(candidate) >= 3:
                normalized_candidate = self._normalize_law_name(candidate)
                if normalized_candidate:
                    bucket.append(('í–‰ì •ê·œì¹™', normalized_candidate))

    def _prepare_admin_base(self, base_name: str) -> str:
        """í–‰ì •ê·œì¹™ìš© ê¸°ë³¸ ëª…ì¹­ ìƒì„±"""
        admin_base = base_name.strip()
        for suffix in [' ë²•ë¥ ', 'ë²•ë¥ ', ' ë²•', 'ë²•']:
            if admin_base.endswith(suffix):
                admin_base = admin_base[:-len(suffix)]
                break
        return admin_base.strip()

    def _get_law_detail(self, law_id: str, law_msn: str,
                       law_name: str, is_admin_rule: bool) -> Optional[Dict[str, Any]]:
        """ë²•ë ¹ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if is_admin_rule:
            return self._get_admin_rule_detail(law_id, law_msn, law_name)
        else:
            return self._get_general_law_detail(law_id, law_msn, law_name)
    
    def _get_general_law_detail(self, law_id: str, law_msn: str, 
                               law_name: str) -> Optional[Dict[str, Any]]:
        """ì¼ë°˜ ë²•ë ¹ ìƒì„¸ ì •ë³´"""
        params = {
            'OC': self.oc_code,
            'target': 'law',
            'type': 'XML',
            'MST': law_msn
        }
        
        try:
            response = self.session.get(
                self.config.LAW_DETAIL_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )
            
            if response.status_code != 200:
                return None
                
            # ìƒì„¸ ì •ë³´ íŒŒì‹±
            return self._parse_law_detail(response.text, law_id, law_msn, law_name)
            
        except Exception as e:
            self.logger.error(f"ë²•ë ¹ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _get_admin_rule_detail(self, law_id: str, law_msn: str,
                               law_name: str) -> Optional[Dict[str, Any]]:
        """í–‰ì •ê·œì¹™ ìƒì„¸ ì •ë³´ - ID íŒŒë¼ë¯¸í„° ì‚¬ìš©"""
        params = {
            'OC': self.oc_code,
            'target': 'admrul',
            'type': 'XML',
            'ID': law_msn  # MSTê°€ ì•„ë‹Œ ID ì‚¬ìš©!
        }

        try:
            self.logger.debug(f"í–‰ì •ê·œì¹™ ìƒì„¸ ì¡°íšŒ: {law_name}")
            self.logger.debug(f"íŒŒë¼ë¯¸í„°: {params}")
            
            response = self.session.get(
                self.config.ADMIN_RULE_DETAIL_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )
            
            if response.status_code != 200:
                self.logger.warning(f"í–‰ì •ê·œì¹™ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                return None
                
            # í–‰ì •ê·œì¹™ ìƒì„¸ íŒŒì‹±
            return self._parse_admin_rule_detail(response.text, law_id, law_msn, law_name)
            
        except Exception as e:
            self.logger.error(f"í–‰ì •ê·œì¹™ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_law_detail(self, content: str, law_id: str, 
                         law_msn: str, law_name: str) -> Dict[str, Any]:
        """ì¼ë°˜ ë²•ë ¹ ìƒì„¸ ì •ë³´ íŒŒì‹± - ê°œì„ ëœ PDF ì¶”ì¶œ"""
        detail = {
            'law_id': law_id,
            'law_msn': law_msn,
            'law_name': law_name,
            'law_type': '',
            'promulgation_date': '',
            'enforcement_date': '',
            'department': '',
            'articles': [],
            'supplementary_provisions': [],
            'attachments': [],
            'attachment_pdfs': [],  # PDF ì²¨ë¶€íŒŒì¼ ì¶”ê°€
            'raw_content': '',
            'is_admin_rule': False,
            'related_laws': [],
            'related_law_names': []
        }
        
        try:
            # ì „ì²˜ë¦¬
            content = self._preprocess_xml_content(content)
            
            # XML íŒŒì‹±
            root = ET.fromstring(content.encode('utf-8'))
            
            # ê¸°ë³¸ ì •ë³´
            basic_info = root.find('.//ê¸°ë³¸ì •ë³´')
            if basic_info is not None:
                detail['law_type'] = basic_info.findtext('ë²•ì¢…êµ¬ë¶„ëª…', '')
                detail['department'] = basic_info.findtext('ì†Œê´€ë¶€ì²˜ëª…', '')
                detail['promulgation_date'] = basic_info.findtext('ê³µí¬ì¼ì', '')
                detail['enforcement_date'] = basic_info.findtext('ì‹œí–‰ì¼ì', '')
            
            # ì¡°ë¬¸ ì¶”ì¶œ
            self._extract_articles(root, detail)
            
            # ë¶€ì¹™ ì¶”ì¶œ
            self._extract_supplementary_provisions(root, detail)
            
            # ë³„í‘œ ì¶”ì¶œ
            self._extract_attachments(root, detail)

            # PDF ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „
            self._extract_pdf_attachments_enhanced(root, detail)

            # ê´€ë ¨ ë²•ë ¹ëª… ì¶”ì¶œ
            detail['related_law_names'] = self._extract_related_law_names(root, law_name)

            # ì›ë¬¸ ì €ì¥ (ì¡°ë¬¸ì´ ì—†ëŠ” ê²½ìš°)
            if not detail['articles']:
                detail['raw_content'] = self._extract_full_text(root)
                
            self.logger.info(f"ìƒì„¸ ì •ë³´ íŒŒì‹± ì™„ë£Œ: {law_name} - ì¡°ë¬¸ {len(detail['articles'])}ê°œ, ë³„í‘œ/ë³„ì²¨ {len(detail['attachments'])}ê°œ")
                
        except Exception as e:
            self.logger.error(f"ìƒì„¸ ì •ë³´ íŒŒì‹± ì˜¤ë¥˜: {e}")
            
        return detail
    
    def _parse_admin_rule_detail(self, content: str, law_id: str,
                                law_msn: str, law_name: str) -> Dict[str, Any]:
        """í–‰ì •ê·œì¹™ ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        detail = {
            'law_id': law_id,
            'law_msn': law_msn,
            'law_name': law_name,
            'law_type': '',
            'promulgation_date': '',
            'enforcement_date': '',
            'department': '',
            'articles': [],
            'supplementary_provisions': [],
            'attachments': [],
            'attachment_pdfs': [],  # PDF ì²¨ë¶€íŒŒì¼ ì¶”ê°€
            'raw_content': '',
            'is_admin_rule': True,
            'related_laws': [],
            'related_law_names': []
        }
        
        try:
            # ì „ì²˜ë¦¬
            content = self._preprocess_xml_content(content)
            
            # XML íŒŒì‹±
            root = ET.fromstring(content.encode('utf-8'))
            
            # í–‰ì •ê·œì¹™ ê¸°ë³¸ ì •ë³´
            basic_info = root.find('.//í–‰ì •ê·œì¹™ê¸°ë³¸ì •ë³´')
            if basic_info is not None:
                detail['law_id'] = basic_info.findtext('í–‰ì •ê·œì¹™ID', '')
                detail['law_type'] = basic_info.findtext('í–‰ì •ê·œì¹™ì¢…ë¥˜', '')
                detail['department'] = basic_info.findtext('ì†Œê´€ë¶€ì²˜ëª…', '')
                detail['promulgation_date'] = basic_info.findtext('ë°œë ¹ì¼ì', '')
                detail['enforcement_date'] = basic_info.findtext('ì‹œí–‰ì¼ì', '')
            else:
                # ëŒ€ì²´ ê²½ë¡œ
                detail['law_id'] = root.findtext('.//í–‰ì •ê·œì¹™ID', '')
                detail['law_type'] = root.findtext('.//í–‰ì •ê·œì¹™ì¢…ë¥˜', '')
                detail['department'] = root.findtext('.//ì†Œê´€ë¶€ì²˜ëª…', '')
                detail['promulgation_date'] = root.findtext('.//ë°œë ¹ì¼ì', '')
                detail['enforcement_date'] = root.findtext('.//ì‹œí–‰ì¼ì', '')
            
            # ì¡°ë¬¸ ì¶”ì¶œ (í–‰ì •ê·œì¹™ë„ ë™ì¼í•œ êµ¬ì¡° ì‚¬ìš© ê°€ëŠ¥)
            self._extract_articles(root, detail)
            
            # ë¶€ì¹™ ì¶”ì¶œ
            self._extract_supplementary_provisions(root, detail)
            
            # ë³„í‘œ ì¶”ì¶œ
            self._extract_attachments(root, detail)

            # PDF ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „
            self._extract_pdf_attachments_enhanced(root, detail)

            # ì›ë¬¸ ì €ì¥
            if not detail['articles']:
                detail['raw_content'] = self._extract_full_text(root)

            # ê´€ë ¨ ë²•ë ¹ëª… ì¶”ì¶œ
            detail['related_law_names'] = self._extract_related_law_names(root, law_name)

            self.logger.info(f"í–‰ì •ê·œì¹™ ìƒì„¸ íŒŒì‹± ì™„ë£Œ: {law_name} - ì¡°ë¬¸ {len(detail['articles'])}ê°œ, ë³„í‘œ/ë³„ì²¨ {len(detail['attachments'])}ê°œ")
                
        except Exception as e:
            self.logger.error(f"í–‰ì •ê·œì¹™ ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return detail

    def _extract_related_law_names(self, root: ET.Element, current_name: str) -> List[str]:
        """ìƒì„¸ XMLì—ì„œ ê´€ë ¨ ë²•ë ¹ëª…ì„ ìˆ˜ì§‘"""
        related: Set[str] = set()
        candidate_tags = ['ê´€ë ¨ë²•ë ¹', 'ê´€ê³„ë²•ë ¹', 'ì—°ê´€ë²•ë ¹', 'ë²•ë ¹ì²´ê³„ë„', 'ëª¨ë²•ë ¹', 'í•˜ìœ„ë²•ë ¹']

        for tag in candidate_tags:
            for elem in root.findall(f'.//{tag}'):
                text = self._collect_text_content(elem)
                related.update(self._extract_law_names_from_text(text))

        for elem in root.iter():
            if 'ë²•ë ¹ëª…' in elem.tag and elem.text:
                name = self._normalize_candidate_name(elem.text)
                if name:
                    related.add(name)

        current_normalized = self._normalize_law_name(current_name)
        filtered = [name for name in related if name and name != current_normalized]
        filtered.sort()
        return filtered

    def _collect_text_content(self, elem: ET.Element) -> str:
        """ìš”ì†Œ ë‚´ë¶€ í…ìŠ¤íŠ¸ë¥¼ ê³µë°±ìœ¼ë¡œ ê²°í•©"""
        parts = [text.strip() for text in elem.itertext() if text and text.strip()]
        return ' '.join(parts)

    def _extract_law_names_from_text(self, text: str) -> Set[str]:
        """í…ìŠ¤íŠ¸ ë¸”ë¡ì—ì„œ ë²•ë ¹ëª… í›„ë³´ ì¶”ì¶œ"""
        candidates: Set[str] = set()
        if not text:
            return candidates

        segments = re.split(r'[\n\r,;Â·â€¢â–¶\-]', text)
        for segment in segments:
            segment = segment.strip()
            if not segment or len(segment) > 80:
                continue

            normalized = self._normalize_candidate_name(segment)
            if not normalized or len(normalized) < 3:
                continue

            if any(keyword in normalized for keyword in self.patterns.LAW_TYPES):
                candidates.add(normalized)

        return candidates

    def _normalize_candidate_name(self, name: str) -> str:
        """ê´€ë ¨ ë²•ë ¹ í›„ë³´ëª…ì„ ì •ê·œí™”"""
        if not name:
            return ''

        cleaned = re.sub(r'\s+', ' ', name)
        cleaned = re.sub(r'\(.*?\)', '', cleaned)
        cleaned = re.sub(r'\[ì‹œí–‰[^\]]*\]', '', cleaned)
        cleaned = cleaned.strip(' -,:;')
        return self._normalize_law_name(cleaned)

    def _extract_articles(self, root: ET.Element, detail: Dict[str, Any]) -> None:
        """ì¡°ë¬¸ ì¶”ì¶œ"""
        # í‘œì¤€ ì¡°ë¬¸ êµ¬ì¡°
        articles_section = root.find('.//ì¡°ë¬¸')
        if articles_section is not None:
            for article_unit in articles_section.findall('.//ì¡°ë¬¸ë‹¨ìœ„'):
                article = self._parse_article_unit(article_unit)
                if article:
                    detail['articles'].append(article)
            return
        
        # ì¡°ë¬¸ë‚´ìš© ì§ì ‘ ì°¾ê¸°
        for article_content in root.findall('.//ì¡°ë¬¸ë‚´ìš©'):
            if article_content.text:
                articles = self._parse_article_text(article_content.text)
                detail['articles'].extend(articles)
    
    def _parse_article_unit(self, article_elem: ET.Element) -> Optional[Dict[str, Any]]:
        """ì¡°ë¬¸ë‹¨ìœ„ íŒŒì‹±"""
        article = {
            'number': '',
            'title': '',
            'content': '',
            'paragraphs': []
        }
        
        # ì¡°ë¬¸ë²ˆí˜¸
        article_num = article_elem.findtext('ì¡°ë¬¸ë²ˆí˜¸', '')
        if article_num:
            article['number'] = f"ì œ{article_num}ì¡°"
        
        # ì¡°ë¬¸ì œëª©
        article['title'] = article_elem.findtext('ì¡°ë¬¸ì œëª©', '')
        
        # ì¡°ë¬¸ë‚´ìš©
        article['content'] = article_elem.findtext('ì¡°ë¬¸ë‚´ìš©', '')
        
        # í•­ ì¶”ì¶œ
        for para in article_elem.findall('.//í•­'):
            paragraph = {
                'number': para.findtext('í•­ë²ˆí˜¸', ''),
                'content': para.findtext('í•­ë‚´ìš©', '')
            }
            if paragraph['content']:
                article['paragraphs'].append(paragraph)
        
        return article if (article['number'] or article['content']) else None
    
    def _parse_article_text(self, text: str) -> List[Dict[str, Any]]:
        """ì¡°ë¬¸ í…ìŠ¤íŠ¸ íŒŒì‹±"""
        articles = []
        
        # ì¡°ë¬¸ íŒ¨í„´
        pattern = r'(ì œ\d+ì¡°(?:ì˜\d+)?)\s*(?:\((.*?)\))?\s*(.*?)(?=ì œ\d+ì¡°|$)'
        matches = re.findall(pattern, text, re.DOTALL)
        
        for match in matches:
            article = {
                'number': match[0],
                'title': match[1],
                'content': match[2].strip(),
                'paragraphs': []
            }
            articles.append(article)
            
        return articles
    
    def _extract_supplementary_provisions(self, root: ET.Element, 
                                        detail: Dict[str, Any]) -> None:
        """ë¶€ì¹™ ì¶”ì¶œ"""
        for addendum in root.findall('.//ë¶€ì¹™'):
            provision = {
                'number': addendum.findtext('ë¶€ì¹™ë²ˆí˜¸', ''),
                'promulgation_date': addendum.findtext('ë¶€ì¹™ê³µí¬ì¼ì', ''),
                'content': self._get_all_text(addendum)
            }
            if provision['content']:
                detail['supplementary_provisions'].append(provision)
        
        # ë¶€ì¹™ë‚´ìš© ì§ì ‘ ì°¾ê¸°
        if not detail['supplementary_provisions']:
            for elem in root.findall('.//ë¶€ì¹™ë‚´ìš©'):
                if elem.text:
                    detail['supplementary_provisions'].append({
                        'number': '',
                        'promulgation_date': '',
                        'content': elem.text
                    })
    
    def _extract_attachments(self, root: ET.Element, detail: Dict[str, Any]) -> None:
        """ë³„í‘œ/ë³„ì²¨ ì¶”ì¶œ"""
        # ë³„í‘œ
        for table in root.findall('.//ë³„í‘œ'):
            attachment = {
                'type': 'ë³„í‘œ',
                'number': table.findtext('ë³„í‘œë²ˆí˜¸', ''),
                'title': table.findtext('ë³„í‘œì œëª©', ''),
                'content': self._get_all_text(table)
            }
            if attachment['content'] or attachment['title']:
                detail['attachments'].append(attachment)
        
        # ë³„ì§€
        for form in root.findall('.//ë³„ì§€'):
            attachment = {
                'type': 'ë³„ì§€',
                'number': form.findtext('ë³„ì§€ë²ˆí˜¸', ''),
                'title': form.findtext('ë³„ì§€ì œëª©', ''),
                'content': self._get_all_text(form)
            }
            if attachment['content'] or attachment['title']:
                detail['attachments'].append(attachment)
    
    def _extract_pdf_attachments_enhanced(self, root: ET.Element, detail: Dict[str, Any]) -> None:
        """PDF ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „ (OCR ì§€ì›)"""
        # ë³„í‘œ/ë³„ì§€ ì •ë³´ì—ì„œ PDF URL íŒ¨í„´ ì¶”ì¶œ
        law_name = detail['law_name']
        law_msn = detail['law_msn']
        promulgation_date = detail.get('promulgation_date', '').replace('-', '').replace('.', '')
        enforcement_date = detail.get('enforcement_date', '').replace('-', '').replace('.', '')
        
        # ë²•ë ¹ëª…ì—ì„œ ê´„í˜¸ ì œê±° (URLì—ì„œ ë¬¸ì œ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŒ)
        clean_law_name = re.sub(r'\([^)]*\)', '', law_name).strip()
        
        # ë³„í‘œ/ë³„ì§€ê°€ ìˆëŠ” ê²½ìš° PDF ì •ë³´ ìƒì„±
        if detail['attachments']:
            for attachment in detail['attachments']:
                att_type = attachment['type']
                att_num = attachment['number']
                
                if att_type and att_num:
                    # PDF ì •ë³´ ìƒì„±
                    pdf_info = {
                        'file_seq': '',
                        'file_name': f"{clean_law_name}_{att_type}{att_num}",
                        'type': att_type,
                        'content_text': attachment.get('content', ''),  # í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥
                        'has_pdf': False,  # PDF ì¡´ì¬ ì—¬ë¶€
                        'ocr_available': True  # OCR ê°€ëŠ¥ ì—¬ë¶€
                    }
                    
                    # ì¤‘ë³µ ì²´í¬
                    if not any(p['file_name'] == pdf_info['file_name'] for p in detail['attachment_pdfs']):
                        detail['attachment_pdfs'].append(pdf_info)
                        self.logger.info(f"ë³„í‘œ/ë³„ì§€ ë°œê²¬: {pdf_info['file_name']} (í…ìŠ¤íŠ¸ {len(pdf_info['content_text'])}ì)")
        
        # ë¡œê·¸ ì¶œë ¥
        if detail['attachment_pdfs']:
            self.logger.info(f"ë³„í‘œ/ë³„ì§€ {len(detail['attachment_pdfs'])}ê°œ ë°œê²¬: {law_name}")
            self.logger.info("ğŸ’¡ PDF ë‹¤ìš´ë¡œë“œ ëŒ€ì‹  í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def _extract_full_text(self, root: ET.Element) -> str:
        """ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        return self._get_all_text(root)
    
    def _get_all_text(self, elem: ET.Element) -> str:
        """ìš”ì†Œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        texts = []
        
        if elem.text:
            texts.append(elem.text.strip())
            
        for child in elem:
            child_text = self._get_all_text(child)
            if child_text:
                texts.append(child_text)
                
            if child.tail:
                texts.append(child.tail.strip())
                
        return ' '.join(texts)
    
    def _remove_duplicates(self, laws: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique_laws = []

        for law in laws:
            law_id = law['law_id']
            if law_id not in seen:
                seen.add(law_id)
                unique_laws.append(law)

        return unique_laws

    # ===== ìì¹˜ë²•ê·œ ê²€ìƒ‰/ì¡°íšŒ =====
    def search_ordinance(self, query: str) -> List[Dict[str, Any]]:
        """ìì¹˜ë²•ê·œ ê²€ìƒ‰ (target=ordin)"""
        params = {
            'OC': self.oc_code,
            'target': 'ordin',
            'type': 'XML',
            'query': query,
            'display': str(self.config.RESULTS_PER_PAGE),
            'page': '1'
        }

        try:
            response = self.session.get(
                self.config.LAW_SEARCH_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                self.logger.warning(f"ìì¹˜ë²•ê·œ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []

            return self._parse_ordinance_search_response(response.text, query)

        except Exception as e:
            self.logger.error(f"ìì¹˜ë²•ê·œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _parse_ordinance_search_response(self, content: str, search_query: str) -> List[Dict[str, Any]]:
        """ìì¹˜ë²•ê·œ ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹±"""
        results = []

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            for item in root.findall('.//law') or root.findall('.//ordin'):
                result = {
                    'law_id': item.findtext('ìì¹˜ë²•ê·œID', '') or item.findtext('ë²•ë ¹ID', ''),
                    'law_msn': item.findtext('ìì¹˜ë²•ê·œì¼ë ¨ë²ˆí˜¸', '') or item.findtext('ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_name': item.findtext('ìì¹˜ë²•ê·œëª…', '') or item.findtext('ë²•ë ¹ëª…í•œê¸€', ''),
                    'law_type': 'ìì¹˜ë²•ê·œ',
                    'local_gov': item.findtext('ìì¹˜ë‹¨ì²´ëª…', '') or item.findtext('ì§€ìì²´ëª…', ''),
                    'promulgation_date': item.findtext('ê³µí¬ì¼ì', ''),
                    'enforcement_date': item.findtext('ì‹œí–‰ì¼ì', ''),
                    'data_type': 'ordinance',
                    'search_query': search_query
                }

                if result['law_id'] and result['law_name']:
                    results.append(result)

        except ET.ParseError as e:
            self.logger.error(f"ìì¹˜ë²•ê·œ XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def get_ordinance_detail(self, ordin_id: str, ordin_msn: str, ordin_name: str) -> Optional[Dict[str, Any]]:
        """ìì¹˜ë²•ê·œ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        params = {
            'OC': self.oc_code,
            'target': 'ordin',
            'type': 'XML',
            'ID': ordin_id
        }

        try:
            response = self.session.get(
                self.config.LAW_DETAIL_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                return None

            return self._parse_ordinance_detail(response.text, ordin_id, ordin_msn, ordin_name)

        except Exception as e:
            self.logger.error(f"ìì¹˜ë²•ê·œ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None

    def _parse_ordinance_detail(self, content: str, ordin_id: str, ordin_msn: str, ordin_name: str) -> Dict[str, Any]:
        """ìì¹˜ë²•ê·œ ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        detail = {
            'law_id': ordin_id,
            'law_msn': ordin_msn,
            'law_name': ordin_name,
            'law_type': 'ìì¹˜ë²•ê·œ',
            'local_gov': '',
            'promulgation_date': '',
            'enforcement_date': '',
            'articles': [],
            'supplementary_provisions': [],
            'attachments': [],
            'raw_content': '',
            'data_type': 'ordinance'
        }

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            # ê¸°ë³¸ ì •ë³´
            detail['local_gov'] = root.findtext('.//ìì¹˜ë‹¨ì²´ëª…', '')
            detail['promulgation_date'] = root.findtext('.//ê³µí¬ì¼ì', '')
            detail['enforcement_date'] = root.findtext('.//ì‹œí–‰ì¼ì', '')

            # ì¡°ë¬¸ ì¶”ì¶œ
            self._extract_articles(root, detail)

            # ë¶€ì¹™ ì¶”ì¶œ
            self._extract_supplementary_provisions(root, detail)

            # ë³„í‘œ ì¶”ì¶œ
            self._extract_attachments(root, detail)

            # ì›ë¬¸ ì €ì¥
            if not detail['articles']:
                detail['raw_content'] = self._extract_full_text(root)

        except Exception as e:
            self.logger.error(f"ìì¹˜ë²•ê·œ ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return detail

    # ===== íŒë¡€ ê²€ìƒ‰/ì¡°íšŒ =====
    def search_precedent(self, query: str) -> List[Dict[str, Any]]:
        """íŒë¡€ ê²€ìƒ‰ (target=prec)"""
        params = {
            'OC': self.oc_code,
            'target': 'prec',
            'type': 'XML',
            'query': query,
            'display': str(self.config.RESULTS_PER_PAGE),
            'page': '1'
        }

        try:
            response = self.session.get(
                self.config.LAW_SEARCH_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                self.logger.warning(f"íŒë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []

            return self._parse_precedent_search_response(response.text, query)

        except Exception as e:
            self.logger.error(f"íŒë¡€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _parse_precedent_search_response(self, content: str, search_query: str) -> List[Dict[str, Any]]:
        """íŒë¡€ ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹±"""
        results = []

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            for item in root.findall('.//prec'):
                result = {
                    'law_id': item.findtext('íŒë¡€ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_msn': item.findtext('íŒë¡€ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_name': item.findtext('ì‚¬ê±´ëª…', ''),
                    'law_type': 'íŒë¡€',
                    'case_no': item.findtext('ì‚¬ê±´ë²ˆí˜¸', ''),
                    'court': item.findtext('ë²•ì›ëª…', ''),
                    'decision_date': item.findtext('ì„ ê³ ì¼ì', ''),
                    'decision_type': item.findtext('ì‚¬ê±´ì¢…ë¥˜ëª…', ''),
                    'data_type': 'precedent',
                    'search_query': search_query
                }

                if result['law_id'] and result['law_name']:
                    results.append(result)

        except ET.ParseError as e:
            self.logger.error(f"íŒë¡€ XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def get_precedent_detail(self, prec_id: str, prec_name: str) -> Optional[Dict[str, Any]]:
        """íŒë¡€ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        params = {
            'OC': self.oc_code,
            'target': 'prec',
            'type': 'XML',
            'ID': prec_id
        }

        try:
            response = self.session.get(
                self.config.LAW_DETAIL_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                return None

            return self._parse_precedent_detail(response.text, prec_id, prec_name)

        except Exception as e:
            self.logger.error(f"íŒë¡€ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None

    def _parse_precedent_detail(self, content: str, prec_id: str, prec_name: str) -> Dict[str, Any]:
        """íŒë¡€ ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        detail = {
            'law_id': prec_id,
            'law_msn': prec_id,
            'law_name': prec_name,
            'law_type': 'íŒë¡€',
            'case_no': '',
            'court': '',
            'decision_date': '',
            'decision_type': '',
            'judgment_summary': '',
            'judgment_content': '',
            'reference_articles': '',
            'reference_cases': '',
            'raw_content': '',
            'data_type': 'precedent'
        }

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            detail['case_no'] = root.findtext('.//ì‚¬ê±´ë²ˆí˜¸', '')
            detail['court'] = root.findtext('.//ë²•ì›ëª…', '')
            detail['decision_date'] = root.findtext('.//ì„ ê³ ì¼ì', '')
            detail['decision_type'] = root.findtext('.//ì‚¬ê±´ì¢…ë¥˜ëª…', '')
            detail['judgment_summary'] = root.findtext('.//íŒì‹œì‚¬í•­', '')
            detail['judgment_content'] = root.findtext('.//íŒê²°ìš”ì§€', '') or root.findtext('.//ì „ë¬¸', '')
            detail['reference_articles'] = root.findtext('.//ì°¸ì¡°ì¡°ë¬¸', '')
            detail['reference_cases'] = root.findtext('.//ì°¸ì¡°íŒë¡€', '')
            detail['raw_content'] = root.findtext('.//ì „ë¬¸', '')

        except Exception as e:
            self.logger.error(f"íŒë¡€ ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return detail

    # ===== í—Œì¬ê²°ì •ë¡€ ê²€ìƒ‰/ì¡°íšŒ =====
    def search_constitutional_decision(self, query: str) -> List[Dict[str, Any]]:
        """í—Œì¬ê²°ì •ë¡€ ê²€ìƒ‰ (target=detc)"""
        params = {
            'OC': self.oc_code,
            'target': 'detc',
            'type': 'XML',
            'query': query,
            'display': str(self.config.RESULTS_PER_PAGE),
            'page': '1'
        }

        try:
            response = self.session.get(
                self.config.LAW_SEARCH_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                self.logger.warning(f"í—Œì¬ê²°ì •ë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []

            return self._parse_constitutional_search_response(response.text, query)

        except Exception as e:
            self.logger.error(f"í—Œì¬ê²°ì •ë¡€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _parse_constitutional_search_response(self, content: str, search_query: str) -> List[Dict[str, Any]]:
        """í—Œì¬ê²°ì •ë¡€ ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹±"""
        results = []

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            for item in root.findall('.//detc'):
                result = {
                    'law_id': item.findtext('í—Œì¬ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_msn': item.findtext('í—Œì¬ê²°ì •ë¡€ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_name': item.findtext('ì‚¬ê±´ëª…', ''),
                    'law_type': 'í—Œì¬ê²°ì •ë¡€',
                    'case_no': item.findtext('ì‚¬ê±´ë²ˆí˜¸', ''),
                    'decision_date': item.findtext('ì¢…êµ­ì¼ì', ''),
                    'data_type': 'constitutional',
                    'search_query': search_query
                }

                if result['law_id'] and result['law_name']:
                    results.append(result)

        except ET.ParseError as e:
            self.logger.error(f"í—Œì¬ê²°ì •ë¡€ XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def get_constitutional_detail(self, detc_id: str, detc_name: str) -> Optional[Dict[str, Any]]:
        """í—Œì¬ê²°ì •ë¡€ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        params = {
            'OC': self.oc_code,
            'target': 'detc',
            'type': 'XML',
            'ID': detc_id
        }

        try:
            response = self.session.get(
                self.config.LAW_DETAIL_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                return None

            return self._parse_constitutional_detail(response.text, detc_id, detc_name)

        except Exception as e:
            self.logger.error(f"í—Œì¬ê²°ì •ë¡€ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None

    def _parse_constitutional_detail(self, content: str, detc_id: str, detc_name: str) -> Dict[str, Any]:
        """í—Œì¬ê²°ì •ë¡€ ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        detail = {
            'law_id': detc_id,
            'law_msn': detc_id,
            'law_name': detc_name,
            'law_type': 'í—Œì¬ê²°ì •ë¡€',
            'case_no': '',
            'decision_date': '',
            'case_type': '',
            'judgment_summary': '',
            'decision_summary': '',
            'full_text': '',
            'reference_articles': '',
            'reference_cases': '',
            'data_type': 'constitutional'
        }

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            detail['case_no'] = root.findtext('.//ì‚¬ê±´ë²ˆí˜¸', '')
            detail['decision_date'] = root.findtext('.//ì¢…êµ­ì¼ì', '')
            detail['case_type'] = root.findtext('.//ì‚¬ê±´ì¢…ë¥˜ëª…', '')
            detail['judgment_summary'] = root.findtext('.//íŒì‹œì‚¬í•­', '')
            detail['decision_summary'] = root.findtext('.//ê²°ì •ìš”ì§€', '')
            detail['full_text'] = root.findtext('.//ì „ë¬¸', '')
            detail['reference_articles'] = root.findtext('.//ì°¸ì¡°ì¡°ë¬¸', '')
            detail['reference_cases'] = root.findtext('.//ì°¸ì¡°íŒë¡€', '')

        except Exception as e:
            self.logger.error(f"í—Œì¬ê²°ì •ë¡€ ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return detail

    # ===== ë²•ë ¹í•´ì„ë¡€ ê²€ìƒ‰/ì¡°íšŒ =====
    def search_interpretation(self, query: str) -> List[Dict[str, Any]]:
        """ë²•ë ¹í•´ì„ë¡€ ê²€ìƒ‰ (target=expc)"""
        params = {
            'OC': self.oc_code,
            'target': 'expc',
            'type': 'XML',
            'query': query,
            'display': str(self.config.RESULTS_PER_PAGE),
            'page': '1'
        }

        try:
            response = self.session.get(
                self.config.LAW_SEARCH_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                self.logger.warning(f"ë²•ë ¹í•´ì„ë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []

            return self._parse_interpretation_search_response(response.text, query)

        except Exception as e:
            self.logger.error(f"ë²•ë ¹í•´ì„ë¡€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _parse_interpretation_search_response(self, content: str, search_query: str) -> List[Dict[str, Any]]:
        """ë²•ë ¹í•´ì„ë¡€ ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹±"""
        results = []

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            for item in root.findall('.//expc'):
                result = {
                    'law_id': item.findtext('ë²•ë ¹í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_msn': item.findtext('ë²•ë ¹í•´ì„ë¡€ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_name': item.findtext('ì•ˆê±´ëª…', ''),
                    'law_type': 'ë²•ë ¹í•´ì„ë¡€',
                    'case_no': item.findtext('ì•ˆê±´ë²ˆí˜¸', ''),
                    'inquiry_org': item.findtext('ì§ˆì˜ê¸°ê´€ëª…', ''),
                    'reply_org': item.findtext('íšŒì‹ ê¸°ê´€ëª…', ''),
                    'reply_date': item.findtext('íšŒì‹ ì¼ì', ''),
                    'data_type': 'interpretation',
                    'search_query': search_query
                }

                if result['law_id'] and result['law_name']:
                    results.append(result)

        except ET.ParseError as e:
            self.logger.error(f"ë²•ë ¹í•´ì„ë¡€ XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def get_interpretation_detail(self, expc_id: str, expc_name: str) -> Optional[Dict[str, Any]]:
        """ë²•ë ¹í•´ì„ë¡€ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        params = {
            'OC': self.oc_code,
            'target': 'expc',
            'type': 'XML',
            'ID': expc_id
        }

        try:
            response = self.session.get(
                self.config.LAW_DETAIL_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                return None

            return self._parse_interpretation_detail(response.text, expc_id, expc_name)

        except Exception as e:
            self.logger.error(f"ë²•ë ¹í•´ì„ë¡€ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None

    def _parse_interpretation_detail(self, content: str, expc_id: str, expc_name: str) -> Dict[str, Any]:
        """ë²•ë ¹í•´ì„ë¡€ ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        detail = {
            'law_id': expc_id,
            'law_msn': expc_id,
            'law_name': expc_name,
            'law_type': 'ë²•ë ¹í•´ì„ë¡€',
            'case_no': '',
            'interpretation_date': '',
            'interpretation_org': '',
            'inquiry_org': '',
            'inquiry_summary': '',
            'reply': '',
            'reason': '',
            'data_type': 'interpretation'
        }

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            detail['case_no'] = root.findtext('.//ì•ˆê±´ë²ˆí˜¸', '')
            detail['interpretation_date'] = root.findtext('.//í•´ì„ì¼ì', '')
            detail['interpretation_org'] = root.findtext('.//í•´ì„ê¸°ê´€ëª…', '')
            detail['inquiry_org'] = root.findtext('.//ì§ˆì˜ê¸°ê´€ëª…', '')
            detail['inquiry_summary'] = root.findtext('.//ì§ˆì˜ìš”ì§€', '')
            detail['reply'] = root.findtext('.//íšŒë‹µ', '')
            detail['reason'] = root.findtext('.//ì´ìœ ', '')

        except Exception as e:
            self.logger.error(f"ë²•ë ¹í•´ì„ë¡€ ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return detail

    # ===== í–‰ì •ì‹¬íŒë¡€ ê²€ìƒ‰/ì¡°íšŒ =====
    def search_admin_decision(self, query: str) -> List[Dict[str, Any]]:
        """í–‰ì •ì‹¬íŒë¡€ ê²€ìƒ‰ (target=decc)"""
        params = {
            'OC': self.oc_code,
            'target': 'decc',
            'type': 'XML',
            'query': query,
            'display': str(self.config.RESULTS_PER_PAGE),
            'page': '1'
        }

        try:
            response = self.session.get(
                self.config.LAW_SEARCH_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                self.logger.warning(f"í–‰ì •ì‹¬íŒë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []

            return self._parse_admin_decision_search_response(response.text, query)

        except Exception as e:
            self.logger.error(f"í–‰ì •ì‹¬íŒë¡€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _parse_admin_decision_search_response(self, content: str, search_query: str) -> List[Dict[str, Any]]:
        """í–‰ì •ì‹¬íŒë¡€ ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹±"""
        results = []

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            for item in root.findall('.//decc'):
                result = {
                    'law_id': item.findtext('í–‰ì •ì‹¬íŒì¬ê²°ë¡€ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_msn': item.findtext('í–‰ì •ì‹¬íŒì¬ê²°ë¡€ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_name': item.findtext('ì‚¬ê±´ëª…', ''),
                    'law_type': 'í–‰ì •ì‹¬íŒë¡€',
                    'case_no': item.findtext('ì‚¬ê±´ë²ˆí˜¸', ''),
                    'disposal_date': item.findtext('ì²˜ë¶„ì¼ì', ''),
                    'decision_date': item.findtext('ì˜ê²°ì¼ì', ''),
                    'disposal_org': item.findtext('ì²˜ë¶„ì²­', ''),
                    'decision_org': item.findtext('ì¬ê²°ì²­', ''),
                    'decision_type': item.findtext('ì¬ê²°êµ¬ë¶„ëª…', ''),
                    'data_type': 'admin_decision',
                    'search_query': search_query
                }

                if result['law_id'] and result['law_name']:
                    results.append(result)

        except ET.ParseError as e:
            self.logger.error(f"í–‰ì •ì‹¬íŒë¡€ XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def get_admin_decision_detail(self, decc_id: str, decc_name: str) -> Optional[Dict[str, Any]]:
        """í–‰ì •ì‹¬íŒë¡€ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        params = {
            'OC': self.oc_code,
            'target': 'decc',
            'type': 'XML',
            'ID': decc_id
        }

        try:
            response = self.session.get(
                self.config.LAW_DETAIL_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                return None

            return self._parse_admin_decision_detail(response.text, decc_id, decc_name)

        except Exception as e:
            self.logger.error(f"í–‰ì •ì‹¬íŒë¡€ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None

    def _parse_admin_decision_detail(self, content: str, decc_id: str, decc_name: str) -> Dict[str, Any]:
        """í–‰ì •ì‹¬íŒë¡€ ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        detail = {
            'law_id': decc_id,
            'law_msn': decc_id,
            'law_name': decc_name,
            'law_type': 'í–‰ì •ì‹¬íŒë¡€',
            'case_no': '',
            'disposal_date': '',
            'decision_date': '',
            'disposal_org': '',
            'decision_org': '',
            'decision_type': '',
            'main_text': '',
            'claim_purport': '',
            'reason': '',
            'decision_summary': '',
            'data_type': 'admin_decision'
        }

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            detail['case_no'] = root.findtext('.//ì‚¬ê±´ë²ˆí˜¸', '')
            detail['disposal_date'] = root.findtext('.//ì²˜ë¶„ì¼ì', '')
            detail['decision_date'] = root.findtext('.//ì˜ê²°ì¼ì', '')
            detail['disposal_org'] = root.findtext('.//ì²˜ë¶„ì²­', '')
            detail['decision_org'] = root.findtext('.//ì¬ê²°ì²­', '')
            detail['decision_type'] = root.findtext('.//ì¬ê²°ë¡€ìœ í˜•ëª…', '')
            detail['main_text'] = root.findtext('.//ì£¼ë¬¸', '')
            detail['claim_purport'] = root.findtext('.//ì²­êµ¬ì·¨ì§€', '')
            detail['reason'] = root.findtext('.//ì´ìœ ', '')
            detail['decision_summary'] = root.findtext('.//ì¬ê²°ìš”ì§€', '')

        except Exception as e:
            self.logger.error(f"í–‰ì •ì‹¬íŒë¡€ ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return detail

    # ===== ì¡°ì•½ ê²€ìƒ‰/ì¡°íšŒ =====
    def search_treaty(self, query: str) -> List[Dict[str, Any]]:
        """ì¡°ì•½ ê²€ìƒ‰ (target=trty)"""
        params = {
            'OC': self.oc_code,
            'target': 'trty',
            'type': 'XML',
            'query': query,
            'display': str(self.config.RESULTS_PER_PAGE),
            'page': '1'
        }

        try:
            response = self.session.get(
                self.config.LAW_SEARCH_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                self.logger.warning(f"ì¡°ì•½ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
                return []

            return self._parse_treaty_search_response(response.text, query)

        except Exception as e:
            self.logger.error(f"ì¡°ì•½ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _parse_treaty_search_response(self, content: str, search_query: str) -> List[Dict[str, Any]]:
        """ì¡°ì•½ ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹±"""
        results = []

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            for item in root.findall('.//trty'):
                result = {
                    'law_id': item.findtext('ì¡°ì•½ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_msn': item.findtext('ì¡°ì•½ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_name': item.findtext('ì¡°ì•½ëª…', ''),
                    'law_type': 'ì¡°ì•½',
                    'treaty_no': item.findtext('ì¡°ì•½ë²ˆí˜¸', ''),
                    'signing_date': item.findtext('ì„œëª…ì¼ì', ''),
                    'enforcement_date': item.findtext('ë°œíš¨ì¼ì', ''),
                    'country': item.findtext('ì²´ê²°êµ­ê°€', ''),
                    'data_type': 'treaty',
                    'search_query': search_query
                }

                if result['law_id'] and result['law_name']:
                    results.append(result)

        except ET.ParseError as e:
            self.logger.error(f"ì¡°ì•½ XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def get_treaty_detail(self, treaty_id: str, treaty_name: str) -> Optional[Dict[str, Any]]:
        """ì¡°ì•½ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        params = {
            'OC': self.oc_code,
            'target': 'trty',
            'type': 'XML',
            'ID': treaty_id
        }

        try:
            response = self.session.get(
                self.config.LAW_DETAIL_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                return None

            return self._parse_treaty_detail(response.text, treaty_id, treaty_name)

        except Exception as e:
            self.logger.error(f"ì¡°ì•½ ìƒì„¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None

    def _parse_treaty_detail(self, content: str, treaty_id: str, treaty_name: str) -> Dict[str, Any]:
        """ì¡°ì•½ ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        detail = {
            'law_id': treaty_id,
            'law_msn': treaty_id,
            'law_name': treaty_name,
            'law_type': 'ì¡°ì•½',
            'treaty_no': '',
            'signing_date': '',
            'enforcement_date': '',
            'country': '',
            'treaty_type': '',
            'full_text': '',
            'articles': [],
            'data_type': 'treaty'
        }

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            detail['treaty_no'] = root.findtext('.//ì¡°ì•½ë²ˆí˜¸', '')
            detail['signing_date'] = root.findtext('.//ì„œëª…ì¼ì', '')
            detail['enforcement_date'] = root.findtext('.//ë°œíš¨ì¼ì', '')
            detail['country'] = root.findtext('.//ì²´ê²°êµ­ê°€', '')
            detail['treaty_type'] = root.findtext('.//ì¡°ì•½ìœ í˜•', '')
            detail['full_text'] = root.findtext('.//ì¡°ì•½ë³¸ë¬¸', '') or self._extract_full_text(root)

            # ì¡°ë¬¸ ì¶”ì¶œ
            self._extract_articles(root, detail)

        except Exception as e:
            self.logger.error(f"ì¡°ì•½ ìƒì„¸ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return detail

    # ===== ë²•ë ¹ ì²´ê³„ë„ ê²€ìƒ‰ ë©”ì„œë“œ =====
    def search_law_hierarchy_list(self, query: str) -> List[Dict[str, Any]]:
        """ë²•ë ¹ ì²´ê³„ë„ ëª©ë¡ ê²€ìƒ‰ (target=lsStmd)"""
        params = {
            'OC': self.oc_code,
            'target': 'lsStmd',
            'type': 'XML',
            'query': query,
            'display': self.config.RESULTS_PER_PAGE
        }

        try:
            time.sleep(self.config.DEFAULT_DELAY)
            response = self.session.get(
                self.config.LAW_SEARCH_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                self.logger.error(f"ì²´ê³„ë„ ëª©ë¡ ê²€ìƒ‰ ì‹¤íŒ¨: HTTP {response.status_code}")
                return []

            return self._parse_hierarchy_list_response(response.text, query)

        except Exception as e:
            self.logger.error(f"ì²´ê³„ë„ ëª©ë¡ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _parse_hierarchy_list_response(self, content: str, query: str) -> List[Dict[str, Any]]:
        """ë²•ë ¹ ì²´ê³„ë„ ëª©ë¡ ì‘ë‹µ íŒŒì‹±"""
        results = []

        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            for item in root.findall('.//law') or root.findall('.//lsStmd'):
                law_data = {
                    'law_id': item.findtext('ë²•ë ¹ID', '') or item.findtext('.//ë²•ë ¹ID', ''),
                    'law_msn': item.findtext('ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', '') or item.findtext('.//ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', ''),
                    'law_name': item.findtext('ë²•ë ¹ëª…', '') or item.findtext('.//ë²•ë ¹ëª…', ''),
                    'law_type': item.findtext('ë²•ë ¹êµ¬ë¶„ëª…', '') or item.findtext('.//ë²•ë ¹êµ¬ë¶„ëª…', ''),
                    'promulgation_date': item.findtext('ê³µí¬ì¼ì', '') or item.findtext('.//ê³µí¬ì¼ì', ''),
                    'enforcement_date': item.findtext('ì‹œí–‰ì¼ì', '') or item.findtext('.//ì‹œí–‰ì¼ì', ''),
                    'department': item.findtext('ì†Œê´€ë¶€ì²˜ëª…', '') or item.findtext('.//ì†Œê´€ë¶€ì²˜ëª…', ''),
                    'search_query': query,
                    'data_type': 'hierarchy'
                }

                if law_data['law_id'] or law_data['law_msn']:
                    results.append(law_data)

        except ET.ParseError as e:
            self.logger.error(f"ì²´ê³„ë„ ëª©ë¡ XML íŒŒì‹± ì˜¤ë¥˜: {e}")

        return results

    def get_law_hierarchy_detail(self, law_id: str = '', law_msn: str = '') -> Optional[Dict[str, Any]]:
        """ë²•ë ¹ ì²´ê³„ë„ ë³¸ë¬¸ ì¡°íšŒ (target=lsStmd) - ìƒí•˜ìœ„ë²• ì •ë³´ í¬í•¨"""
        params = {
            'OC': self.oc_code,
            'target': 'lsStmd',
            'type': 'XML'
        }

        if law_id:
            params['ID'] = law_id
        elif law_msn:
            params['MST'] = law_msn
        else:
            self.logger.error("ë²•ë ¹ ID ë˜ëŠ” MSTê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return None

        try:
            time.sleep(self.config.DEFAULT_DELAY)
            response = self.session.get(
                self.config.LAW_DETAIL_URL,
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code != 200:
                self.logger.error(f"ì²´ê³„ë„ ë³¸ë¬¸ ì¡°íšŒ ì‹¤íŒ¨: HTTP {response.status_code}")
                return None

            return self._parse_hierarchy_detail_response(response.text)

        except Exception as e:
            self.logger.error(f"ì²´ê³„ë„ ë³¸ë¬¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None

    def _parse_hierarchy_detail_response(self, content: str) -> Optional[Dict[str, Any]]:
        """ë²•ë ¹ ì²´ê³„ë„ ë³¸ë¬¸ ì‘ë‹µ íŒŒì‹± - ìƒí•˜ìœ„ë²• êµ¬ì¡° ì¶”ì¶œ"""
        try:
            content = self._preprocess_xml_content(content)
            root = ET.fromstring(content.encode('utf-8'))

            hierarchy = {
                'law_id': '',
                'law_msn': '',
                'law_name': '',
                'law_type': '',
                'enforcement_date': '',
                'promulgation_date': '',
                'related_laws': {
                    'laws': [],          # ë²•ë¥ 
                    'enforcement_decrees': [],  # ì‹œí–‰ë ¹
                    'enforcement_rules': [],    # ì‹œí–‰ê·œì¹™
                    'admin_rules': []    # í–‰ì •ê·œì¹™ (ê³ ì‹œ, í›ˆë ¹ ë“±)
                },
                'all_related_names': []  # ëª¨ë“  ê´€ë ¨ ë²•ë ¹ëª… ë¦¬ìŠ¤íŠ¸
            }

            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            basic_info = root.find('.//ê¸°ë³¸ì •ë³´') or root
            hierarchy['law_id'] = basic_info.findtext('.//ë²•ë ¹ID', '') or root.findtext('.//ë²•ë ¹ID', '')
            hierarchy['law_msn'] = basic_info.findtext('.//ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', '') or root.findtext('.//ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', '')
            hierarchy['law_name'] = basic_info.findtext('.//ë²•ë ¹ëª…', '') or root.findtext('.//ë²•ë ¹ëª…', '')
            hierarchy['law_type'] = basic_info.findtext('.//ë²•ì¢…êµ¬ë¶„', '') or root.findtext('.//ë²•ì¢…êµ¬ë¶„', '')
            hierarchy['enforcement_date'] = basic_info.findtext('.//ì‹œí–‰ì¼ì', '') or root.findtext('.//ì‹œí–‰ì¼ì', '')
            hierarchy['promulgation_date'] = basic_info.findtext('.//ê³µí¬ì¼ì', '') or root.findtext('.//ê³µí¬ì¼ì', '')

            # ìƒí•˜ìœ„ë²• ì •ë³´ ì¶”ì¶œ
            hierarchy_section = root.find('.//ìƒí•˜ìœ„ë²•') or root

            # ë²•ë¥  ì¶”ì¶œ
            for law_elem in hierarchy_section.findall('.//ë²•ë¥ ') or []:
                law_info = self._extract_hierarchy_law_info(law_elem, 'ë²•ë¥ ')
                if law_info:
                    hierarchy['related_laws']['laws'].append(law_info)
                    hierarchy['all_related_names'].append(law_info['name'])

            # ì‹œí–‰ë ¹ ì¶”ì¶œ
            for decree_elem in hierarchy_section.findall('.//ì‹œí–‰ë ¹') or []:
                decree_info = self._extract_hierarchy_law_info(decree_elem, 'ì‹œí–‰ë ¹')
                if decree_info:
                    hierarchy['related_laws']['enforcement_decrees'].append(decree_info)
                    hierarchy['all_related_names'].append(decree_info['name'])

            # ì‹œí–‰ê·œì¹™ ì¶”ì¶œ
            for rule_elem in hierarchy_section.findall('.//ì‹œí–‰ê·œì¹™') or []:
                rule_info = self._extract_hierarchy_law_info(rule_elem, 'ì‹œí–‰ê·œì¹™')
                if rule_info:
                    hierarchy['related_laws']['enforcement_rules'].append(rule_info)
                    hierarchy['all_related_names'].append(rule_info['name'])

            # í–‰ì •ê·œì¹™ ì¶”ì¶œ (ê³ ì‹œ, í›ˆë ¹ ë“±)
            for admin_type in ['í–‰ì •ê·œì¹™', 'ê³ ì‹œ', 'í›ˆë ¹', 'ì˜ˆê·œ', 'ê¸°íƒ€']:
                for admin_elem in hierarchy_section.findall(f'.//{admin_type}') or []:
                    admin_info = self._extract_hierarchy_law_info(admin_elem, admin_type)
                    if admin_info:
                        hierarchy['related_laws']['admin_rules'].append(admin_info)
                        hierarchy['all_related_names'].append(admin_info['name'])

            # ë³¸ë¬¸ì—ì„œ ì¶”ê°€ ë²•ë ¹ ì •ë³´ ì¶”ì¶œ (í…ìŠ¤íŠ¸ íŒŒì‹±)
            self._extract_additional_hierarchy_laws(root, hierarchy)

            return hierarchy

        except ET.ParseError as e:
            self.logger.error(f"ì²´ê³„ë„ ë³¸ë¬¸ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            self.logger.error(f"ì²´ê³„ë„ ë³¸ë¬¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None

    def _extract_hierarchy_law_info(self, elem: ET.Element, law_type: str) -> Optional[Dict[str, str]]:
        """ì²´ê³„ë„ì—ì„œ ê°œë³„ ë²•ë ¹ ì •ë³´ ì¶”ì¶œ"""
        # í…ìŠ¤íŠ¸ë¡œ ë²•ë ¹ëª… ì¶”ì¶œ ì‹œë„
        law_name = elem.text.strip() if elem.text else ''

        # í•˜ìœ„ ìš”ì†Œì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ ì‹œë„
        if not law_name:
            law_name = elem.findtext('.//ë²•ë ¹ëª…', '') or elem.findtext('.//ëª…ì¹­', '')

        # ì†ì„±ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ ì‹œë„
        if not law_name:
            law_name = elem.get('ë²•ë ¹ëª…', '') or elem.get('ëª…ì¹­', '')

        if not law_name:
            return None

        return {
            'name': law_name.strip(),
            'type': law_type,
            'id': elem.findtext('.//ë²•ë ¹ID', '') or elem.get('ë²•ë ¹ID', ''),
            'msn': elem.findtext('.//ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', '') or elem.get('ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', ''),
            'enforcement_date': elem.findtext('.//ì‹œí–‰ì¼ì', '') or elem.get('ì‹œí–‰ì¼ì', ''),
            'promulgation_date': elem.findtext('.//ê³µí¬ì¼ì', '') or elem.get('ê³µí¬ì¼ì', '')
        }

    def _extract_additional_hierarchy_laws(self, root: ET.Element, hierarchy: Dict[str, Any]):
        """XML ì „ì²´ì—ì„œ ì¶”ê°€ ë²•ë ¹ ì •ë³´ ì¶”ì¶œ"""
        # ë³¸ë¬¸ì—ì„œ í–‰ì •ê·œì¹™ ì •ë³´ ì¶”ì¶œ (ë‹¤ì–‘í•œ êµ¬ì¡° ì§€ì›)
        admin_patterns = ['í–‰ì •ê·œì¹™', 'í•˜ìœ„í–‰ì •ê·œì¹™', 'ê´€ë ¨í–‰ì •ê·œì¹™', 'ìœ„ì„í–‰ì •ê·œì¹™']

        for pattern in admin_patterns:
            section = root.find(f'.//{pattern}')
            if section is not None:
                # ì„¹ì…˜ ë‚´ì˜ ëª¨ë“  í•­ëª© ê²€ìƒ‰
                for child in section:
                    if child.text and child.text.strip():
                        name = child.text.strip()
                        if name not in hierarchy['all_related_names']:
                            tag_name = child.tag if child.tag else 'í–‰ì •ê·œì¹™'
                            hierarchy['related_laws']['admin_rules'].append({
                                'name': name,
                                'type': tag_name,
                                'id': child.get('ë²•ë ¹ID', '') or child.findtext('.//ë²•ë ¹ID', ''),
                                'msn': child.get('í–‰ì •ê·œì¹™ì¼ë ¨ë²ˆí˜¸', '') or child.findtext('.//í–‰ì •ê·œì¹™ì¼ë ¨ë²ˆí˜¸', ''),
                                'enforcement_date': '',
                                'promulgation_date': ''
                            })
                            hierarchy['all_related_names'].append(name)

    def _extract_law_keywords(self, law_name: str) -> List[str]:
        """ë²•ë ¹ëª…ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (í–‰ì •ê·œì¹™ ê²€ìƒ‰ìš©)"""
        keywords = []

        # ë²•, ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ ë“± ì ‘ë¯¸ì‚¬ ì œê±°
        suffixes = ['ë²•', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™', 'ê·œì •', 'ê·œì¹™', 'ì§€ì¹¨', 'ê³ ì‹œ', 'í›ˆë ¹', 'ì˜ˆê·œ']
        base_name = law_name
        for suffix in suffixes:
            if base_name.endswith(suffix):
                base_name = base_name[:-len(suffix)]
                break

        if base_name:
            keywords.append(base_name)

        # ë„ì–´ì“°ê¸° ì—†ëŠ” ë²„ì „ê³¼ ìˆëŠ” ë²„ì „ ëª¨ë‘ ì‹œë„
        if ' ' in base_name:
            keywords.append(base_name.replace(' ', ''))

        return keywords

    def _search_related_admin_rules(self, law_name: str, seen_ids: set) -> List[Dict[str, Any]]:
        """ë²•ë ¹ëª… í‚¤ì›Œë“œë¡œ ê´€ë ¨ í–‰ì •ê·œì¹™ ê²€ìƒ‰"""
        results = []
        keywords = self._extract_law_keywords(law_name)

        self.logger.info(f"í–‰ì •ê·œì¹™ í‚¤ì›Œë“œ ê²€ìƒ‰: {keywords}")

        for keyword in keywords:
            if len(keyword) < 2:
                continue

            try:
                params = {
                    'OC': self.oc_code,
                    'target': 'admrul',
                    'type': 'XML',
                    'query': keyword,
                    'display': '100',
                    'page': '1'
                }

                response = self.session.get(
                    self.config.ADMIN_RULE_SEARCH_URL,
                    params=params,
                    timeout=self.config.TIMEOUT
                )

                if response.status_code == 200:
                    rules = self._parse_admin_rule_search_response(response.text, keyword)

                    for rule in rules:
                        rule_id = rule.get('law_id', '')
                        if rule_id and rule_id not in seen_ids:
                            # í‚¤ì›Œë“œê°€ ì‹¤ì œë¡œ ê·œì¹™ëª…ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                            rule_name = rule.get('law_name', '')
                            if keyword in rule_name:
                                seen_ids.add(rule_id)
                                rule['hierarchy_source'] = f'ê´€ë ¨ í–‰ì •ê·œì¹™ ({keyword})'
                                results.append(rule)
                                self.logger.info(f"ê´€ë ¨ í–‰ì •ê·œì¹™ ë°œê²¬: {rule_name}")

            except Exception as e:
                self.logger.error(f"í–‰ì •ê·œì¹™ í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")

        return results

    def _get_delegated_admin_rules(self, law_id: str) -> List[str]:
        """ìœ„ì„ë²•ë ¹ ì¡°íšŒ APIë¥¼ í†µí•´ ìœ„ì„ëœ í–‰ì •ê·œì¹™ ëª©ë¡ ì¡°íšŒ"""
        delegated_rules = []

        if not law_id:
            return delegated_rules

        try:
            params = {
                'OC': self.oc_code,
                'target': 'lsDelegated',
                'type': 'XML',
                'ID': law_id
            }

            self.logger.info(f"ìœ„ì„ë²•ë ¹ ì¡°íšŒ: ID={law_id}")

            response = self.session.get(
                'http://www.law.go.kr/DRF/lawService.do',
                params=params,
                timeout=self.config.TIMEOUT
            )

            if response.status_code == 200:
                content = self._preprocess_xml_content(response.text)
                root = ET.fromstring(content.encode('utf-8'))

                # ìœ„ì„í–‰ì •ê·œì¹™ì œëª© ì¶”ì¶œ
                for elem in root.findall('.//ìœ„ì„í–‰ì •ê·œì¹™ì œëª©'):
                    if elem.text and elem.text.strip():
                        rule_name = elem.text.strip()
                        if rule_name not in delegated_rules:
                            delegated_rules.append(rule_name)
                            self.logger.info(f"ìœ„ì„ í–‰ì •ê·œì¹™ ë°œê²¬: {rule_name}")

                # ìœ„ì„ë²•ë ¹ì œëª©ë„ ì¶”ì¶œ (ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ ë“±)
                for elem in root.findall('.//ìœ„ì„ë²•ë ¹ì œëª©'):
                    if elem.text and elem.text.strip():
                        rule_name = elem.text.strip()
                        if rule_name not in delegated_rules:
                            delegated_rules.append(rule_name)
                            self.logger.info(f"ìœ„ì„ ë²•ë ¹ ë°œê²¬: {rule_name}")

        except ET.ParseError as e:
            self.logger.error(f"ìœ„ì„ë²•ë ¹ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            self.logger.error(f"ìœ„ì„ë²•ë ¹ ì¡°íšŒ ì˜¤ë¥˜: {e}")

        return delegated_rules

    def _search_delegated_rules(self, law_id: str, seen_ids: set) -> List[Dict[str, Any]]:
        """ìœ„ì„ëœ í–‰ì •ê·œì¹™ì„ ê²€ìƒ‰í•˜ì—¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        results = []

        # ìœ„ì„ëœ ë²•ë ¹/í–‰ì •ê·œì¹™ ëª©ë¡ ì¡°íšŒ
        delegated_names = self._get_delegated_admin_rules(law_id)

        self.logger.info(f"ìœ„ì„ ë²•ë ¹/í–‰ì •ê·œì¹™ {len(delegated_names)}ê°œ ë°œê²¬")

        for rule_name in delegated_names:
            # ë¨¼ì € í–‰ì •ê·œì¹™ìœ¼ë¡œ ê²€ìƒ‰
            search_results = self._search_admin_rule(rule_name)

            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ë²•ë ¹ìœ¼ë¡œ ê²€ìƒ‰
            if not search_results:
                search_results = self._search_exact_match(rule_name)

            if not search_results:
                search_results = self._search_general_law(rule_name)

            for rule in search_results:
                rule_id = rule.get('law_id', '')
                if rule_id and rule_id not in seen_ids:
                    seen_ids.add(rule_id)
                    rule['hierarchy_source'] = f'ìœ„ì„ ë²•ë ¹ ({rule_name})'
                    results.append(rule)
                    self.logger.info(f"ìœ„ì„ ë²•ë ¹ ì¶”ê°€: {rule.get('law_name', '')}")

        return results

    def search_with_hierarchy(self, query: str, progress_callback=None) -> Dict[str, Any]:
        """ë²•ë ¹ ì²´ê³„ë„ ê¸°ë°˜ í†µí•© ê²€ìƒ‰ - ìƒìœ„ë²•ê³¼ ëª¨ë“  í•˜ìœ„ë²•ë ¹ì„ í•¨ê»˜ ê²€ìƒ‰"""
        result = {
            'query': query,
            'hierarchy_info': None,
            'laws': [],
            'search_summary': {
                'total': 0,
                'laws_count': 0,
                'decrees_count': 0,
                'rules_count': 0,
                'admin_rules_count': 0
            }
        }

        # Step 1: ì²´ê³„ë„ ëª©ë¡ ê²€ìƒ‰
        if progress_callback:
            progress_callback(0.1, "ì²´ê³„ë„ ëª©ë¡ ê²€ìƒ‰ ì¤‘...")

        hierarchy_list = self.search_law_hierarchy_list(query)

        if not hierarchy_list:
            self.logger.warning(f"ì²´ê³„ë„ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {query}")
            # ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
            return self._fallback_to_regular_search(query, progress_callback)

        # Step 2: ì²´ê³„ë„ ë³¸ë¬¸ ì¡°íšŒ (ê°€ì¥ ìœ ì‚¬í•œ ê²°ê³¼ ì‚¬ìš©)
        if progress_callback:
            progress_callback(0.2, "ì²´ê³„ë„ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘...")

        target_law = self._find_best_match(hierarchy_list, query)
        hierarchy_detail = self.get_law_hierarchy_detail(
            law_id=target_law.get('law_id', ''),
            law_msn=target_law.get('law_msn', '')
        )

        if not hierarchy_detail:
            self.logger.warning("ì²´ê³„ë„ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨")
            return self._fallback_to_regular_search(query, progress_callback)

        result['hierarchy_info'] = hierarchy_detail

        # Step 3: ì²´ê³„ë„ì—ì„œ ì¶”ì¶œí•œ ëª¨ë“  ë²•ë ¹ ê²€ìƒ‰
        all_law_names = hierarchy_detail.get('all_related_names', [])

        # ê¸°ë³¸ ë²•ë ¹ëª…ë„ ì¶”ê°€
        if hierarchy_detail.get('law_name'):
            if hierarchy_detail['law_name'] not in all_law_names:
                all_law_names.insert(0, hierarchy_detail['law_name'])

        if not all_law_names:
            all_law_names = [query]

        self.logger.info(f"ì²´ê³„ë„ì—ì„œ {len(all_law_names)}ê°œ ë²•ë ¹ ë°œê²¬: {all_law_names}")

        # Step 4: ê° ë²•ë ¹ ê²€ìƒ‰ ë° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        if progress_callback:
            progress_callback(0.3, f"{len(all_law_names)}ê°œ ë²•ë ¹ ê²€ìƒ‰ ì¤‘...")

        collected_laws = []
        seen_ids = set()

        for idx, law_name in enumerate(all_law_names):
            if progress_callback:
                progress = 0.3 + (0.6 * (idx + 1) / len(all_law_names))
                progress_callback(progress, f"ê²€ìƒ‰ ì¤‘: {law_name}")

            # ë²•ë ¹ ê²€ìƒ‰ (ì •í™•í•œ ë§¤ì¹­ ì‚¬ìš©)
            search_results = self._search_exact_match(law_name)

            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ ì‹œë„
            if not search_results:
                search_results = self._search_general_law(law_name)

            # í–‰ì •ê·œì¹™ ê²€ìƒ‰ë„ ì‹œë„
            if not search_results:
                search_results = self._search_admin_rule(law_name)

            for law in search_results:
                if law['law_id'] not in seen_ids:
                    seen_ids.add(law['law_id'])
                    law['hierarchy_source'] = law_name
                    collected_laws.append(law)

        # Step 5: ìœ„ì„ë²•ë ¹ ì¡°íšŒ APIë¥¼ í†µí•œ ìœ„ì„ í–‰ì •ê·œì¹™ ê²€ìƒ‰
        # ë²•ë¥ , ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ ëª¨ë‘ì— ëŒ€í•´ ìœ„ì„ë²•ë ¹ ì¡°íšŒ ìˆ˜í–‰
        if progress_callback:
            progress_callback(0.75, "ìœ„ì„ ë²•ë ¹/í–‰ì •ê·œì¹™ ê²€ìƒ‰ ì¤‘...")

        # ìˆ˜ì§‘ëœ ëª¨ë“  ë²•ë ¹ì˜ ID ëª©ë¡ (ë²•ë¥ , ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™)
        law_ids_to_check = []

        # ê¸°ë³¸ ë²•ë ¹ ID ì¶”ê°€
        main_law_id = hierarchy_detail.get('law_id', '') or target_law.get('law_id', '')
        if main_law_id:
            law_ids_to_check.append(('ê¸°ë³¸ ë²•ë ¹', main_law_id))

        # ìˆ˜ì§‘ëœ ë²•ë ¹ë“¤ì˜ ID ì¶”ê°€ (ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ í¬í•¨)
        for law in collected_laws:
            law_id = law.get('law_id', '')
            law_name = law.get('law_name', '')
            if law_id and law_id not in [lid for _, lid in law_ids_to_check]:
                # í–‰ì •ê·œì¹™ì´ ì•„ë‹Œ ë²•ë ¹ë§Œ ì¶”ê°€ (ë²•ë¥ , ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™)
                if not law.get('is_admin_rule', False):
                    law_ids_to_check.append((law_name, law_id))

        self.logger.info(f"ìœ„ì„ë²•ë ¹ ì¡°íšŒ ëŒ€ìƒ: {len(law_ids_to_check)}ê°œ ë²•ë ¹")

        # ê° ë²•ë ¹ì— ëŒ€í•´ ìœ„ì„ë²•ë ¹ ì¡°íšŒ
        total_delegated = 0
        for idx, (source_name, law_id) in enumerate(law_ids_to_check):
            if progress_callback:
                progress = 0.75 + (0.15 * (idx + 1) / max(len(law_ids_to_check), 1))
                progress_callback(progress, f"ìœ„ì„ë²•ë ¹ ì¡°íšŒ ì¤‘: {source_name[:20]}...")

            delegated_rules = self._search_delegated_rules(law_id, seen_ids)
            if delegated_rules:
                self.logger.info(f"{source_name}ì˜ ìœ„ì„ ë²•ë ¹/í–‰ì •ê·œì¹™ {len(delegated_rules)}ê°œ ì¶”ê°€")
                # ì¶œì²˜ ì •ë³´ ì—…ë°ì´íŠ¸
                for rule in delegated_rules:
                    rule['hierarchy_source'] = f"ìœ„ì„ ({source_name})"
                collected_laws.extend(delegated_rules)
                total_delegated += len(delegated_rules)

        self.logger.info(f"ì´ ìœ„ì„ ë²•ë ¹/í–‰ì •ê·œì¹™ {total_delegated}ê°œ ì¶”ê°€")

        # Step 6: ê´€ë ¨ í–‰ì •ê·œì¹™ í‚¤ì›Œë“œ ê²€ìƒ‰ (ë²•ë ¹ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰)
        if progress_callback:
            progress_callback(0.92, "ê´€ë ¨ í–‰ì •ê·œì¹™ ê²€ìƒ‰ ì¤‘...")

        # ê¸°ë³¸ ë²•ë ¹ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ í–‰ì •ê·œì¹™ ê²€ìƒ‰
        main_law_name = hierarchy_detail.get('law_name', query)
        related_admin_rules = self._search_related_admin_rules(main_law_name, seen_ids)

        if related_admin_rules:
            self.logger.info(f"ê´€ë ¨ í–‰ì •ê·œì¹™ {len(related_admin_rules)}ê°œ ì¶”ê°€")
            collected_laws.extend(related_admin_rules)

        result['laws'] = collected_laws

        # í†µê³„ ì—…ë°ì´íŠ¸
        result['search_summary']['total'] = len(collected_laws)
        for law in collected_laws:
            law_type = law.get('law_type', '')
            if law.get('is_admin_rule'):
                result['search_summary']['admin_rules_count'] += 1
            elif 'ì‹œí–‰ë ¹' in law_type or 'ì‹œí–‰ë ¹' in law.get('law_name', ''):
                result['search_summary']['decrees_count'] += 1
            elif 'ì‹œí–‰ê·œì¹™' in law_type or 'ì‹œí–‰ê·œì¹™' in law.get('law_name', ''):
                result['search_summary']['rules_count'] += 1
            else:
                result['search_summary']['laws_count'] += 1

        if progress_callback:
            progress_callback(1.0, "ê²€ìƒ‰ ì™„ë£Œ")

        return result

    def _find_best_match(self, results: List[Dict[str, Any]], query: str) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ í•­ëª© ì°¾ê¸°"""
        if not results:
            return {}

        best_match = results[0]
        best_similarity = 0

        for result in results:
            law_name = result.get('law_name', '')
            similarity = self._calculate_similarity(query, law_name)

            if similarity > best_similarity:
                best_similarity = similarity
                best_match = result

        return best_match

    def _fallback_to_regular_search(self, query: str, progress_callback=None) -> Dict[str, Any]:
        """ì²´ê³„ë„ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±"""
        if progress_callback:
            progress_callback(0.5, "ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜...")

        result = {
            'query': query,
            'hierarchy_info': None,
            'laws': [],
            'search_summary': {
                'total': 0,
                'laws_count': 0,
                'decrees_count': 0,
                'rules_count': 0,
                'admin_rules_count': 0
            },
            'fallback': True
        }

        # ì¼ë°˜ ë²•ë ¹ ê²€ìƒ‰
        laws = self._search_with_variations(query)
        result['laws'] = laws
        result['search_summary']['total'] = len(laws)

        for law in laws:
            if law.get('is_admin_rule'):
                result['search_summary']['admin_rules_count'] += 1
            else:
                result['search_summary']['laws_count'] += 1

        if progress_callback:
            progress_callback(1.0, "ê²€ìƒ‰ ì™„ë£Œ")

        return result

    # ===== í†µí•© ê²€ìƒ‰ ë©”ì„œë“œ =====
    def search_by_type(self, query: str, data_type: str) -> List[Dict[str, Any]]:
        """ë°ì´í„° ìœ í˜•ë³„ ê²€ìƒ‰"""
        search_methods = {
            'law': lambda q: self.search_single_law(q),
            'ordinance': self.search_ordinance,
            'precedent': self.search_precedent,
            'constitutional': self.search_constitutional_decision,
            'interpretation': self.search_interpretation,
            'admin_decision': self.search_admin_decision,
            'treaty': self.search_treaty
        }

        method = search_methods.get(data_type)
        if method:
            return method(query)
        else:
            self.logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° ìœ í˜•: {data_type}")
            return []

    def get_detail_by_type(self, item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ë°ì´í„° ìœ í˜•ë³„ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        data_type = item.get('data_type', 'law')

        if data_type == 'ordinance':
            return self.get_ordinance_detail(
                item['law_id'], item.get('law_msn', ''), item['law_name']
            )
        elif data_type == 'precedent':
            return self.get_precedent_detail(item['law_id'], item['law_name'])
        elif data_type == 'constitutional':
            return self.get_constitutional_detail(item['law_id'], item['law_name'])
        elif data_type == 'interpretation':
            return self.get_interpretation_detail(item['law_id'], item['law_name'])
        elif data_type == 'admin_decision':
            return self.get_admin_decision_detail(item['law_id'], item['law_name'])
        elif data_type == 'treaty':
            return self.get_treaty_detail(item['law_id'], item['law_name'])
        else:
            # ê¸°ë³¸: ë²•ë ¹/í–‰ì •ê·œì¹™
            return self._get_law_detail(
                item['law_id'],
                item.get('law_msn', ''),
                item['law_name'],
                item.get('is_admin_rule', False)
            )


# ===== ë²•ë ¹ ë‚´ë³´ë‚´ê¸° í´ë˜ìŠ¤ =====
class LawExporter:
    """ë²•ë ¹ ë‚´ë³´ë‚´ê¸° í´ë˜ìŠ¤ - PDF ì§€ì› ìˆ˜ì •"""
    
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def export_to_zip(self, laws_dict: Dict[str, Dict[str, Any]],
                     include_pdfs: bool = False) -> bytes:
        """ZIP íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° - OCR í…ìŠ¤íŠ¸ í¬í•¨"""
        zip_buffer = BytesIO()

        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # ë©”íƒ€ë°ì´í„°
            metadata = {
                'collection_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_laws': len(laws_dict),
                'admin_rule_count': sum(1 for law in laws_dict.values() if law.get('is_admin_rule', False)),
                'attachment_count': sum(len(law.get('attachments', [])) for law in laws_dict.values()),
                'laws': laws_dict
            }
            
            # ì „ì²´ JSON
            zip_file.writestr(
                'all_laws.json',
                json.dumps(metadata, ensure_ascii=False, indent=2)
            )
            
            # ì „ì²´ Markdown
            all_laws_md = self._create_all_laws_markdown(laws_dict)
            zip_file.writestr('all_laws.md', all_laws_md)
            
            # ê°œë³„ íŒŒì¼
            for law_id, law in laws_dict.items():
                safe_name = self._sanitize_filename(law['law_name'])
                
                # JSON
                zip_file.writestr(
                    f'laws/{safe_name}.json',
                    json.dumps(law, ensure_ascii=False, indent=2)
                )
                
                # í…ìŠ¤íŠ¸
                text_content = self._format_law_text(law)
                zip_file.writestr(f'laws/{safe_name}.txt', text_content)
                
                # Markdown
                md_content = self._format_law_markdown(law)
                zip_file.writestr(f'laws/{safe_name}.md', md_content)
            
            # README
            readme = self._create_readme(laws_dict, include_pdfs)
            zip_file.writestr('README.md', readme)
        
        zip_buffer.seek(0)
        return zip_buffer.getvalue()

    def export_markdown_by_file(self,
                                grouped_laws: Dict[str, Dict[str, Dict[str, Any]]],
                                file_metadata: Dict[str, Dict[str, Any]]) -> bytes:
        """íŒŒì¼ë³„ë¡œ í†µí•©ëœ Markdown ë²ˆë“¤ì„ ZIPìœ¼ë¡œ ë°˜í™˜"""
        zip_buffer = BytesIO()

        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for file_key, laws in grouped_laws.items():
                if not laws:
                    continue

                meta = file_metadata.get(file_key, {})
                file_name = meta.get('file_name') or ("ì§ì ‘_ê²€ìƒ‰" if file_key == 'direct_input' else file_key)
                safe_name = self._sanitize_filename(file_name)
                markdown_content = self._create_all_laws_markdown(laws)
                zip_file.writestr(f'{safe_name}.md', markdown_content)

        zip_buffer.seek(0)
        return zip_buffer.getvalue()
    
    def export_single_file(self, laws_dict: Dict[str, Dict[str, Any]], 
                          format: str = 'json') -> str:
        """ë‹¨ì¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° - ëª¨ë“  í˜•ì‹ ì§€ì›"""
        exporters = {
            'json': self._export_as_json,
            'markdown': self._export_as_markdown,
            'text': self._export_as_text
        }
        
        exporter = exporters.get(format.lower(), self._export_as_json)
        return exporter(laws_dict)
    
    def _sanitize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª… ì•ˆì „í•˜ê²Œ ë³€í™˜"""
        return re.sub(r'[\\/*?:"<>|]', '_', filename)
    
    def _export_as_json(self, laws_dict: Dict[str, Dict[str, Any]]) -> str:
        """JSON í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        data = {
            'collection_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_laws': len(laws_dict),
            'laws': laws_dict
        }
        return json.dumps(data, ensure_ascii=False, indent=2)
    
    def _export_as_markdown(self, laws_dict: Dict[str, Dict[str, Any]]) -> str:
        """Markdown í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        return self._create_all_laws_markdown(laws_dict)
    
    def _export_as_text(self, laws_dict: Dict[str, Dict[str, Any]]) -> str:
        """í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        lines = []
        lines.append("ë²•ë ¹ ìˆ˜ì§‘ ê²°ê³¼")
        lines.append(f"ìˆ˜ì§‘ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"ì´ ë²•ë ¹ ìˆ˜: {len(laws_dict)}ê°œ")
        lines.append("=" * 80 + "\n")
        
        for law_id, law in laws_dict.items():
            lines.append(self._format_law_text(law))
            lines.append("\n" + "=" * 80 + "\n")
            
        return '\n'.join(lines)
    
    def _format_law_text(self, law: Dict[str, Any]) -> str:
        """ë²•ë ¹ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·"""
        lines = []
        
        # í—¤ë”
        lines.append(f"ë²•ë ¹ëª…: {law['law_name']}")
        lines.append(f"ë²•ì¢…êµ¬ë¶„: {law.get('law_type', '')}")
        if law.get('department'):
            lines.append(f"ì†Œê´€ë¶€ì²˜: {law.get('department', '')}")
        lines.append(f"ê³µí¬ì¼ì: {law.get('promulgation_date', '')}")
        lines.append(f"ì‹œí–‰ì¼ì: {law.get('enforcement_date', '')}")
        
        # ë³„í‘œ/ë³„ì²¨ ê°œìˆ˜
        if law.get('attachments'):
            lines.append(f"ë³„í‘œ/ë³„ì²¨: {len(law['attachments'])}ê°œ")
        
        lines.append("-" * 60)
        
        # ì¡°ë¬¸
        if law.get('articles'):
            lines.append("\nã€ì¡° ë¬¸ã€‘\n")
            for article in law['articles']:
                lines.append(f"{article['number']} {article.get('title', '')}")
                lines.append(article['content'])
                
                if article.get('paragraphs'):
                    for para in article['paragraphs']:
                        lines.append(f"  {para['number']} {para['content']}")
                lines.append("")
        
        # ë¶€ì¹™
        if law.get('supplementary_provisions'):
            lines.append("\nã€ë¶€ ì¹™ã€‘\n")
            for provision in law['supplementary_provisions']:
                if provision.get('promulgation_date'):
                    lines.append(f"ë¶€ì¹™ <{provision['promulgation_date']}>")
                lines.append(provision['content'])
                lines.append("")
        
        # ë³„í‘œ
        if law.get('attachments'):
            lines.append("\nã€ë³„í‘œ/ë³„ì²¨ã€‘\n")
            for attachment in law['attachments']:
                lines.append(f"[{attachment['type']}] {attachment.get('title', '')}")
                lines.append(attachment['content'])
                lines.append("")
        
        # ì›ë¬¸ (ì¡°ë¬¸ì´ ì—†ëŠ” ê²½ìš°)
        if not law.get('articles') and law.get('raw_content'):
            lines.append("\nã€ì› ë¬¸ã€‘\n")
            lines.append(law['raw_content'])
        
        return '\n'.join(lines)
    
    def _format_law_markdown(self, law: Dict[str, Any]) -> str:
        """ë²•ë ¹ì„ Markdownìœ¼ë¡œ í¬ë§·"""
        lines = []
        
        # ì œëª©
        lines.append(f"# {law['law_name']}\n")
        
        # ê¸°ë³¸ ì •ë³´
        lines.append("## ğŸ“‹ ê¸°ë³¸ ì •ë³´\n")
        lines.append(f"- **ë²•ì¢…êµ¬ë¶„**: {law.get('law_type', '')}")
        if law.get('department'):
            lines.append(f"- **ì†Œê´€ë¶€ì²˜**: {law.get('department', '')}")
        lines.append(f"- **ê³µí¬ì¼ì**: {law.get('promulgation_date', '')}")
        lines.append(f"- **ì‹œí–‰ì¼ì**: {law.get('enforcement_date', '')}")
        
        # ë³„í‘œ/ë³„ì²¨ ì •ë³´
        if law.get('attachments'):
            lines.append(f"- **ë³„í‘œ/ë³„ì²¨**: {len(law['attachments'])}ê°œ")
        
        lines.append("")
        
        # ì¡°ë¬¸
        if law.get('articles'):
            lines.append("## ğŸ“– ì¡°ë¬¸\n")
            for article in law['articles']:
                lines.append(f"### {article['number']}")
                if article.get('title'):
                    lines.append(f"**{article['title']}**\n")
                lines.append(article['content'])
                
                if article.get('paragraphs'):
                    for para in article['paragraphs']:
                        lines.append(f"\n> {para['number']} {para['content']}")
                lines.append("")
        
        # ë¶€ì¹™
        if law.get('supplementary_provisions'):
            lines.append("## ğŸ“Œ ë¶€ì¹™\n")
            for provision in law['supplementary_provisions']:
                if provision.get('promulgation_date'):
                    lines.append(f"### ë¶€ì¹™ <{provision['promulgation_date']}>")
                lines.append(provision['content'])
                lines.append("")
        
        # ë³„í‘œ
        if law.get('attachments'):
            lines.append("## ğŸ“ ë³„í‘œ/ë³„ì²¨\n")
            for attachment in law['attachments']:
                lines.append(f"### [{attachment['type']}] {attachment.get('title', '')}")
                lines.append(attachment['content'])
                lines.append("")
        
        return '\n'.join(lines)
    
    def _create_all_laws_markdown(self, laws_dict: Dict[str, Dict[str, Any]]) -> str:
        """ì „ì²´ ë²•ë ¹ Markdown ìƒì„±"""
        lines = []
        
        # í—¤ë”
        lines.append("# ğŸ“š ë²•ë ¹ ìˆ˜ì§‘ ê²°ê³¼\n")
        lines.append(f"**ìˆ˜ì§‘ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"**ì´ ë²•ë ¹ ìˆ˜**: {len(laws_dict)}ê°œ")
        
        # í†µê³„
        admin_rule_count = sum(1 for law in laws_dict.values() if law.get('is_admin_rule', False))
        attachment_count = sum(len(law.get('attachments', [])) for law in laws_dict.values())
        
        if admin_rule_count > 0:
            lines.append(f"**í–‰ì •ê·œì¹™ ìˆ˜**: {admin_rule_count}ê°œ")
        if attachment_count > 0:
            lines.append(f"**ë³„í‘œ/ë³„ì²¨ ì´ê³„**: {attachment_count}ê°œ")
        
        lines.append("")
        
        # ëª©ì°¨
        lines.append("## ğŸ“‘ ëª©ì°¨\n")
        for idx, (law_id, law) in enumerate(laws_dict.items(), 1):
            anchor = self._sanitize_filename(law['law_name'])
            type_emoji = "ğŸ“‹" if law.get('is_admin_rule', False) else "ğŸ“–"
            attachment_mark = " ğŸ“" if law.get('attachments') else ""
            lines.append(f"{idx}. {type_emoji} [{law['law_name']}](#{anchor}){attachment_mark}")
        lines.append("\n---\n")
        
        # ê° ë²•ë ¹
        for law_id, law in laws_dict.items():
            lines.append(self._format_law_markdown(law))
            lines.append("\n---\n")
            
        return '\n'.join(lines)
    
    def _create_readme(self, laws_dict: Dict[str, Dict[str, Any]], 
                      include_pdfs: bool = False) -> str:
        """README ìƒì„±"""
        # í†µê³„ ê³„ì‚°
        total_articles = sum(len(law.get('articles', [])) for law in laws_dict.values())
        total_provisions = sum(len(law.get('supplementary_provisions', [])) for law in laws_dict.values())
        total_attachments = sum(len(law.get('attachments', [])) for law in laws_dict.values())
        admin_rule_count = sum(1 for law in laws_dict.values() if law.get('is_admin_rule', False))
        
        content = f"""# ë²•ë ¹ ìˆ˜ì§‘ ê²°ê³¼

ìˆ˜ì§‘ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ì´ ë²•ë ¹ ìˆ˜: {len(laws_dict)}ê°œ

## ğŸ“ íŒŒì¼ êµ¬ì¡°

- `all_laws.json`: ì „ì²´ ë²•ë ¹ ë°ì´í„° (JSON)
- `all_laws.md`: ì „ì²´ ë²•ë ¹ í†µí•© ë¬¸ì„œ (Markdown)
- `laws/`: ê°œë³„ ë²•ë ¹ íŒŒì¼
  - `*.json`: ë²•ë ¹ë³„ ìƒì„¸ ë°ì´í„°
  - `*.txt`: ë²•ë ¹ë³„ í…ìŠ¤íŠ¸
  - `*.md`: ë²•ë ¹ë³„ Markdown
- `README.md`: ì´ íŒŒì¼

## ğŸ“Š í†µê³„

- ì´ ì¡°ë¬¸ ìˆ˜: {total_articles:,}ê°œ
- ì´ ë¶€ì¹™ ìˆ˜: {total_provisions}ê°œ
- ì´ ë³„í‘œ/ë³„ì²¨ ìˆ˜: {total_attachments}ê°œ
- í–‰ì •ê·œì¹™ ìˆ˜: {admin_rule_count}ê°œ

## ğŸ“– ìˆ˜ì§‘ëœ ë²•ë ¹ ëª©ë¡

"""
        
        # ì¼ë°˜ ë²•ë ¹ê³¼ í–‰ì •ê·œì¹™ ë¶„ë¦¬
        general_laws = []
        admin_rules = []
        
        for law_id, law in laws_dict.items():
            if law.get('is_admin_rule', False):
                admin_rules.append((law_id, law))
            else:
                general_laws.append((law_id, law))
        
        # ì¼ë°˜ ë²•ë ¹ ëª©ë¡
        if general_laws:
            content += "\n### ğŸ“– ì¼ë°˜ ë²•ë ¹\n\n"
            for law_id, law in general_laws:
                content += f"#### {law['law_name']}\n"
                content += f"- ë²•ì¢…êµ¬ë¶„: {law.get('law_type', '')}\n"
                content += f"- ì‹œí–‰ì¼ì: {law.get('enforcement_date', '')}\n"
                content += f"- ì¡°ë¬¸: {len(law.get('articles', []))}ê°œ\n"
                if law.get('attachments'):
                    content += f"- ë³„í‘œ/ë³„ì²¨: {len(law['attachments'])}ê°œ\n"
                content += "\n"
        
        # í–‰ì •ê·œì¹™ ëª©ë¡
        if admin_rules:
            content += "\n### ğŸ“‹ í–‰ì •ê·œì¹™\n\n"
            for law_id, law in admin_rules:
                content += f"#### {law['law_name']}\n"
                content += f"- ìœ í˜•: {law.get('law_type', '')}\n"
                if law.get('department'):
                    content += f"- ì†Œê´€ë¶€ì²˜: {law.get('department', '')}\n"
                content += f"- ì‹œí–‰ì¼ì: {law.get('enforcement_date', '')}\n"
                content += f"- ì¡°ë¬¸: {len(law.get('articles', []))}ê°œ\n"
                if law.get('attachments'):
                    content += f"- ë³„í‘œ/ë³„ì²¨: {len(law['attachments'])}ê°œ\n"
                content += "\n"

        return content

    def export_merged_pdf_content(self, laws_dict: Dict[str, Dict[str, Any]],
                                   base_law_name: str = '') -> bytes:
        """ì—¬ëŸ¬ ë²•ë ¹ì„ í•˜ë‚˜ì˜ í†µí•© íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° (PDF ëŒ€ì²´ìš© Markdown)"""
        content = self._create_merged_markdown(laws_dict, base_law_name)
        return content.encode('utf-8')

    def export_merged_markdown(self, laws_dict: Dict[str, Dict[str, Any]],
                                base_law_name: str = '') -> str:
        """ì—¬ëŸ¬ ë²•ë ¹ì„ í•˜ë‚˜ì˜ Markdown íŒŒì¼ë¡œ ë³‘í•©"""
        return self._create_merged_markdown(laws_dict, base_law_name)

    def _create_merged_markdown(self, laws_dict: Dict[str, Dict[str, Any]],
                                 base_law_name: str = '') -> str:
        """í†µí•© Markdown ì½˜í…ì¸  ìƒì„±"""
        lines = []

        # ì œëª©
        title = f"{base_law_name} ë²•ë ¹ ì²´ê³„ë„" if base_law_name else "ë²•ë ¹ í†µí•© ë¬¸ì„œ"
        lines.append(f"# ğŸ“š {title}\n")
        lines.append(f"> ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        lines.append(f"> ì´ ë²•ë ¹ ìˆ˜: {len(laws_dict)}ê°œ\n")

        # ëª©ì°¨ ìƒì„±
        lines.append("\n---\n")
        lines.append("## ğŸ“‘ ëª©ì°¨\n")

        # ë²•ë ¹ì„ ìœ í˜•ë³„ë¡œ ë¶„ë¥˜
        law_types = {
            'ë²•ë¥ ': [],
            'ì‹œí–‰ë ¹': [],
            'ì‹œí–‰ê·œì¹™': [],
            'í–‰ì •ê·œì¹™': []
        }

        for law_id, law in laws_dict.items():
            law_name = law.get('law_name', '')
            law_type = law.get('law_type', '')

            if law.get('is_admin_rule') or any(k in law_name for k in ['ê³ ì‹œ', 'í›ˆë ¹', 'ì˜ˆê·œ', 'ê·œì •', 'ì„¸ì¹™']):
                law_types['í–‰ì •ê·œì¹™'].append((law_id, law))
            elif 'ì‹œí–‰ê·œì¹™' in law_name or 'ì‹œí–‰ê·œì¹™' in law_type:
                law_types['ì‹œí–‰ê·œì¹™'].append((law_id, law))
            elif 'ì‹œí–‰ë ¹' in law_name or 'ì‹œí–‰ë ¹' in law_type:
                law_types['ì‹œí–‰ë ¹'].append((law_id, law))
            else:
                law_types['ë²•ë¥ '].append((law_id, law))

        # ëª©ì°¨ ì‘ì„±
        toc_num = 1
        for type_name, type_laws in law_types.items():
            if type_laws:
                lines.append(f"\n### {type_name}\n")
                for law_id, law in type_laws:
                    # ì•µì»¤ ë§í¬ ìƒì„±
                    anchor = self._sanitize_filename(law['law_name']).replace(' ', '-').lower()
                    lines.append(f"{toc_num}. [{law['law_name']}](#{anchor})")
                    toc_num += 1

        lines.append("\n---\n")
        lines.append("## ğŸ“– ë²•ë ¹ ë³¸ë¬¸\n")

        # ê° ë²•ë ¹ ë³¸ë¬¸ ì‘ì„±
        for type_name, type_laws in law_types.items():
            if type_laws:
                lines.append(f"\n### ğŸ“‚ {type_name}\n")
                lines.append("---\n")

                for law_id, law in type_laws:
                    lines.append(self._format_law_for_merge(law))
                    lines.append("\n---\n")

        return '\n'.join(lines)

    def _format_law_for_merge(self, law: Dict[str, Any]) -> str:
        """ë³‘í•© ë¬¸ì„œìš© ê°œë³„ ë²•ë ¹ í¬ë§·"""
        lines = []

        # ë²•ë ¹ ì œëª© (ì•µì»¤ í¬í•¨)
        anchor = self._sanitize_filename(law['law_name']).replace(' ', '-').lower()
        lines.append(f"<a name=\"{anchor}\"></a>")
        lines.append(f"## ğŸ“œ {law['law_name']}\n")

        # ê¸°ë³¸ ì •ë³´ í…Œì´ë¸”
        lines.append("| í•­ëª© | ë‚´ìš© |")
        lines.append("|------|------|")
        lines.append(f"| **ë²•ì¢…êµ¬ë¶„** | {law.get('law_type', '-')} |")
        if law.get('department'):
            lines.append(f"| **ì†Œê´€ë¶€ì²˜** | {law.get('department', '-')} |")
        lines.append(f"| **ê³µí¬ì¼ì** | {law.get('promulgation_date', '-')} |")
        lines.append(f"| **ì‹œí–‰ì¼ì** | {law.get('enforcement_date', '-')} |")
        if law.get('articles'):
            lines.append(f"| **ì¡°ë¬¸ ìˆ˜** | {len(law['articles'])}ê°œ |")
        if law.get('attachments'):
            lines.append(f"| **ë³„í‘œ/ë³„ì²¨** | {len(law['attachments'])}ê°œ |")

        lines.append("")

        # ì¡°ë¬¸
        if law.get('articles'):
            lines.append("### ğŸ“– ì¡°ë¬¸\n")
            for article in law['articles']:
                lines.append(f"#### {article['number']} {article.get('title', '')}\n")
                lines.append(f"{article['content']}\n")

                if article.get('paragraphs'):
                    for para in article['paragraphs']:
                        lines.append(f"> {para['number']} {para['content']}\n")
                lines.append("")

        # ë¶€ì¹™
        if law.get('supplementary_provisions'):
            lines.append("### ğŸ“‹ ë¶€ì¹™\n")
            for provision in law['supplementary_provisions']:
                if provision.get('promulgation_date'):
                    lines.append(f"#### ë¶€ì¹™ <{provision['promulgation_date']}>\n")
                lines.append(f"{provision['content']}\n")
                lines.append("")

        # ë³„í‘œ/ë³„ì²¨
        if law.get('attachments'):
            lines.append("### ğŸ“ ë³„í‘œ/ë³„ì²¨\n")
            for attachment in law['attachments']:
                lines.append(f"#### [{attachment['type']}] {attachment.get('title', '')}\n")
                if attachment.get('content'):
                    # ê¸´ ë‚´ìš©ì€ ì ‘ê¸°ë¡œ ì²˜ë¦¬
                    content = attachment['content']
                    if len(content) > 500:
                        lines.append("<details>")
                        lines.append("<summary>ë‚´ìš© ë³´ê¸° (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>\n")
                        lines.append(f"```\n{content}\n```")
                        lines.append("</details>\n")
                    else:
                        lines.append(f"```\n{content}\n```\n")
                lines.append("")

        # ì›ë¬¸ (ì¡°ë¬¸ì´ ì—†ëŠ” ê²½ìš°)
        if not law.get('articles') and law.get('raw_content'):
            lines.append("### ğŸ“„ ì›ë¬¸\n")
            lines.append(f"```\n{law['raw_content']}\n```\n")

        return '\n'.join(lines)

    def export_merged_zip(self, laws_dict: Dict[str, Dict[str, Any]],
                          base_law_name: str = '') -> bytes:
        """í†µí•© íŒŒì¼ê³¼ ê°œë³„ íŒŒì¼ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” ZIP ë‚´ë³´ë‚´ê¸°"""
        zip_buffer = BytesIO()

        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # 1. í†µí•© Markdown íŒŒì¼
            merged_md = self._create_merged_markdown(laws_dict, base_law_name)
            safe_base_name = self._sanitize_filename(base_law_name) if base_law_name else 'ë²•ë ¹_í†µí•©'
            zip_file.writestr(f'{safe_base_name}_í†µí•©.md', merged_md)

            # 2. í†µí•© JSON íŒŒì¼
            metadata = {
                'collection_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'base_law_name': base_law_name,
                'total_laws': len(laws_dict),
                'laws': laws_dict
            }
            zip_file.writestr(
                f'{safe_base_name}_í†µí•©.json',
                json.dumps(metadata, ensure_ascii=False, indent=2)
            )

            # 3. ê°œë³„ íŒŒì¼ë“¤
            for law_id, law in laws_dict.items():
                safe_name = self._sanitize_filename(law['law_name'])

                # ê°œë³„ Markdown
                md_content = self._format_law_markdown(law)
                zip_file.writestr(f'laws/{safe_name}.md', md_content)

                # ê°œë³„ JSON
                zip_file.writestr(
                    f'laws/{safe_name}.json',
                    json.dumps(law, ensure_ascii=False, indent=2)
                )

            # 4. README
            readme = self._create_merged_readme(laws_dict, base_law_name)
            zip_file.writestr('README.md', readme)

        zip_buffer.seek(0)
        return zip_buffer.getvalue()

    def _create_merged_readme(self, laws_dict: Dict[str, Dict[str, Any]],
                               base_law_name: str = '') -> str:
        """í†µí•© ë‚´ë³´ë‚´ê¸°ìš© README ìƒì„±"""
        lines = []

        lines.append(f"# ğŸ“š {base_law_name or 'ë²•ë ¹'} ì²´ê³„ë„ ìˆ˜ì§‘ ê²°ê³¼\n")
        lines.append(f"> ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        lines.append("## ğŸ“Š ìˆ˜ì§‘ í†µê³„\n")
        lines.append(f"- **ì´ ë²•ë ¹ ìˆ˜**: {len(laws_dict)}ê°œ")

        # ìœ í˜•ë³„ í†µê³„
        admin_count = sum(1 for law in laws_dict.values() if law.get('is_admin_rule', False))
        general_count = len(laws_dict) - admin_count
        article_count = sum(len(law.get('articles', [])) for law in laws_dict.values())
        attachment_count = sum(len(law.get('attachments', [])) for law in laws_dict.values())

        lines.append(f"- **ì¼ë°˜ ë²•ë ¹**: {general_count}ê°œ")
        lines.append(f"- **í–‰ì •ê·œì¹™**: {admin_count}ê°œ")
        lines.append(f"- **ì´ ì¡°ë¬¸ ìˆ˜**: {article_count}ê°œ")
        lines.append(f"- **ì´ ë³„í‘œ/ë³„ì²¨**: {attachment_count}ê°œ\n")

        lines.append("## ğŸ“ íŒŒì¼ êµ¬ì¡°\n")
        lines.append("```")
        safe_base_name = self._sanitize_filename(base_law_name) if base_law_name else 'ë²•ë ¹_í†µí•©'
        lines.append(f"â”œâ”€â”€ {safe_base_name}_í†µí•©.md    # ëª¨ë“  ë²•ë ¹ì„ í•˜ë‚˜ë¡œ í†µí•©í•œ íŒŒì¼")
        lines.append(f"â”œâ”€â”€ {safe_base_name}_í†µí•©.json  # ì „ì²´ ë°ì´í„° (JSON)")
        lines.append("â”œâ”€â”€ laws/                       # ê°œë³„ ë²•ë ¹ íŒŒì¼ë“¤")
        lines.append("â”‚   â”œâ”€â”€ [ë²•ë ¹ëª…].md")
        lines.append("â”‚   â””â”€â”€ [ë²•ë ¹ëª…].json")
        lines.append("â””â”€â”€ README.md                   # ì´ íŒŒì¼")
        lines.append("```\n")

        lines.append("## ğŸ“‹ ìˆ˜ì§‘ëœ ë²•ë ¹ ëª©ë¡\n")

        for law_id, law in laws_dict.items():
            law_type_icon = "ğŸ“‹" if law.get('is_admin_rule') else "ğŸ“–"
            lines.append(f"- {law_type_icon} **{law['law_name']}** ({law.get('law_type', '')})")

        return '\n'.join(lines)


# ===== Streamlit UI í•¨ìˆ˜ë“¤ =====
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    defaults = {
        'mode': 'direct',
        'extracted_laws': [],
        'file_extractions': {},
        'search_results': [],
        'search_results_by_file': {},
        'selected_laws': [],
        'selected_laws_by_file': {},
        'collected_laws': {},
        'collected_laws_by_file': {},
        'file_processed': False,
        'openai_api_key': None,
        'use_ai': False,
        'oc_code': '',
        'include_pdfs': False,  # PDF ë‹¤ìš´ë¡œë“œ ì˜µì…˜
        'current_data_type': 'law'  # í˜„ì¬ ì„ íƒëœ ë°ì´í„° ìœ í˜•
    }

    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


def show_sidebar():
    """ì‚¬ì´ë“œë°” UI - ê°œì„ ëœ API í‚¤ ì²˜ë¦¬"""
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ê¸°ê´€ì½”ë“œ ì…ë ¥ - ê³ ìœ  í‚¤ ì‚¬ìš©
        oc_code = st.text_input(
            "ê¸°ê´€ì½”ë“œ (OC)",
            value=st.session_state.get('oc_code', ''),
            placeholder="ì´ë©”ì¼ @ ì•ë¶€ë¶„",
            help="ì˜ˆ: test@korea.kr â†’ test",
            key="sidebar_oc_code"  # ê³ ìœ  í‚¤ ì¶”ê°€
        )
        
        # ê°’ì´ ë³€ê²½ë˜ë©´ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        if oc_code != st.session_state.get('oc_code', ''):
            st.session_state.oc_code = oc_code
        
        st.divider()
        
        # AI ì„¤ì • (ê°œì„ ëœ ë²„ì „)
        with st.expander("ğŸ¤– AI ì„¤ì • (ì„ íƒì‚¬í•­)", expanded=False):
            st.markdown("**ChatGPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë²•ë ¹ëª… ì¶”ì¶œ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤**")
            
            # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸
            try:
                import openai
                openai_available = True
            except ImportError:
                openai_available = False
                st.warning("âš ï¸ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.info("ì„¤ì¹˜í•˜ë ¤ë©´: `pip install openai`")
            
            if openai_available:
                # í˜„ì¬ API í‚¤ ìƒíƒœ í‘œì‹œ
                if st.session_state.get('use_ai', False) and st.session_state.get('openai_api_key'):
                    st.success("âœ… AI ê¸°ëŠ¥ í™œì„±í™”ë¨")
                    st.caption(f"í˜„ì¬ API í‚¤: {st.session_state.openai_api_key[:10]}...")
                    
                    if st.button("ğŸ”„ API í‚¤ ì¬ì„¤ì •", type="secondary"):
                        st.session_state.openai_api_key = None
                        st.session_state.use_ai = False
                        st.rerun()
                else:
                    # API í‚¤ ì…ë ¥
                    api_key_input = st.text_input(
                        "OpenAI API Key",
                        type="password",
                        value="",
                        key="openai_key_new_input",  # ê³ ìœ  í‚¤
                        help="https://platform.openai.com/api-keys ì—ì„œ ë°œê¸‰",
                        placeholder="sk-..."
                    )
                    
                    if st.button("ğŸ”‘ API í‚¤ ì„¤ì •", type="primary"):
                        if api_key_input:
                            # í‚¤ ì •ë¦¬ ë° ê²€ì¦
                            cleaned_key = api_key_input.strip()
                            
                            if cleaned_key.startswith(('sk-', 'sess-')) and len(cleaned_key) > 40:
                                with st.spinner("API í‚¤ ê²€ì¦ ì¤‘..."):
                                    try:
                                        from openai import OpenAI
                                        # í…ŒìŠ¤íŠ¸ìš© í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                                        test_client = OpenAI(api_key=cleaned_key)
                                        
                                        # ë²„ì „ ë…ë¦½ì ì¸ API í…ŒìŠ¤íŠ¸
                                        success = False
                                        try:
                                            # ê°€ì¥ ê°„ë‹¨í•œ API í˜¸ì¶œ - chat completion
                                            test_response = test_client.chat.completions.create(
                                                model="gpt-5",
                                                messages=[{"role": "user", "content": "test"}],
                                                max_tokens=1
                                            )
                                            success = True
                                        except Exception as chat_error:
                                            # chat API ì‹¤íŒ¨ ì‹œ models API ì‹œë„
                                            try:
                                                # ì‹ ë²„ì „ API í˜¸í™˜
                                                test_response = test_client.models.list()
                                                if test_response and hasattr(test_response, 'data'):
                                                    success = True
                                            except:
                                                # gpt-5ë¡œ ì¬ì‹œë„
                                                try:
                                                    test_response = test_client.chat.completions.create(
                                                        model="gpt-5",
                                                        messages=[{"role": "user", "content": "test"}],
                                                        max_tokens=1
                                                    )
                                                    success = True
                                                except:
                                                    success = False
                                        
                                        if success:
                                            # ì„±ê³µí•˜ë©´ ì„¸ì…˜ì— ì €ì¥
                                            st.session_state.openai_api_key = cleaned_key
                                            st.session_state.use_ai = True
                                            st.success("âœ… API í‚¤ê°€ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                            logger.info(f"API í‚¤ ì„¤ì • ë° ê²€ì¦ ì™„ë£Œ")
                                            st.rerun()
                                        else:
                                            raise Exception("API í‚¤ ê²€ì¦ ì‹¤íŒ¨")
                                        
                                    except Exception as e:
                                        error_msg = str(e)
                                        if "401" in error_msg or "Incorrect API key" in error_msg:
                                            st.error("âŒ API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                                            st.info("ì˜¬ë°”ë¥¸ OpenAI API í‚¤ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                                        elif "429" in error_msg:
                                            st.warning("âš ï¸ API ì‚¬ìš© í•œë„ ì´ˆê³¼. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                                        else:
                                            st.error(f"âŒ API í‚¤ ê²€ì¦ ì‹¤íŒ¨: {error_msg}")
                                        
                                        logger.error(f"API í‚¤ ê²€ì¦ ì‹¤íŒ¨: {e}")
                            else:
                                st.error("âŒ ì˜¬ë°”ë¥¸ í˜•ì‹ì˜ API í‚¤ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                                st.info("'sk-' ë˜ëŠ” 'sess-'ë¡œ ì‹œì‘í•˜ëŠ” í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        else:
                            st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        st.divider()
        
        # PDF/OCR ì˜µì…˜
        st.subheader("ğŸ“„ ë³„í‘œ/ë³„ì²¨ ì²˜ë¦¬")
        st.info("ë³„í‘œ/ë³„ì²¨ì€ í…ìŠ¤íŠ¸ë¡œ ìë™ ìˆ˜ì§‘ë©ë‹ˆë‹¤.")
        st.caption("PDFë¡œë§Œ ì œê³µë˜ëŠ” ê²½ìš° ìˆ˜ì§‘ í›„ OCR ì²˜ë¦¬ ê°€ëŠ¥")
        
        st.divider()
        
        # ëª¨ë“œ ì„ íƒ
        st.subheader("ğŸ¯ ìˆ˜ì§‘ ë°©ì‹")
        mode = st.radio(
            "ë°©ì‹ ì„ íƒ",
            ["ì§ì ‘ ê²€ìƒ‰", "íŒŒì¼ ì—…ë¡œë“œ"],
            help="ì§ì ‘ ê²€ìƒ‰: ë²•ë ¹ëª…ì„ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰\níŒŒì¼ ì—…ë¡œë“œ: íŒŒì¼ì—ì„œ ë²•ë ¹ ì¶”ì¶œ",
            key="sidebar_mode"  # ê³ ìœ  í‚¤
        )
        st.session_state.mode = 'direct' if mode == "ì§ì ‘ ê²€ìƒ‰" else 'file'
        
        # í…ŒìŠ¤íŠ¸ ë²„íŠ¼ ì¶”ê°€
        st.divider()
        st.subheader("ğŸ§ª í…ŒìŠ¤íŠ¸")
        
        if st.button("í–‰ì •ê·œì¹™ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸", type="secondary", use_container_width=True):
            if not st.session_state.get('oc_code', ''):
                st.error("ê¸°ê´€ì½”ë“œë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”!")
            else:
                test_admin_rule_search(st.session_state.oc_code)
        
        # ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ”„ ì´ˆê¸°í™”", type="secondary", use_container_width=True):
            # ìœ ì§€í•  í‚¤ ëª©ë¡ - ê¸°ê´€ì½”ë“œì™€ API í‚¤ ì¶”ê°€
            keys_to_keep = ['mode', 'oc_code', 'openai_api_key', 'use_ai']
            for key in list(st.session_state.keys()):
                if key not in keys_to_keep:
                    del st.session_state[key]
            st.rerun()
        
        return st.session_state.get('oc_code', '')


def test_admin_rule_search(oc_code: str):
    """í–‰ì •ê·œì¹™ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ - PDF ë””ë²„ê¹… ì •ë³´ ì¶”ê°€"""
    with st.spinner("í–‰ì •ê·œì¹™ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘..."):
        collector = LawCollectorAPI(oc_code)
        
        # í…ŒìŠ¤íŠ¸í•  í–‰ì •ê·œì¹™ë“¤
        test_rules = [
            "ê¸ˆìœµê¸°ê´€ê²€ì‚¬ë°ì œì¬ì—ê´€í•œê·œì •",
            "ì—¬ì‹ ì „ë¬¸ê¸ˆìœµì—…ê°ë…ê·œì •",
            "ì—¬ì‹ ì „ë¬¸ê¸ˆìœµì—…ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™"
        ]
        
        for rule_name in test_rules:
            st.write(f"\nğŸ” ê²€ìƒ‰ ì¤‘: {rule_name}")
            
            # ì‹¤ì œ ê²€ìƒ‰ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
            found = collector.search_single_law(rule_name)
            if found:
                st.success(f"âœ… {len(found)}ê°œ ë°œê²¬!")
                for item in found:
                    type_emoji = "ğŸ“‹" if item.get('is_admin_rule') else "ğŸ“–"
                    st.write(f"    {type_emoji} {item['law_name']} ({item['law_type']})")
                    
                    # ë³„í‘œ/ë³„ì²¨ í™•ì¸
                    if found:
                        with st.expander(f"ìƒì„¸ í…ŒìŠ¤íŠ¸: {item['law_name']}"):
                            detail = collector._get_law_detail(
                                item['law_id'], 
                                item['law_msn'], 
                                item['law_name'], 
                                item.get('is_admin_rule', False)
                            )
                            if detail:
                                if detail.get('attachments'):
                                    st.info(f"ğŸ“ ë³„í‘œ/ë³„ì§€: {len(detail['attachments'])}ê°œ")
                                    
                                    # ë³„í‘œ/ë³„ì§€ ì •ë³´ í‘œì‹œ
                                    for att in detail['attachments']:
                                        st.write(f"**{att['type']} {att.get('number', '')}**: {att.get('title', '')}")
                                        if att.get('content'):
                                            st.text(f"ë‚´ìš© ê¸¸ì´: {len(att['content'])}ì")
                                else:
                                    st.warning("ğŸ“ ë³„í‘œ/ë³„ì§€ ì—†ìŒ")
            else:
                st.warning(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            time.sleep(0.5)


def handle_hierarchy_search(collector: LawCollectorAPI, query: str):
    """ë²•ë ¹ ì²´ê³„ë„ ê²€ìƒ‰ ì²˜ë¦¬ - ìƒí•˜ìœ„ë²• ì¼ê´„ ê²€ìƒ‰"""
    st.subheader(f"ğŸ“Š '{query}' ë²•ë ¹ ì²´ê³„ë„ ê²€ìƒ‰")

    # ì§„í–‰ ìƒíƒœ í‘œì‹œ
    progress_bar = st.progress(0)
    status_text = st.empty()

    def update_progress(progress: float, message: str):
        progress_bar.progress(progress)
        status_text.text(message)

    # ì²´ê³„ë„ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰
    hierarchy_result = collector.search_with_hierarchy(query, update_progress)

    # ì§„í–‰ ìƒíƒœ ì •ë¦¬
    progress_bar.empty()
    status_text.empty()

    # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    if not hierarchy_result.get('laws'):
        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.info("ğŸ’¡ Tip: ë‹¤ë¥¸ ë²•ë ¹ëª…ìœ¼ë¡œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”.")
        return

    # í´ë°± ì—¬ë¶€ í‘œì‹œ
    if hierarchy_result.get('fallback'):
        st.info("â„¹ï¸ ì²´ê³„ë„ ì •ë³´ê°€ ì—†ì–´ ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")

    # ì²´ê³„ë„ ì •ë³´ í‘œì‹œ
    hierarchy_info = hierarchy_result.get('hierarchy_info')
    if hierarchy_info:
        with st.expander("ğŸ“Š ë²•ë ¹ ì²´ê³„ë„ êµ¬ì¡°", expanded=True):
            # ê¸°ë³¸ ì •ë³´
            st.markdown(f"**ê¸°ì¤€ ë²•ë ¹:** {hierarchy_info.get('law_name', query)}")
            st.markdown(f"**ë²•ì¢…:** {hierarchy_info.get('law_type', '-')}")

            # ê´€ë ¨ ë²•ë ¹ êµ¬ì¡°
            related = hierarchy_info.get('related_laws', {})

            col1, col2 = st.columns(2)

            with col1:
                st.markdown("**ğŸ“œ ë²•ë¥ **")
                laws = related.get('laws', [])
                if laws:
                    for law in laws:
                        st.write(f"  â€¢ {law.get('name', '')}")
                else:
                    st.write("  (ì—†ìŒ)")

                st.markdown("**ğŸ“‹ ì‹œí–‰ë ¹**")
                decrees = related.get('enforcement_decrees', [])
                if decrees:
                    for decree in decrees:
                        st.write(f"  â€¢ {decree.get('name', '')}")
                else:
                    st.write("  (ì—†ìŒ)")

            with col2:
                st.markdown("**ğŸ“‘ ì‹œí–‰ê·œì¹™**")
                rules = related.get('enforcement_rules', [])
                if rules:
                    for rule in rules:
                        st.write(f"  â€¢ {rule.get('name', '')}")
                else:
                    st.write("  (ì—†ìŒ)")

                st.markdown("**ğŸ“Œ í–‰ì •ê·œì¹™ (ê³ ì‹œ/í›ˆë ¹ ë“±)**")
                admin_rules = related.get('admin_rules', [])
                if admin_rules:
                    for admin in admin_rules:
                        st.write(f"  â€¢ {admin.get('name', '')} [{admin.get('type', '')}]")
                else:
                    st.write("  (ì—†ìŒ)")

    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
    summary = hierarchy_result.get('search_summary', {})
    st.success(f"âœ… ì´ {summary.get('total', 0)}ê°œì˜ ë²•ë ¹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

    # í†µê³„ í‘œì‹œ
    stats_cols = st.columns(4)
    with stats_cols[0]:
        st.metric("ë²•ë¥ ", summary.get('laws_count', 0))
    with stats_cols[1]:
        st.metric("ì‹œí–‰ë ¹", summary.get('decrees_count', 0))
    with stats_cols[2]:
        st.metric("ì‹œí–‰ê·œì¹™", summary.get('rules_count', 0))
    with stats_cols[3]:
        st.metric("í–‰ì •ê·œì¹™", summary.get('admin_rules_count', 0))

    # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
    results = hierarchy_result.get('laws', [])
    st.session_state.search_results = results
    st.session_state.current_data_type = 'hierarchy'
    st.session_state.hierarchy_info = hierarchy_info

    # ê²°ê³¼ ëª©ë¡ í‘œì‹œ
    st.subheader("ğŸ“‹ ê²€ìƒ‰ëœ ë²•ë ¹ ëª©ë¡")

    # ì „ì²´ ì„ íƒ ì˜µì…˜
    select_all = st.checkbox("ì „ì²´ ì„ íƒ", value=True, key="hierarchy_select_all")

    # ê²°ê³¼ í‘œì‹œ ë° ì„ íƒ
    selected_laws = []

    # ìœ í˜•ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ
    law_groups = {
        'ë²•ë¥ ': [],
        'ì‹œí–‰ë ¹': [],
        'ì‹œí–‰ê·œì¹™': [],
        'í–‰ì •ê·œì¹™': []
    }

    for law in results:
        law_name = law.get('law_name', '')
        law_type = law.get('law_type', '')

        if law.get('is_admin_rule') or any(k in law_name for k in ['ê³ ì‹œ', 'í›ˆë ¹', 'ì˜ˆê·œ', 'ê·œì •', 'ì„¸ì¹™']):
            law_groups['í–‰ì •ê·œì¹™'].append(law)
        elif 'ì‹œí–‰ê·œì¹™' in law_name or 'ì‹œí–‰ê·œì¹™' in law_type:
            law_groups['ì‹œí–‰ê·œì¹™'].append(law)
        elif 'ì‹œí–‰ë ¹' in law_name or 'ì‹œí–‰ë ¹' in law_type:
            law_groups['ì‹œí–‰ë ¹'].append(law)
        else:
            law_groups['ë²•ë¥ '].append(law)

    # ê·¸ë£¹ë³„ë¡œ í‘œì‹œ
    for group_name, group_laws in law_groups.items():
        if group_laws:
            with st.expander(f"{group_name} ({len(group_laws)}ê°œ)", expanded=True):
                for idx, law in enumerate(group_laws):
                    col1, col2, col3 = st.columns([0.5, 5, 2])

                    with col1:
                        is_selected = st.checkbox(
                            "",
                            value=select_all,
                            key=f"hierarchy_law_{law['law_id']}_{idx}",
                            label_visibility="collapsed"
                        )
                        if is_selected:
                            selected_laws.append(law)

                    with col2:
                        st.write(f"**{law.get('law_name', '')}**")
                        if law.get('hierarchy_source'):
                            st.caption(f"ì²´ê³„ë„ ì¶œì²˜: {law['hierarchy_source']}")

                    with col3:
                        law_type = law.get('law_type', '')
                        if law.get('is_admin_rule'):
                            st.caption(f"ğŸ›ï¸ í–‰ì •ê·œì¹™ | {law_type}")
                        else:
                            st.caption(f"ğŸ“œ {law_type}")

    # ì„ íƒëœ ë²•ë ¹ ì €ì¥
    st.session_state.hierarchy_selected_laws = selected_laws

    # ìˆ˜ì§‘ ë²„íŠ¼
    st.divider()

    if st.button("ğŸ“¥ ì„ íƒí•œ ë²•ë ¹ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘", type="primary", use_container_width=True):
        if not selected_laws:
            st.error("ìˆ˜ì§‘í•  ë²•ë ¹ì„ ì„ íƒí•´ì£¼ì„¸ìš”!")
        else:
            collect_hierarchy_laws(collector, selected_laws)


def collect_hierarchy_laws(collector: LawCollectorAPI, laws: List[Dict[str, Any]]):
    """ì²´ê³„ë„ì—ì„œ ì„ íƒí•œ ë²•ë ¹ë“¤ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
    st.subheader("ğŸ“¥ ë²•ë ¹ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

    progress_bar = st.progress(0)
    status_text = st.empty()

    collected_details = {}
    errors = []

    for idx, law in enumerate(laws):
        progress = (idx + 1) / len(laws)
        progress_bar.progress(progress)
        status_text.text(f"ìˆ˜ì§‘ ì¤‘: {law.get('law_name', '')} ({idx + 1}/{len(laws)})")

        try:
            # ìƒì„¸ ì •ë³´ ì¡°íšŒ
            detail = collector.get_detail_by_type(law)

            if detail:
                collected_details[law['law_id']] = detail
            else:
                errors.append(law.get('law_name', ''))

        except Exception as e:
            logger.error(f"ë²•ë ¹ ìˆ˜ì§‘ ì˜¤ë¥˜: {law.get('law_name', '')}: {e}")
            errors.append(law.get('law_name', ''))

        time.sleep(0.2)  # API ë¶€í•˜ ë°©ì§€

    progress_bar.empty()
    status_text.empty()

    # ê²°ê³¼ ì €ì¥
    st.session_state.collected_laws = collected_details

    # ê²°ê³¼ í‘œì‹œ
    st.success(f"âœ… {len(collected_details)}ê°œ ë²•ë ¹ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")

    if errors:
        with st.expander(f"âš ï¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({len(errors)}ê°œ)"):
            for err in errors:
                st.write(f"- {err}")

    # í†µê³„ í‘œì‹œ
    total_articles = sum(len(d.get('articles', [])) for d in collected_details.values())
    total_attachments = sum(len(d.get('attachments', [])) for d in collected_details.values())

    stats_cols = st.columns(3)
    with stats_cols[0]:
        st.metric("ìˆ˜ì§‘ ë²•ë ¹", len(collected_details))
    with stats_cols[1]:
        st.metric("ì¡°ë¬¸ ìˆ˜", total_articles)
    with stats_cols[2]:
        st.metric("ë³„í‘œ/ë³„ì§€", total_attachments)


def handle_direct_search_mode(oc_code: str):
    """ì§ì ‘ ê²€ìƒ‰ ëª¨ë“œ ì²˜ë¦¬"""
    st.header("ğŸ” ì§ì ‘ ê²€ìƒ‰ ëª¨ë“œ")

    # ë°ì´í„° ìœ í˜• ì„ íƒ
    st.subheader("ğŸ“‚ ë°ì´í„° ìœ í˜• ì„ íƒ")

    data_type_options = {
        "ğŸ“Š ë²•ë ¹ ì²´ê³„ë„ (ìƒí•˜ìœ„ë²• ì¼ê´„)": "hierarchy",
        "ë²•ë ¹/í–‰ì •ê·œì¹™": "law",
        "ìì¹˜ë²•ê·œ": "ordinance",
        "íŒë¡€": "precedent",
        "í—Œì¬ê²°ì •ë¡€": "constitutional",
        "ë²•ë ¹í•´ì„ë¡€": "interpretation",
        "í–‰ì •ì‹¬íŒë¡€": "admin_decision",
        "ì¡°ì•½": "treaty"
    }

    selected_type_label = st.selectbox(
        "ê²€ìƒ‰í•  ë°ì´í„° ìœ í˜•",
        options=list(data_type_options.keys()),
        index=0,
        help="ê²€ìƒ‰í•  ë²•ë¥  ë°ì´í„° ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš” (ê¸°ë³¸: ë²•ë ¹ ì²´ê³„ë„ ê²€ìƒ‰)"
    )

    selected_data_type = data_type_options[selected_type_label]

    # ë°ì´í„° ìœ í˜•ë³„ ì„¤ëª…
    type_descriptions = {
        "law": "ğŸ’¡ ë²•ë ¹/í–‰ì •ê·œì¹™ ê²€ìƒ‰: ë„ì–´ì“°ê¸° ë³€í˜•ì„ í¬í•¨í•˜ì—¬ ìµœëŒ€í•œ ë§ì€ ë²•ë ¹ì„ ì°¾ìŠµë‹ˆë‹¤.",
        "hierarchy": "ğŸ“Š ë²•ë ¹ ì²´ê³„ë„ ê²€ìƒ‰: ìƒìœ„ë²•(ë²•ë¥ )ì„ ê²€ìƒ‰í•˜ë©´ ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™, í–‰ì •ê·œì¹™(ê³ ì‹œ, í›ˆë ¹ ë“±) ë“± ê´€ë ¨ëœ ëª¨ë“  í•˜ìœ„ë²•ë ¹ì„ í•¨ê»˜ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "ordinance": "ğŸ“œ ìì¹˜ë²•ê·œ ê²€ìƒ‰: ì§€ë°©ìì¹˜ë‹¨ì²´ì˜ ì¡°ë¡€, ê·œì¹™ ë“±ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "precedent": "âš–ï¸ íŒë¡€ ê²€ìƒ‰: ëŒ€ë²•ì› ë° í•˜ê¸‰ë²•ì› íŒë¡€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "constitutional": "ğŸ›ï¸ í—Œì¬ê²°ì •ë¡€ ê²€ìƒ‰: í—Œë²•ì¬íŒì†Œì˜ ê²°ì •ë¡€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "interpretation": "ğŸ“– ë²•ë ¹í•´ì„ë¡€ ê²€ìƒ‰: ë²•ì œì²˜ ë“±ì˜ ë²•ë ¹ í•´ì„ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "admin_decision": "ğŸ“‹ í–‰ì •ì‹¬íŒë¡€ ê²€ìƒ‰: í–‰ì •ì‹¬íŒìœ„ì›íšŒì˜ ì¬ê²°ë¡€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "treaty": "ğŸŒ ì¡°ì•½ ê²€ìƒ‰: êµ­ì œ ì¡°ì•½ ë° í˜‘ì •ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."
    }

    st.info(type_descriptions.get(selected_data_type, ""))

    # ê²€ìƒ‰ì–´ ì…ë ¥
    placeholder_texts = {
        "law": "ì˜ˆ: ë¯¼ë²•, ìƒë²•, ê¸ˆìœµê°ë…ê·œì •",
        "hierarchy": "ì˜ˆ: ê¸ˆìœµì§€ì£¼íšŒì‚¬ë²•, ìë³¸ì‹œì¥ë²•, ê°œì¸ì •ë³´ë³´í˜¸ë²•",
        "ordinance": "ì˜ˆ: ì£¼ì°¨ì¥, í™˜ê²½, ì²­ì†Œë…„",
        "precedent": "ì˜ˆ: ì†í•´ë°°ìƒ, ê³„ì•½í•´ì œ",
        "constitutional": "ì˜ˆ: ìœ„í—Œ, ê¸°ë³¸ê¶Œ",
        "interpretation": "ì˜ˆ: ì„ëŒ€ì°¨, ê±´ì¶•",
        "admin_decision": "ì˜ˆ: ì˜ì—…ì •ì§€, í—ˆê°€ì·¨ì†Œ",
        "treaty": "ì˜ˆ: ë¬´ì—­, íˆ¬ì, ì¸ê¶Œ"
    }

    search_query = st.text_input(
        "ê²€ìƒ‰ì–´",
        placeholder=placeholder_texts.get(selected_data_type, "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”"),
        help="ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”"
    )

    if st.button("ğŸ” ê²€ìƒ‰", type="primary", use_container_width=True):
        if not oc_code:
            st.error("ê¸°ê´€ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        elif not search_query:
            st.error("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        else:
            with st.spinner(f"'{search_query}' ê²€ìƒ‰ ì¤‘... ({selected_type_label})"):
                collector = LawCollectorAPI(oc_code)

                # ë°ì´í„° ìœ í˜•ì— ë”°ë¥¸ ê²€ìƒ‰
                if selected_data_type == "hierarchy":
                    # ë²•ë ¹ ì²´ê³„ë„ ê²€ìƒ‰ (ìƒí•˜ìœ„ë²• ì¼ê´„ ê²€ìƒ‰)
                    handle_hierarchy_search(collector, search_query)
                    return  # ì²´ê³„ë„ ê²€ìƒ‰ì€ ë³„ë„ ì²˜ë¦¬
                elif selected_data_type == "law":
                    # ê¸°ì¡´ ë²•ë ¹/í–‰ì •ê·œì¹™ ê²€ìƒ‰ (ë³€í˜• ê²€ìƒ‰ í¬í•¨)
                    results = collector._search_with_variations(search_query)
                else:
                    # ìƒˆë¡œìš´ ë°ì´í„° ìœ í˜• ê²€ìƒ‰
                    results = collector.search_by_type(search_query, selected_data_type)

                if results:
                    st.success(f"{len(results)}ê°œì˜ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

                    # ê²°ê³¼ ìœ í˜•ë³„ ì •ë³´ í‘œì‹œ
                    if selected_data_type == "law":
                        admin_count = sum(1 for r in results if r.get('is_admin_rule'))
                        if admin_count > 0:
                            st.info(f"ğŸ“‹ ì´ ì¤‘ {admin_count}ê°œëŠ” í–‰ì •ê·œì¹™ì…ë‹ˆë‹¤.")

                        # ê²€ìƒ‰ ë³€í˜• í‘œì‹œ
                        variations_used = set()
                        for r in results:
                            if 'found_with_variation' in r:
                                variations_used.add(r['found_with_variation'])

                        if variations_used and len(variations_used) > 1:
                            with st.expander("ğŸ” ê²€ìƒ‰ì— ì‚¬ìš©ëœ ë³€í˜•"):
                                for var in variations_used:
                                    st.write(f"- {var}")

                    st.session_state.search_results = results
                    st.session_state.current_data_type = selected_data_type
                else:
                    st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    st.info("ğŸ’¡ Tip: ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ê±°ë‚˜ ê¸°ê´€ì½”ë“œë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
                    st.session_state.search_results = []


def handle_file_upload_mode(oc_code: str):
    """íŒŒì¼ ì—…ë¡œë“œ ëª¨ë“œ ì²˜ë¦¬"""
    st.header("ğŸ“„ íŒŒì¼ ì—…ë¡œë“œ ëª¨ë“œ")
    
    # AI ìƒíƒœ í‘œì‹œ (ìˆ˜ì •)
    if st.session_state.use_ai and st.session_state.openai_api_key:
        st.info(f"ğŸ¤– AI ê°•í™” ëª¨ë“œ í™œì„±í™”")
    else:
        st.info("ğŸ’¡ AI ì„¤ì •ì„ í†µí•´ ë²•ë ¹ëª… ì¶”ì¶œ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    
    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì„ íƒ",
        type=['pdf', 'xlsx', 'xls', 'md', 'txt'],
        help="PDF, Excel, Markdown, í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤",
        accept_multiple_files=True
    )

    if uploaded_files:
        st.subheader("ğŸ“‹ STEP 1: ë²•ë ¹ëª… ì¶”ì¶œ")

        extractor = EnhancedLawFileExtractor(
            use_ai=st.session_state.use_ai,
            api_key=st.session_state.openai_api_key
        )

        newly_processed = []

        for uploaded_file in uploaded_files:
            file_type = uploaded_file.name.split('.')[-1].lower()
            file_key = f"{uploaded_file.name}_{uploaded_file.size}"

            if file_key in st.session_state.file_extractions:
                continue

            with st.spinner(f"'{uploaded_file.name}'ì—ì„œ ë²•ë ¹ëª…ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                try:
                    uploaded_file.seek(0)
                    extracted_laws = extractor.extract_from_file(uploaded_file, file_type)

                    st.session_state.file_extractions[file_key] = {
                        'file_name': uploaded_file.name,
                        'file_type': file_type,
                        'laws': extracted_laws,
                        'edited_laws': extracted_laws.copy()
                    }

                    newly_processed.append((uploaded_file.name, len(extracted_laws)))

                except Exception as e:
                    st.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({uploaded_file.name}): {str(e)}")
                    logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)

        if newly_processed:
            for name, count in newly_processed:
                if count:
                    st.success(f"âœ… {name}: {count}ê°œì˜ ë²•ë ¹ëª…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                else:
                    st.warning(f"âš ï¸ {name}: ë²•ë ¹ëª…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")

        st.session_state.file_processed = bool(st.session_state.file_extractions)

        # ì „ì²´ ë¦¬ìŠ¤íŠ¸ë„ ìœ ì§€ (ê¸°ì¡´ ê¸°ëŠ¥ í˜¸í™˜)
        st.session_state.extracted_laws = [
            law
            for data in st.session_state.file_extractions.values()
            for law in data.get('edited_laws', [])
        ]

    # ì¶”ì¶œëœ ë²•ë ¹ í‘œì‹œ
    if st.session_state.file_extractions:
        display_extracted_laws(oc_code)


def display_extracted_laws(oc_code: str):
    """ì¶”ì¶œëœ ë²•ë ¹ í‘œì‹œ ë° í¸ì§‘"""
    st.subheader("âœï¸ STEP 2: ë²•ë ¹ëª… í™•ì¸ ë° í¸ì§‘")

    file_extractions = st.session_state.file_extractions
    removal_queue = []

    total_law_count = 0
    total_admin_count = 0

    for file_key, data in file_extractions.items():
        laws_for_file = data.get('edited_laws', [])
        total_law_count += len(laws_for_file)
        total_admin_count += sum(1 for law in laws_for_file
                                 if any(k in law for k in LawPatterns.ADMIN_KEYWORDS))

        with st.expander(f"ğŸ“„ {data['file_name']} ({len(laws_for_file)}ê°œ)", expanded=True):
            st.caption("í•œ ì¤„ì— í•˜ë‚˜ì”© ë²•ë ¹ëª…ì„ ì…ë ¥í•˜ê±°ë‚˜ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            default_text = "\n".join(laws_for_file)
            edited_text = st.text_area(
                "ë²•ë ¹ëª… ëª©ë¡",
                value=default_text,
                height=200,
                key=f"law_area_{file_key}"
            )

            updated_laws = [line.strip() for line in edited_text.split('\n') if line.strip()]
            st.session_state.file_extractions[file_key]['edited_laws'] = updated_laws

            col_a, col_b = st.columns([3, 1])
            with col_a:
                new_law = st.text_input(
                    "ìƒˆ ë²•ë ¹ëª… ì¶”ê°€",
                    key=f"new_law_{file_key}"
                )
                if st.button("â• ì¶”ê°€", key=f"add_btn_{file_key}"):
                    if new_law and new_law.strip():
                        updated_laws.append(new_law.strip())
                        st.session_state.file_extractions[file_key]['edited_laws'] = updated_laws
                        st.success(f"'{new_law.strip()}'ì„(ë¥¼) ì¶”ê°€í–ˆìŠµë‹ˆë‹¤")
                        st.session_state.extracted_laws = [
                            law
                            for item in st.session_state.file_extractions.values()
                            for law in item.get('edited_laws', [])
                        ]
                        st.experimental_rerun()
                    else:
                        st.warning("ì¶”ê°€í•  ë²•ë ¹ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")

            with col_b:
                st.metric("ë²•ë ¹ ìˆ˜", len(updated_laws))
                if st.button("ğŸ—‘ï¸ íŒŒì¼ ì œê±°", key=f"remove_{file_key}"):
                    removal_queue.append(file_key)

    if removal_queue:
        for key in removal_queue:
            st.session_state.file_extractions.pop(key, None)
        # ê´€ë ¨ ìƒíƒœ ì •ë¦¬
        for state_key in ['search_results_by_file', 'selected_laws_by_file', 'collected_laws_by_file']:
            if state_key in st.session_state:
                for key in removal_queue:
                    if key in st.session_state[state_key]:
                        st.session_state[state_key].pop(key, None)

        # ì „ì²´ ë¦¬ìŠ¤íŠ¸ ê°±ì‹ 
        st.session_state.extracted_laws = [
            law
            for item in st.session_state.file_extractions.values()
            for law in item.get('edited_laws', [])
        ]

        st.experimental_rerun()

    summary_col1, summary_col2 = st.columns(2)
    with summary_col1:
        st.metric("ì´ ë²•ë ¹", total_law_count)
    with summary_col2:
        st.metric("ì¶”ì • í–‰ì •ê·œì¹™", total_admin_count)

    st.session_state.extracted_laws = [
        law
        for item in st.session_state.file_extractions.values()
        for law in item.get('edited_laws', [])
    ]

    # ê²€ìƒ‰ ë²„íŠ¼
    if st.button("ğŸ” ëª¨ë“  íŒŒì¼ì—ì„œ ë²•ë ¹ ê²€ìƒ‰", type="primary", use_container_width=True):
        if not oc_code:
            st.error("ê¸°ê´€ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        else:
            # íŒŒì¼ë³„ ê²€ìƒ‰ ìš”ì²­ ìƒì„±
            law_requests = []
            for file_key, data in st.session_state.file_extractions.items():
                for order, law_name in enumerate(data.get('edited_laws', [])):
                    law_requests.append({
                        'file_key': file_key,
                        'file_name': data['file_name'],
                        'law_name': law_name,
                        'order': order
                    })

            if law_requests:
                st.session_state.search_requests = law_requests
                search_laws_from_list(oc_code, law_requests, is_from_file=True)
            else:
                st.warning("ê²€ìƒ‰í•  ë²•ë ¹ëª…ì´ ì—†ìŠµë‹ˆë‹¤. ëª©ë¡ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


def search_laws_from_list(oc_code: str, law_inputs: List[Any], is_from_file: bool = True):
    """íŒŒì¼ ë˜ëŠ” ì…ë ¥ì—ì„œ ìˆ˜ì§‘í•œ ë²•ë ¹ëª…ì„ ê²€ìƒ‰"""

    collector = LawCollectorAPI(oc_code)

    law_requests: Optional[List[Dict[str, Any]]] = None
    law_names: List[str] = []

    if law_inputs:
        if isinstance(law_inputs[0], dict):
            law_requests = [cast(Dict[str, Any], item) for item in law_inputs]  # type: ignore[misc]
            law_names = [req['law_name'] for req in law_requests]
        else:
            law_names = [str(name) for name in law_inputs]

    if not law_names:
        st.warning("ê²€ìƒ‰í•  ë²•ë ¹ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.session_state.search_results = []
        st.session_state.search_results_by_file = {}
        return None

    progress_bar = st.progress(0)
    status_text = st.empty()

    def update_progress(progress):
        progress_bar.progress(progress)

    if is_from_file:
        st.info("ğŸ“‹ ë²•ë ¹ì²´ê³„ë„ ëª¨ë“œ: ì¶”ì¶œëœ ë²•ë ¹ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë²•ë ¹ë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    else:
        st.info("ğŸ” ì§ì ‘ ê²€ìƒ‰ ëª¨ë“œ: ë„ì–´ì“°ê¸° ë³€í˜• ë“±ì„ í¬í•¨í•˜ì—¬ í¬ê´„ì ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

    with st.spinner("ë²•ë ¹ì„ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
        results = collector.search_laws(
            law_names,
            progress_callback=update_progress,
            use_variations=(not is_from_file)
        )

    progress_bar.progress(1.0)
    status_text.text("ê²€ìƒ‰ ì™„ë£Œ!")

    results_by_file: Dict[str, List[Dict[str, Any]]] = {}

    if law_requests:
        request_map: Dict[str, List[Dict[str, Any]]] = {}
        for req in law_requests:
            request_map.setdefault(req['law_name'], []).append(req)

        results_by_query: Dict[str, List[Dict[str, Any]]] = {}
        for result in results:
            results_by_query.setdefault(result['search_query'], []).append(result)

        for law_name, requests in request_map.items():
            matches = results_by_query.get(law_name, [])
            if not matches:
                continue

            unique_matches: Dict[str, Dict[str, Any]] = {}
            for match in matches:
                unique_matches.setdefault(match['law_id'], match)

            for req in requests:
                bucket = results_by_file.setdefault(req['file_key'], [])
                for match in unique_matches.values():
                    law_copy = match.copy()
                    law_copy['source_file_key'] = req['file_key']
                    law_copy['source_file_name'] = req['file_name']
                    law_copy['source_law_name'] = req['law_name']
                    law_copy['source_order'] = req['order']
                    bucket.append(law_copy)

        results = [
            law
            for laws in results_by_file.values()
            for law in laws
        ]

    st.session_state.search_results_by_file = results_by_file

    if results:
        st.success(f"âœ… ì´ {len(results)}ê°œì˜ ë²•ë ¹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

        admin_count = sum(1 for r in results if r.get('is_admin_rule'))
        if admin_count > 0:
            st.info(f"ğŸ“‹ ì´ ì¤‘ {admin_count}ê°œëŠ” í–‰ì •ê·œì¹™ì…ë‹ˆë‹¤.")

        if is_from_file:
            with st.expander("ğŸ’¡ ê²€ìƒ‰ ëª¨ë“œ ì •ë³´"):
                st.write("**ë²•ë ¹ì²´ê³„ë„ ëª¨ë“œ**ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì‘ë™í•©ë‹ˆë‹¤:")
                st.write("- âœ… ì¶”ì¶œëœ ë²•ë ¹ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë²•ë ¹ë§Œ ê²€ìƒ‰")
                st.write("- âŒ ë„ì–´ì“°ê¸° ë³€í˜•ì´ë‚˜ ìœ ì‚¬ ë²•ë ¹ëª… ê²€ìƒ‰í•˜ì§€ ì•ŠìŒ")
                st.write("- ğŸ’¡ ë²•ë ¹ì²´ê³„ë„ì— ëª…ì‹œëœ ë²•ë ¹ë§Œ ìˆ˜ì§‘í•˜ì—¬ ì •í™•ì„± ë³´ì¥")

        st.session_state.search_results = results

        if law_requests:
            for file_key, laws in results_by_file.items():
                file_name = next(
                    (req['file_name'] for req in law_requests if req['file_key'] == file_key),
                    file_key
                )
                st.caption(f"ğŸ“ {file_name}: {len(laws)}ê±´ ê²€ìƒ‰")
    else:
        st.session_state.search_results = []
        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")

    return None


def get_data_type_emoji(law: Dict[str, Any]) -> str:
    """ë°ì´í„° ìœ í˜•ì— ë”°ë¥¸ ì´ëª¨ì§€ ë°˜í™˜"""
    data_type = law.get('data_type', '')
    type_emojis = {
        'ordinance': 'ğŸ“œ',
        'precedent': 'âš–ï¸',
        'constitutional': 'ğŸ›ï¸',
        'interpretation': 'ğŸ“–',
        'admin_decision': 'ğŸ“‹',
        'treaty': 'ğŸŒ'
    }

    if data_type in type_emojis:
        return type_emojis[data_type]
    elif law.get('is_admin_rule'):
        return 'ğŸ“‹'
    else:
        return 'ğŸ“–'


def display_search_results_and_collect(oc_code: str):
    """ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ ë° ìˆ˜ì§‘"""
    results_by_file = st.session_state.get('search_results_by_file', {})

    if not st.session_state.search_results and not any(results_by_file.values()):
        return

    st.subheader("ğŸ“‘ ê²€ìƒ‰ ê²°ê³¼")

    selected_laws_by_file: Dict[str, List[Dict[str, Any]]] = {}

    if results_by_file:
        for file_key, laws in results_by_file.items():
            if not laws:
                continue

            file_name = st.session_state.file_extractions.get(file_key, {}).get('file_name', file_key)
            st.markdown(f"### ğŸ“„ {file_name}")

            cols = st.columns([1, 1, 3, 2, 2])
            headers = ["ì„ íƒ", "ìœ í˜•", "ë²•ë ¹ëª…", "ë²•ì¢…êµ¬ë¶„", "ê²€ìƒ‰ì–´"]
            for col, header in zip(cols, headers):
                col.markdown(f"**{header}**")

            select_all_file = st.checkbox("ì „ì²´ ì„ íƒ", key=f"select_all_{file_key}")

            file_selected: List[Dict[str, Any]] = []
            for idx, law in enumerate(laws):
                row_cols = st.columns([1, 1, 3, 2, 2])

                with row_cols[0]:
                    if st.checkbox(
                        "ì„ íƒ",
                        key=f"sel_{file_key}_{idx}",
                        value=select_all_file,
                        label_visibility="collapsed"
                    ):
                        file_selected.append(law)

                with row_cols[1]:
                    st.write(get_data_type_emoji(law))

                with row_cols[2]:
                    st.write(law['law_name'])

                with row_cols[3]:
                    st.write(law.get('law_type', ''))

                with row_cols[4]:
                    st.write(law.get('source_law_name', law.get('search_query', '')))

            selected_laws_by_file[file_key] = file_selected

            st.divider()
    else:
        # ì§ì ‘ ê²€ìƒ‰ ëª¨ë“œ (íŒŒì¼ ì—†ìŒ)ìš© ê¸°ì¡´ í…Œì´ë¸” ìœ ì§€
        current_data_type = st.session_state.get('current_data_type', 'law')

        select_all = st.checkbox("ì „ì²´ ì„ íƒ")

        # ë°ì´í„° ìœ í˜•ì— ë”°ë¼ í—¤ë” ì¡°ì •
        if current_data_type == 'precedent':
            cols = st.columns([1, 1, 3, 2, 2, 2])
            headers = ["ì„ íƒ", "ìœ í˜•", "ì‚¬ê±´ëª…", "ë²•ì›", "ì„ ê³ ì¼ì", "ì‚¬ê±´ë²ˆí˜¸"]
        elif current_data_type == 'constitutional':
            cols = st.columns([1, 1, 3, 2, 2])
            headers = ["ì„ íƒ", "ìœ í˜•", "ì‚¬ê±´ëª…", "ì¢…êµ­ì¼ì", "ì‚¬ê±´ë²ˆí˜¸"]
        elif current_data_type == 'interpretation':
            cols = st.columns([1, 1, 3, 2, 2])
            headers = ["ì„ íƒ", "ìœ í˜•", "ì•ˆê±´ëª…", "íšŒì‹ ì¼ì", "íšŒì‹ ê¸°ê´€"]
        elif current_data_type == 'admin_decision':
            cols = st.columns([1, 1, 3, 2, 2])
            headers = ["ì„ íƒ", "ìœ í˜•", "ì‚¬ê±´ëª…", "ì˜ê²°ì¼ì", "ì¬ê²°êµ¬ë¶„"]
        elif current_data_type == 'treaty':
            cols = st.columns([1, 1, 3, 2, 2])
            headers = ["ì„ íƒ", "ìœ í˜•", "ì¡°ì•½ëª…", "ë°œíš¨ì¼ì", "ì²´ê²°êµ­ê°€"]
        elif current_data_type == 'ordinance':
            cols = st.columns([1, 1, 3, 2, 2])
            headers = ["ì„ íƒ", "ìœ í˜•", "ìì¹˜ë²•ê·œëª…", "ì‹œí–‰ì¼ì", "ìì¹˜ë‹¨ì²´"]
        else:
            cols = st.columns([1, 1, 3, 2, 2, 2])
            headers = ["ì„ íƒ", "ìœ í˜•", "ë²•ë ¹ëª…", "ë²•ì¢…êµ¬ë¶„", "ì‹œí–‰ì¼ì", "ê²€ìƒ‰ì–´"]

        for col, header in zip(cols, headers):
            col.markdown(f"**{header}**")

        st.divider()

        selected_indices = []
        for idx, law in enumerate(st.session_state.search_results):
            if current_data_type == 'precedent':
                row_cols = st.columns([1, 1, 3, 2, 2, 2])
            elif current_data_type in ['constitutional', 'interpretation', 'admin_decision', 'treaty', 'ordinance']:
                row_cols = st.columns([1, 1, 3, 2, 2])
            else:
                row_cols = st.columns([1, 1, 3, 2, 2, 2])

            with row_cols[0]:
                if st.checkbox(
                    "ì„ íƒ",
                    key=f"sel_direct_{idx}",
                    value=select_all,
                    label_visibility="collapsed"
                ):
                    selected_indices.append(idx)

            with row_cols[1]:
                st.write(get_data_type_emoji(law))

            with row_cols[2]:
                st.write(law['law_name'])

            # ë°ì´í„° ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ í•„ë“œ í‘œì‹œ
            if current_data_type == 'precedent':
                with row_cols[3]:
                    st.write(law.get('court', ''))
                with row_cols[4]:
                    st.write(law.get('decision_date', ''))
                with row_cols[5]:
                    st.write(law.get('case_no', ''))
            elif current_data_type == 'constitutional':
                with row_cols[3]:
                    st.write(law.get('decision_date', ''))
                with row_cols[4]:
                    st.write(law.get('case_no', ''))
            elif current_data_type == 'interpretation':
                with row_cols[3]:
                    st.write(law.get('reply_date', ''))
                with row_cols[4]:
                    st.write(law.get('reply_org', ''))
            elif current_data_type == 'admin_decision':
                with row_cols[3]:
                    st.write(law.get('decision_date', ''))
                with row_cols[4]:
                    st.write(law.get('decision_type', ''))
            elif current_data_type == 'treaty':
                with row_cols[3]:
                    st.write(law.get('enforcement_date', ''))
                with row_cols[4]:
                    st.write(law.get('country', ''))
            elif current_data_type == 'ordinance':
                with row_cols[3]:
                    st.write(law.get('enforcement_date', ''))
                with row_cols[4]:
                    st.write(law.get('local_gov', ''))
            else:
                with row_cols[3]:
                    st.write(law.get('law_type', ''))
                with row_cols[4]:
                    st.write(law.get('enforcement_date', ''))
                with row_cols[5]:
                    st.write(law.get('search_query', ''))

        direct_selection = [st.session_state.search_results[i] for i in selected_indices]
        if direct_selection:
            selected_laws_by_file['direct_input'] = direct_selection

    st.session_state.selected_laws_by_file = selected_laws_by_file

    flattened_selected = [
        law
        for laws in selected_laws_by_file.values()
        for law in laws
    ]
    st.session_state.selected_laws = flattened_selected

    if flattened_selected:
        st.success(f"{len(flattened_selected)}ê°œ í•­ëª©ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤")

        # ìœ í˜•ë³„ í†µê³„ í‘œì‹œ
        type_counts = {}
        for law in flattened_selected:
            data_type = law.get('data_type', 'law')
            law_type = law.get('law_type', 'ê¸°íƒ€')
            key = f"{get_data_type_emoji(law)} {law_type}"
            type_counts[key] = type_counts.get(key, 0) + 1

        if len(type_counts) > 1 or (len(type_counts) == 1 and list(type_counts.values())[0] > 0):
            type_info = ", ".join([f"{k}: {v}ê°œ" for k, v in type_counts.items()])
            st.info(type_info)

        if st.button("ğŸ“¥ ì„ íƒí•œ í•­ëª© ìˆ˜ì§‘", type="primary", use_container_width=True):
            collect_selected_laws(oc_code)


def collect_selected_laws(oc_code: str):
    """ì„ íƒëœ ë²•ë ¹ ìˆ˜ì§‘ - PDF ë‹¤ìš´ë¡œë“œ ì œê±°, í…ìŠ¤íŠ¸ ë‚´ìš© í™œìš©"""
    collector = LawCollectorAPI(oc_code)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    def update_progress(progress):
        progress_bar.progress(progress)

    with st.spinner("ë²•ë ¹ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì¤‘..."):
        selected_by_file = st.session_state.get('selected_laws_by_file', {})
        expand_hierarchy = 'direct_input' in selected_by_file
        collected = collector.collect_law_details(
            st.session_state.selected_laws,
            progress_callback=update_progress,
            expand_hierarchy=expand_hierarchy
        )

    def _law_key(law: Dict[str, Any]) -> Tuple[str, str]:
        return (law.get('law_id') or '', law.get('law_msn') or '')

    selected_keys = {_law_key(law) for law in st.session_state.selected_laws}
    auto_added_ids = [
        law_id
        for law_id, detail in collected.items()
        if (law_id, detail.get('law_msn') or '') not in selected_keys
    ]

    if auto_added_ids:
        st.success(f"ë²•ë ¹ ì²´ê³„ í™•ì¥ìœ¼ë¡œ {len(auto_added_ids)}ê°œì˜ ê´€ë ¨ ë²•ë ¹ì„ ì¶”ê°€ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        with st.expander("ìë™ìœ¼ë¡œ ì¶”ê°€ëœ ë²•ë ¹ í™•ì¸"):
            for law_id in auto_added_ids:
                law_detail = collected[law_id]
                relation = law_detail.get('relationship_from_parent', 'ê´€ë ¨ ë²•ë ¹')
                emoji = "ğŸ“‹" if law_detail.get('is_admin_rule') else "ğŸ“–"
                st.write(f"{emoji} {law_detail['law_name']} ({relation})")

    # ë³„í‘œ/ë³„ì²¨ ì •ë³´ í‘œì‹œ
    total_attachments = sum(len(law.get('attachments', [])) for law in collected.values())
    if total_attachments > 0:
        st.info(f"ğŸ“ ì´ {total_attachments}ê°œì˜ ë³„í‘œ/ë³„ì²¨ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # PDF ëŒ€ì‹  í…ìŠ¤íŠ¸ ë‚´ìš© í™œìš© ì•ˆë‚´
        with st.expander("ğŸ“„ ë³„í‘œ/ë³„ì²¨ ì²˜ë¦¬ ì•ˆë‚´"):
            st.write("**ë³„í‘œ/ë³„ì²¨ ë‚´ìš© ì²˜ë¦¬ ë°©ë²•:**")
            st.write("1. í…ìŠ¤íŠ¸ë¡œ ì œê³µë˜ëŠ” ë‚´ìš©ì€ ìë™ìœ¼ë¡œ ìˆ˜ì§‘ë©ë‹ˆë‹¤.")
            st.write("2. PDFë¡œë§Œ ì œê³µë˜ëŠ” ê²½ìš°:")
            st.write("   - ë²•ì œì²˜ ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ")
            st.write("   - ë‹¤ìš´ë¡œë“œí•œ PDFë¥¼ ì•„ë˜ì—ì„œ ì—…ë¡œë“œí•˜ì—¬ OCR ì²˜ë¦¬")
            
            # PDF ì—…ë¡œë“œ ë° OCR ì²˜ë¦¬
            st.subheader("ğŸ“¤ PDF íŒŒì¼ ì—…ë¡œë“œ (OCR ì²˜ë¦¬)")
            uploaded_pdfs = st.file_uploader(
                "ë³„í‘œ/ë³„ì²¨ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
                type=['pdf'],
                accept_multiple_files=True,
                help="ë²•ì œì²˜ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ë³„í‘œ/ë³„ì²¨ PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ OCRë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
            )
            
            if uploaded_pdfs:
                for pdf_file in uploaded_pdfs:
                    with st.spinner(f"{pdf_file.name} OCR ì²˜ë¦¬ ì¤‘..."):
                        try:
                            # OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            text = extract_text_from_pdf(pdf_file)
                            if text:
                                st.success(f"âœ… {pdf_file.name}: {len(text)}ì ì¶”ì¶œ ì™„ë£Œ")
                                
                                # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ í•´ë‹¹ ë²•ë ¹ì— ì¶”ê°€
                                # PDF íŒŒì¼ëª…ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ ì‹œë„
                                for law_id, law in collected.items():
                                    if any(keyword in pdf_file.name for keyword in [law['law_name'], law_id]):
                                        # ë³„í‘œ/ë³„ì²¨ì— OCR í…ìŠ¤íŠ¸ ì¶”ê°€
                                        ocr_attachment = {
                                            'type': 'OCR ì¶”ì¶œ',
                                            'number': '',
                                            'title': pdf_file.name,
                                            'content': text
                                        }
                                        law['attachments'].append(ocr_attachment)
                                        st.info(f"'{law['law_name']}'ì— OCR í…ìŠ¤íŠ¸ ì¶”ê°€ë¨")
                                        break
                            else:
                                st.warning(f"âŒ {pdf_file.name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                        except Exception as e:
                            st.error(f"OCR ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    progress_bar.progress(1.0)
    
    success_count = len(collected)
    total_count = len(st.session_state.selected_laws)
    status_text.text(f"ìˆ˜ì§‘ ì™„ë£Œ! (ì„±ê³µ: {success_count}/{total_count})")
    
    if success_count < total_count:
        failed_laws = [law['law_name'] for law in st.session_state.selected_laws 
                      if law['law_id'] not in collected]
        with st.expander("âŒ ìˆ˜ì§‘ ì‹¤íŒ¨í•œ ë²•ë ¹"):
            for law_name in failed_laws:
                st.write(f"- {law_name}")
    
    st.session_state.collected_laws = collected

    collected_by_file: Dict[str, Dict[str, Dict[str, Any]]] = {}
    for file_key, laws in selected_by_file.items():
        for law in laws:
            law_id = law.get('law_id')
            if not law_id:
                continue
            detail = collected.get(law_id)
            if not detail:
                continue
            target = collected_by_file.setdefault(file_key, {})
            target[law_id] = detail

    if auto_added_ids and 'direct_input' in selected_by_file:
        direct_bucket = collected_by_file.setdefault('direct_input', {})
        for law_id in auto_added_ids:
            direct_bucket[law_id] = collected[law_id]

    st.session_state.collected_laws_by_file = collected_by_file

    # í†µê³„ í‘œì‹œ
    display_collection_stats(collected)
    display_hierarchy_overview(collected)


def extract_text_from_pdf(pdf_file) -> str:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)"""
    text = ""
    
    try:
        # pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        
        # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ PyPDF2ë¡œ ì¬ì‹œë„
        if not text.strip():
            pdf_file.seek(0)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            for page in pdf_reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    
    except Exception as e:
        logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
    return text.strip()


def display_collection_stats(collected_laws: Dict[str, Dict[str, Any]]):
    """ìˆ˜ì§‘ í†µê³„ í‘œì‹œ - ë³„í‘œ/ë³„ì²¨ í…ìŠ¤íŠ¸ í†µê³„ë¡œ ë³€ê²½"""
    total_articles = sum(len(law.get('articles', [])) for law in collected_laws.values())
    total_provisions = sum(len(law.get('supplementary_provisions', [])) for law in collected_laws.values())
    total_attachments = sum(len(law.get('attachments', [])) for law in collected_laws.values())
    admin_rule_count = sum(1 for law in collected_laws.values() if law.get('is_admin_rule', False))
    
    # ë³„í‘œ/ë³„ì²¨ í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
    total_attachment_chars = sum(
        len(att.get('content', '')) 
        for law in collected_laws.values() 
        for att in law.get('attachments', [])
    )
    
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("ì´ ì¡°ë¬¸", f"{total_articles:,}ê°œ")
    with col2:
        st.metric("ì´ ë¶€ì¹™", f"{total_provisions}ê°œ")
    with col3:
        st.metric("ì´ ë³„í‘œ/ë³„ì²¨", f"{total_attachments}ê°œ")
    with col4:
        st.metric("í–‰ì •ê·œì¹™", f"{admin_rule_count}ê°œ")
    with col5:
        st.metric("ë³„í‘œ/ë³„ì²¨ í…ìŠ¤íŠ¸", f"{total_attachment_chars:,}ì")


def display_hierarchy_overview(collected_laws: Dict[str, Dict[str, Any]]):
    """ìë™ í™•ì¥ëœ ë²•ë ¹ ê³„ì¸µì„ íŠ¸ë¦¬ í˜•íƒœë¡œ í‘œì‹œ"""
    if not collected_laws:
        return

    child_map: Dict[str, List[str]] = defaultdict(list)
    for law_id, law in collected_laws.items():
        parent_id = law.get('parent_law_id')
        if parent_id and parent_id in collected_laws:
            child_map[parent_id].append(law_id)

    if not child_map:
        return

    for children in child_map.values():
        children.sort(key=lambda cid: collected_laws[cid]['law_name'])

    root_ids = [law_id for law_id, law in collected_laws.items() if not law.get('parent_law_id')]
    root_ids.sort(key=lambda rid: collected_laws[rid]['law_name'])

    def render_node(node_id: str, level: int = 0) -> None:
        detail = collected_laws[node_id]
        relation = detail.get('relationship_from_parent')
        emoji = "ğŸ“‹" if detail.get('is_admin_rule') else "ğŸ“–"
        indent = "&nbsp;" * (level * 4)
        label = f"{emoji} {detail['law_name']}"
        if relation:
            label += f" <span style='color:#888'>({relation})</span>"
        st.markdown(f"{indent}- {label}", unsafe_allow_html=True)

        for child_id in child_map.get(node_id, []):
            render_node(child_id, level + 1)

    with st.expander("ğŸŒ³ ìë™ìœ¼ë¡œ í™•ì¥ëœ ë²•ë ¹ ì²´ê³„ë„", expanded=True):
        for root_id in root_ids:
            render_node(root_id)


def display_download_section():
    """ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ í‘œì‹œ - ëª¨ë“  í˜•ì‹ ì§€ì›"""
    if not st.session_state.collected_laws:
        return

    st.header("ğŸ’¾ ë‹¤ìš´ë¡œë“œ")

    exporter = LawExporter()

    # ì²´ê³„ë„ ê²€ìƒ‰ ê²°ê³¼ì¸ ê²½ìš° íŠ¹ë³„ ë‹¤ìš´ë¡œë“œ ì˜µì…˜ í‘œì‹œ
    is_hierarchy_search = st.session_state.get('current_data_type') == 'hierarchy'
    hierarchy_info = st.session_state.get('hierarchy_info')

    # ë‹¤ìš´ë¡œë“œ ì˜µì…˜
    st.subheader("ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì˜µì…˜")

    if is_hierarchy_search:
        download_option = st.radio(
            "ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì„ íƒ",
            ["ğŸ“Š í†µí•© íŒŒì¼ (Merge)", "ê°œë³„ íŒŒì¼ (ZIP)", "í†µí•© íŒŒì¼ (ë‹¨ì¼)"],
            help="í†µí•© íŒŒì¼ (Merge): ëª¨ë“  ë²•ë ¹ì„ ì²´ê³„ë„ í˜•ì‹ìœ¼ë¡œ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ë³‘í•©\nê°œë³„ íŒŒì¼: ê° ë²•ë ¹ë³„ë¡œ íŒŒì¼ ìƒì„±\ní†µí•© íŒŒì¼ (ë‹¨ì¼): ê¸°ì¡´ ë‹¨ì¼ íŒŒì¼ í˜•ì‹"
        )
    else:
        download_option = st.radio(
            "ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì„ íƒ",
            ["ê°œë³„ íŒŒì¼ (ZIP)", "í†µí•© íŒŒì¼ (ë‹¨ì¼)"],
            help="ê°œë³„ íŒŒì¼: ê° ë²•ë ¹ë³„ë¡œ íŒŒì¼ ìƒì„±\ní†µí•© íŒŒì¼: ëª¨ë“  ë²•ë ¹ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ"
        )
    
    # Merge ë‹¤ìš´ë¡œë“œ (ì²´ê³„ë„ ê²€ìƒ‰ ê²°ê³¼ìš©)
    if download_option == "ğŸ“Š í†µí•© íŒŒì¼ (Merge)":
        st.info("ğŸ“Š **ë²•ë ¹ ì²´ê³„ë„ í†µí•© ë‹¤ìš´ë¡œë“œ**: ëª¨ë“  ë²•ë ¹ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ë³‘í•©í•©ë‹ˆë‹¤.")

        # ê¸°ì¤€ ë²•ë ¹ëª… ì¶”ì¶œ
        base_law_name = ""
        if hierarchy_info:
            base_law_name = hierarchy_info.get('law_name', '')

        # í˜•ì‹ ì„ íƒ
        merge_format = st.selectbox(
            "í†µí•© íŒŒì¼ í˜•ì‹",
            ["Markdown (í†µí•© + ê°œë³„ ZIP)", "Markdown ë‹¨ì¼ íŒŒì¼", "JSON ë‹¨ì¼ íŒŒì¼"],
            help="Markdown (í†µí•© + ê°œë³„ ZIP): í†µí•© ë¬¸ì„œì™€ ê°œë³„ íŒŒì¼ì„ ëª¨ë‘ í¬í•¨í•œ ZIP\nMarkdown ë‹¨ì¼: í†µí•© Markdown íŒŒì¼ë§Œ\nJSON ë‹¨ì¼: ì „ì²´ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ"
        )

        # í†µê³„ í‘œì‹œ
        total_laws = len(st.session_state.collected_laws)
        total_articles = sum(len(law.get('articles', [])) for law in st.session_state.collected_laws.values())
        total_attachments = sum(len(law.get('attachments', [])) for law in st.session_state.collected_laws.values())

        st.markdown(f"""
        **í†µí•© íŒŒì¼ ë‚´ìš©:**
        - ğŸ“š ë²•ë ¹ ìˆ˜: {total_laws}ê°œ
        - ğŸ“– ì¡°ë¬¸ ìˆ˜: {total_articles}ê°œ
        - ğŸ“ ë³„í‘œ/ë³„ì²¨: {total_attachments}ê°œ
        """)

        if merge_format == "Markdown (í†µí•© + ê°œë³„ ZIP)":
            # í†µí•© + ê°œë³„ ZIP
            zip_data = exporter.export_merged_zip(st.session_state.collected_laws, base_law_name)

            st.download_button(
                label="ğŸ“¦ í†µí•© ZIP ë‹¤ìš´ë¡œë“œ (Merge + ê°œë³„)",
                data=zip_data,
                file_name=f"{base_law_name or 'ë²•ë ¹'}_ì²´ê³„ë„_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                mime="application/zip",
                use_container_width=True
            )

        elif merge_format == "Markdown ë‹¨ì¼ íŒŒì¼":
            # Markdown ë‹¨ì¼ íŒŒì¼
            merged_md = exporter.export_merged_markdown(st.session_state.collected_laws, base_law_name)

            # íŒŒì¼ í¬ê¸° í‘œì‹œ
            file_size = len(merged_md.encode('utf-8'))
            st.caption(f"ğŸ“Š ì˜ˆìƒ íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024:.1f} KB)")

            st.download_button(
                label="ğŸ“„ í†µí•© Markdown ë‹¤ìš´ë¡œë“œ",
                data=merged_md,
                file_name=f"{base_law_name or 'ë²•ë ¹'}_ì²´ê³„ë„_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                mime="text/markdown",
                use_container_width=True
            )

            # ë¯¸ë¦¬ë³´ê¸°
            with st.expander("ğŸ“„ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 2000ì)"):
                st.markdown(merged_md[:2000] + "..." if len(merged_md) > 2000 else merged_md)

        else:  # JSON ë‹¨ì¼ íŒŒì¼
            # JSON ë°ì´í„°
            json_data = {
                'collection_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'base_law_name': base_law_name,
                'total_laws': total_laws,
                'hierarchy_info': hierarchy_info,
                'laws': st.session_state.collected_laws
            }
            json_content = json.dumps(json_data, ensure_ascii=False, indent=2)

            # íŒŒì¼ í¬ê¸° í‘œì‹œ
            file_size = len(json_content.encode('utf-8'))
            st.caption(f"ğŸ“Š ì˜ˆìƒ íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024:.1f} KB)")

            st.download_button(
                label="ğŸ“„ í†µí•© JSON ë‹¤ìš´ë¡œë“œ",
                data=json_content,
                file_name=f"{base_law_name or 'ë²•ë ¹'}_ì²´ê³„ë„_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )

    elif download_option == "ê°œë³„ íŒŒì¼ (ZIP)":
        # ZIP ë‹¤ìš´ë¡œë“œ
        zip_data = exporter.export_to_zip(st.session_state.collected_laws)
        
        st.download_button(
            label="ğŸ“¦ ZIP ë‹¤ìš´ë¡œë“œ (JSON+TXT+MD)",
            data=zip_data,
            file_name=f"laws_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
            mime="application/zip",
            use_container_width=True
        )
    else:
        # í†µí•© íŒŒì¼ - ëª…í™•í•œ í˜•ì‹ ì„ íƒ
        st.info("ğŸ“Œ ë‹¨ì¼ íŒŒì¼ë¡œ ëª¨ë“  ë²•ë ¹ì„ í†µí•©í•˜ì—¬ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")

        # í˜•ì‹ë³„ ì„¤ëª… ì¶”ê°€
        format_descriptions = {
            "JSON": "êµ¬ì¡°í™”ëœ ë°ì´í„° í˜•ì‹ (í”„ë¡œê·¸ë˜ë° í™œìš©ì— ì í•©)",
            "Markdown": "ì½ê¸° ì‰¬ìš´ ë¬¸ì„œ í˜•ì‹ (GitHub, ë…¸ì…˜ ë“±ì— ì í•©)",
            "Text": "ìˆœìˆ˜ í…ìŠ¤íŠ¸ í˜•ì‹ (ë©”ëª¨ì¥ ë“±ì—ì„œ ì—´ê¸° ê°€ëŠ¥)"
        }
        
        file_format = st.selectbox(
            "íŒŒì¼ í˜•ì‹ ì„ íƒ",
            ["JSON", "Markdown", "Text"],
            help="ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ í˜•ì‹ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        
        st.caption(f"ğŸ’¡ {format_descriptions[file_format]}")
        
        # í˜•ì‹ë³„ ë‚´ë³´ë‚´ê¸° ì²˜ë¦¬
        if file_format == "JSON":
            content = exporter.export_single_file(st.session_state.collected_laws, 'json')
            mime = "application/json"
            ext = "json"
        elif file_format == "Markdown":
            content = exporter.export_single_file(st.session_state.collected_laws, 'markdown')
            mime = "text/markdown"
            ext = "md"
        else:  # Text
            content = exporter.export_single_file(st.session_state.collected_laws, 'text')
            mime = "text/plain"
            ext = "txt"
        
        # íŒŒì¼ í¬ê¸° í‘œì‹œ
        file_size = len(content.encode('utf-8'))
        st.caption(f"ğŸ“Š ì˜ˆìƒ íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
        
        st.download_button(
            label=f"ğŸ’¾ {file_format} í†µí•© íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.{ext})",
            data=content,
            file_name=f"all_laws_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{ext}",
            mime=mime,
            use_container_width=True
        )
        
        # ë¯¸ë¦¬ë³´ê¸° ì˜µì…˜
        with st.expander("ğŸ“„ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 1000ì)"):
            st.text(content[:1000] + "..." if len(content) > 1000 else content)

    file_grouped = {
        key: laws
        for key, laws in st.session_state.get('collected_laws_by_file', {}).items()
        if laws
    }

    if file_grouped:
        st.subheader("ğŸ—‚ï¸ íŒŒì¼ë³„ Markdown ë¬¶ìŒ")
        st.caption("ì—…ë¡œë“œí•œ ê° íŒŒì¼ë³„ë¡œ í†µí•©ëœ Markdown ë¬¸ì„œë¥¼ ZIPìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.")

        file_bundle = exporter.export_markdown_by_file(
            file_grouped,
            st.session_state.get('file_extractions', {})
        )

        st.download_button(
            label="ğŸ—‚ï¸ íŒŒì¼ë³„ Markdown ZIP ë‹¤ìš´ë¡œë“œ",
            data=file_bundle,
            file_name=f"file_grouped_markdown_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
            mime="application/zip",
            use_container_width=True
        )

    # ìˆ˜ì§‘ ê²°ê³¼ ìƒì„¸
    with st.expander("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìƒì„¸"):
        for law_id, law in st.session_state.collected_laws.items():
            emoji = "ğŸ“‹" if law.get('is_admin_rule', False) else "ğŸ“–"
            st.subheader(f"{emoji} {law['law_name']}")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.write(f"ì¡°ë¬¸: {len(law.get('articles', []))}ê°œ")
            with col2:
                st.write(f"ë¶€ì¹™: {len(law.get('supplementary_provisions', []))}ê°œ")
            with col3:
                st.write(f"ë³„í‘œ: {len(law.get('attachments', []))}ê°œ")
            with col4:
                # ë³„í‘œ/ë³„ì²¨ í…ìŠ¤íŠ¸ ê¸¸ì´
                att_chars = sum(len(att.get('content', '')) for att in law.get('attachments', []))
                if att_chars > 0:
                    st.write(f"ë³„í‘œ í…ìŠ¤íŠ¸: {att_chars:,}ì")
            
            # ìƒ˜í”Œ ì¡°ë¬¸
            if law.get('articles'):
                st.write("**ìƒ˜í”Œ ì¡°ë¬¸:**")
                sample = law['articles'][0]
                st.text(f"{sample['number']} {sample.get('title', '')}")
                st.text(sample['content'][:200] + "...")
            
            # ë³„í‘œ/ë³„ì²¨ ëª©ë¡
            if law.get('attachments'):
                st.write("**ë³„í‘œ/ë³„ì²¨:**")
                for att in law['attachments']:
                    st.write(f"  - {att['type']} {att.get('number', '')}: {att.get('title', '')} ({len(att.get('content', ''))}ì)")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # ì œëª©
    st.title("ğŸ“š ë²•ì œì²˜ ë²•ë ¹ ìˆ˜ì§‘ê¸°")
    st.markdown("ë²•ì œì²˜ Open APIë¥¼ í™œìš©í•œ ë²•ë ¹ ìˆ˜ì§‘ ë„êµ¬ (v7.0)")
    st.markdown("**âœ¨ ì‹ ê·œ ê¸°ëŠ¥: ìì¹˜ë²•ê·œ, íŒë¡€, í—Œì¬ê²°ì •ë¡€, ë²•ë ¹í•´ì„ë¡€, í–‰ì •ì‹¬íŒë¡€, ì¡°ì•½ ê²€ìƒ‰ ì§€ì›**")
    
    # ì‚¬ì´ë“œë°”
    oc_code = show_sidebar()
    
    # ëª¨ë“œë³„ ì²˜ë¦¬
    if st.session_state.mode == 'direct':
        handle_direct_search_mode(oc_code)
    else:
        handle_file_upload_mode(oc_code)
    
    # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
    if st.session_state.search_results:
        display_search_results_and_collect(oc_code)
    
    # ë‹¤ìš´ë¡œë“œ ì„¹ì…˜
    display_download_section()


if __name__ == "__main__":
    main()
