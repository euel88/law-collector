"""
ë²•ì œì²˜ ë²•ë ¹ ìˆ˜ì§‘ê¸° - ChatGPT API í†µí•© ë²„ì „ (v3.0)
- ChatGPT APIë¥¼ í™œìš©í•œ ë²•ë ¹ëª… ì¶”ì¶œ ì •í™•ë„ í–¥ìƒ
- API í‚¤ ì„ íƒì  ì‚¬ìš© (ì—†ì–´ë„ ê¸°ë³¸ ê¸°ëŠ¥ ë™ì‘)
- ì‚¬ìš©ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤
"""

import streamlit as st
import requests
import xml.etree.ElementTree as ET
import json
import time
import re
from datetime import datetime
from io import BytesIO
import base64
import urllib3
import zipfile
import pandas as pd
import openpyxl
import PyPDF2
import pdfplumber
from typing import List, Set, Dict, Optional, Tuple, Any

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë²•ì œì²˜ ë²•ë ¹ ìˆ˜ì§‘ê¸°",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'mode' not in st.session_state:
    st.session_state.mode = 'direct'  # 'direct' or 'file'
if 'extracted_laws' not in st.session_state:
    st.session_state.extracted_laws = []
if 'search_results' not in st.session_state:
    st.session_state.search_results = []
if 'selected_laws' not in st.session_state:
    st.session_state.selected_laws = []
if 'collected_laws' not in st.session_state:
    st.session_state.collected_laws = {}
if 'file_processed' not in st.session_state:
    st.session_state.file_processed = False
if 'openai_api_key' not in st.session_state:
    st.session_state.openai_api_key = None
if 'use_ai' not in st.session_state:
    st.session_state.use_ai = False


class EnhancedLawFileExtractor:
    """ChatGPT APIë¥¼ í™œìš©í•œ ê°œì„ ëœ ë²•ë ¹ëª… ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ì œì™¸í•  í‚¤ì›Œë“œ (ì¹´í…Œê³ ë¦¬, ì„¤ëª… ë“±)
        self.exclude_keywords = [
            'ìƒí•˜ìœ„ë²•', 'í–‰ì •ê·œì¹™', 'ë²•ë ¹', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™', 'ëŒ€í†µë ¹ë ¹', 
            'ì´ë¦¬ë ¹', 'ë¶€ë ¹', 'ê´€í•œ ê·œì •', 'ìƒìœ„ë²•', 'í•˜ìœ„ë²•', 'ê´€ë ¨ë²•ë ¹'
        ]
        
        # ê°œì„ ëœ ë²•ë ¹ëª… íŒ¨í„´ - í–‰ì •ê·œì¹™ ìš°ì„  ë°°ì¹˜
        self.law_patterns = [
            # ì‹œí–‰ ë‚ ì§œ íŒ¨í„´ì„ ëª¨ë“  íŒ¨í„´ì— í¬í•¨
            r'([ê°€-í£]+(?:\s+[ê°€-í£]+)*(?:ë²•|ë²•ë¥ |ê·œì •|ê·œì¹™|ì„¸ì¹™|ë¶„ë¥˜))\s*\[ì‹œí–‰\s*\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.\]',
            
            # íŒ¨í„´ 1: ë…ë¦½ì ì¸ ê·œì •/ì„¸ì¹™ (í–‰ì •ê·œì¹™) 
            r'^([ê°€-í£]+(?:(?:\s+ë°\s+)|(?:\s+))?[ê°€-í£]*(?:ì—\s*ê´€í•œ\s*)?(?:ê·œì •|ì—…ë¬´ê·œì •|ê°ë…ê·œì •|ìš´ì˜ê·œì •|ê´€ë¦¬ê·œì •))(?:\s|$)',
            
            # íŒ¨í„´ 2: ì‹œí–‰ì„¸ì¹™ (ë…ë¦½ì )
            r'^([ê°€-í£]+(?:(?:\s+ë°\s+)|(?:\s+))?[ê°€-í£]*(?:ì—…ë¬´)?ì‹œí–‰ì„¸ì¹™)(?:\s|$)',
            
            # íŒ¨í„´ 3: ë¶™ì–´ìˆëŠ” í˜•íƒœì˜ ê·œì • ì²˜ë¦¬
            r'([ê°€-í£]+(?:ê²€ì‚¬ë°ì œì¬ì—ê´€í•œ|ì—ê´€í•œ)?ê·œì •)(?:\s|$)',
            
            # íŒ¨í„´ 4: ì¼ë°˜ì ì¸ ë²•ë¥ ëª… 
            r'^([ê°€-í£]+(?:\s+[ê°€-í£]+)*(?:ì—\s*ê´€í•œ\s*)?(?:íŠ¹ë³„|ê¸°ë³¸|ê´€ë¦¬|ì´‰ì§„|ì§€ì›|ìœ¡ì„±|ì§„í¥|ë³´í˜¸|ê·œì œ|ë°©ì§€)?ë²•(?:ë¥ )?)(?:\s|$)',
            
            # íŒ¨í„´ 5: ì‹œí–‰ë ¹ 
            r'^([ê°€-í£]+(?:\s+[ê°€-í£]+)*ë²•(?:ë¥ )?)\s+ì‹œí–‰ë ¹(?:\s|$)',
            
            # íŒ¨í„´ 6: ì‹œí–‰ê·œì¹™ 
            r'^([ê°€-í£]+(?:\s+[ê°€-í£]+)*ë²•(?:ë¥ )?)\s+ì‹œí–‰ê·œì¹™(?:\s|$)',
            
            # íŒ¨í„´ 7: ê·œì • + ì‹œí–‰ì„¸ì¹™ ì¡°í•©
            r'^([ê°€-í£]+(?:\s+[ê°€-í£]+)*(?:ì—\s*ê´€í•œ\s*)?ê·œì •\s+ì‹œí–‰ì„¸ì¹™)(?:\s|$)',
            
            # íŒ¨í„´ 8: ë¶„ë¥˜ (í•œêµ­í‘œì¤€ì‚°ì—…ë¶„ë¥˜ ë“±)
            r'^([ê°€-í£]+(?:\s+[ê°€-í£]+)*ë¶„ë¥˜)(?:\s|$)',
            
            # íŒ¨í„´ 9: ê³ ì‹œ, í›ˆë ¹, ì˜ˆê·œ
            r'^([ê°€-í£]+(?:\s+[ê°€-í£]+)*(?:ì—\s*ê´€í•œ\s*)?(?:ê³ ì‹œ|í›ˆë ¹|ì˜ˆê·œ|ì§€ì¹¨))(?:\s|$)',
        ]
        
        # AI ì„¤ì • í™•ì¸
        self.use_ai = st.session_state.get('use_ai', False)
        self.api_key = st.session_state.get('openai_api_key', None)
        
    def extract_from_pdf(self, file) -> List[str]:
        """PDF íŒŒì¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ - ChatGPT API í†µí•©"""
        all_text = ""
        
        try:
            # pdfplumberë¡œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            with pdfplumber.open(file) as pdf:
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        all_text += text + "\n"
        except:
            # ì‹¤íŒ¨ ì‹œ PyPDF2ë¡œ ì‹œë„
            try:
                file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text = page.extract_text()
                    all_text += text + "\n"
            except Exception as e:
                st.error(f"PDF ì½ê¸° ì˜¤ë¥˜: {str(e)}")
                return []
        
        # ê¸°ë³¸ ì¶”ì¶œ ë¡œì§
        laws = self._extract_laws_from_pdf_structure(all_text)
        
        # ChatGPT APIê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©
        if self.use_ai and self.api_key:
            with st.spinner("ğŸ¤– AIê°€ ë²•ë ¹ëª…ì„ ì •êµí™”í•˜ëŠ” ì¤‘..."):
                laws = self._enhance_with_chatgpt(all_text, laws)
        
        return sorted(list(laws))
    
    def _enhance_with_chatgpt(self, text: str, initial_laws: Set[str]) -> Set[str]:
        """ChatGPT APIë¥¼ í™œìš©í•œ ë²•ë ¹ëª… ì¶”ì¶œ ê°œì„ """
        try:
            import openai
            
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìƒˆë¡œìš´ ë°©ì‹)
            from openai import OpenAI
            client = OpenAI(api_key=self.api_key)
            
            # í…ìŠ¤íŠ¸ ìƒ˜í”Œë§ (í† í° ì œí•œì„ ìœ„í•´)
            sample_text = text[:3000]  # ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
            
            # í”„ë¡¬í”„íŠ¸ ê°œì„ 
            prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ë²•ë ¹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ë²•ë ¹ëª…ì„ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì¤‘ìš”í•œ ê·œì¹™:
1. "ìƒí•˜ìœ„ë²•", "í–‰ì •ê·œì¹™", "ê´€ë ¨ë²•ë ¹" ê°™ì€ ì¹´í…Œê³ ë¦¬ ì œëª©ì€ ì œì™¸
2. ë²•ë ¹ëª…ì€ ë²•ì œì²˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì •í™•í•œ ê³µì‹ ëª…ì¹­ìœ¼ë¡œ ì¶”ì¶œ
3. "ì—ê´€í•œ" â†’ "ì— ê´€í•œ"ìœ¼ë¡œ í†µì¼
4. ì‹œí–‰ë ¹, ì‹œí–‰ê·œì¹™ì€ ê¸°ë³¸ ë²•ë¥ ëª…ê³¼ í•¨ê»˜ ì™„ì „í•œ í˜•íƒœë¡œ í‘œê¸°
5. ì¤‘ë³µ ì œê±°í•˜ê³  ê³ ìœ í•œ ë²•ë ¹ëª…ë§Œ ì¶œë ¥
6. ì‹œí–‰ ë‚ ì§œ ì •ë³´([ì‹œí–‰ YYYY.MM.DD.])ëŠ” ì œì™¸

í…ìŠ¤íŠ¸:
{sample_text}

í˜„ì¬ ì¶”ì¶œëœ ë²•ë ¹ëª… (ì°¸ê³ ìš©):
{', '.join(list(initial_laws)[:20])}

ì •í™•í•œ ë²•ë ¹ëª…ì„ í•œ ì¤„ì— í•˜ë‚˜ì”© ì¶œë ¥í•˜ì„¸ìš”. ë²•ë ¹ëª…ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”:"""
            
            # API í˜¸ì¶œ (ìƒˆë¡œìš´ ë°©ì‹)
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system", 
                        "content": "ë‹¹ì‹ ì€ í•œêµ­ ë²•ë ¹ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë²•ì œì²˜ í˜•ì‹ì— ë§ëŠ” ì •í™•í•œ ë²•ë ¹ëª…ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1000
            )
            
            # ì‘ë‹µ íŒŒì‹±
            ai_laws = set()
            ai_response = response.choices[0].message.content.strip()
            
            for line in ai_response.split('\n'):
                line = line.strip()
                
                # ë²ˆí˜¸ë‚˜ ê¸°í˜¸ ì œê±°
                line = re.sub(r'^[\d\-\.\*\â€¢\Â·]+\s*', '', line)
                
                # ì¶”ê°€ ì •ì œ
                line = line.strip('"\'')
                
                if line and self._is_valid_law_name(line) and len(line) > 3:
                    ai_laws.add(line)
            
            # AIê°€ ì°¾ì€ ë²•ë ¹ ìˆ˜ í‘œì‹œ
            new_laws = ai_laws - initial_laws
            if new_laws:
                st.success(f"âœ¨ AIê°€ {len(new_laws)}ê°œì˜ ì¶”ê°€ ë²•ë ¹ëª…ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤")
                with st.expander("AIê°€ ì¶”ê°€ë¡œ ë°œê²¬í•œ ë²•ë ¹ëª…"):
                    for law in sorted(new_laws):
                        st.write(f"- {law}")
            
            # ê¸°ì¡´ ê²°ê³¼ì™€ AI ê²°ê³¼ ë³‘í•©
            combined_laws = initial_laws.union(ai_laws)
            
            # AIê°€ ì œê±°í•œ ë²•ë ¹ í‘œì‹œ (ì˜ëª» ì¶”ì¶œëœ ê²ƒë“¤)
            removed_laws = initial_laws - ai_laws
            if removed_laws and len(removed_laws) < len(initial_laws) * 0.3:  # 30% ì´í•˜ë§Œ ì œê±°
                with st.expander("AIê°€ ì œì™¸í•œ í•­ëª© (ì˜ëª» ì¶”ì¶œëœ ê²ƒ)"):
                    for law in sorted(removed_laws):
                        st.write(f"- {law}")
            
            return combined_laws
                
        except ImportError:
            st.warning("âš ï¸ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.info("í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”: `pip install openai`")
            return initial_laws
        except Exception as e:
            if "API key" in str(e):
                st.error("âŒ API í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                st.warning(f"âš ï¸ AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return initial_laws
    
    def _extract_laws_from_text(self, text: str) -> Set[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ - ê¸°ë³¸ ë¡œì§"""
        laws = set()
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text = self._preprocess_text(text)
        
        # ì¹´í…Œê³ ë¦¬ì™€ ë²•ë ¹ëª… ë¶„ë¦¬ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
        text = self._remove_categories(text)
        
        # ëª¨ë“  íŒ¨í„´ìœ¼ë¡œ ë²•ë ¹ëª… ì¶”ì¶œ
        for pattern in self.law_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                law_name = self._clean_law_name(match)
                if self._is_valid_law_name(law_name):
                    laws.add(law_name)
        
        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬
        laws.update(self._extract_compound_laws(text))
        laws.update(self._extract_attached_regulations(text))
        
        # ì¤‘ë³µ ë° ë¶€ë¶„ ë¬¸ìì—´ ì œê±°
        laws = self._remove_duplicates_and_substrings(laws)
        
        return laws
    
    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        # íŠ¹ì • íŒ¨í„´ ì‚¬ì´ì˜ ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'([ê°€-í£]+)\s*\n\s*([ê°€-í£]+(?:\s+ë°\s+)?[ê°€-í£]*(?:ì—\s*ê´€í•œ)?)', r'\1 \2', text)
        
        # "ë°" ì£¼ë³€ì˜ ê³µë°± ì •ê·œí™”
        text = re.sub(r'\s*ë°\s*', ' ë° ', text)
        
        return text
    
    def _remove_categories(self, text: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ë ˆì´ë¸” ì œê±°"""
        categories = ['ìƒí•˜ìœ„ë²•', 'í–‰ì •ê·œì¹™', 'ê´€ë ¨ë²•ë ¹', 'ë²•ë ¹']
        
        for category in categories:
            text = re.sub(rf'^\s*{category}\s*$', '', text, flags=re.MULTILINE)
            text = re.sub(rf'{category}\s+([ê°€-í£]+)', r'\1', text)
            text = re.sub(rf'([ê°€-í£]+)\s+{category}\s+([ê°€-í£]+)', r'\1 \2', text)
        
        return text
    
    def _extract_laws_from_pdf_structure(self, text: str) -> Set[str]:
        """PDF êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        laws = set()
        
        lines = text.split('\n')
        date_pattern = r'\[ì‹œí–‰\s*\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.\]'
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # ì‹œí–‰ ë‚ ì§œê°€ í¬í•¨ëœ ë¼ì¸ ì²˜ë¦¬
            if re.search(date_pattern, line):
                law_match = re.match(r'(.+?)\s*\[ì‹œí–‰', line)
                if law_match:
                    law_name = law_match.group(1).strip()
                    
                    # ì¹´í…Œê³ ë¦¬ ì œê±°
                    categories = ['ìƒí•˜ìœ„ë²•', 'í–‰ì •ê·œì¹™', 'ê´€ë ¨ë²•ë ¹', 'ë²•ë ¹']
                    for cat in categories:
                        law_name = law_name.replace(cat, '').strip()
                    
                    if self._is_valid_law_name(law_name):
                        laws.add(law_name)
            else:
                # ì‹œí–‰ ë‚ ì§œê°€ ì—†ëŠ” ê²½ìš°
                if line in ['ìƒí•˜ìœ„ë²•', 'í–‰ì •ê·œì¹™', 'ê´€ë ¨ë²•ë ¹', 'ë²•ë ¹']:
                    continue
                
                processed_line = self._preprocess_text(line)
                line_laws = self._extract_from_line(processed_line)
                laws.update(line_laws)
        
        return self._remove_duplicates_and_substrings(laws)
    
    def _extract_from_line(self, line: str) -> Set[str]:
        """í•œ ì¤„ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        laws = set()
        
        for pattern in self.law_patterns:
            matches = re.findall(pattern, line, re.IGNORECASE)
            for match in matches:
                law_name = self._clean_law_name(match)
                if self._is_valid_law_name(law_name):
                    laws.add(law_name)
        
        return laws
    
    def _extract_attached_regulations(self, text: str) -> Set[str]:
        """ë¶™ì–´ìˆëŠ” í˜•íƒœì˜ í–‰ì •ê·œì¹™ ì¶”ì¶œ"""
        attached_laws = set()
        
        special_patterns = [
            r'ê¸ˆìœµê¸°ê´€ê²€ì‚¬ë°ì œì¬ì—ê´€í•œê·œì •',
            r'ì—¬ì‹ ì „ë¬¸ê¸ˆìœµì—…ê°ë…ê·œì •',
            r'ì—¬ì‹ ì „ë¬¸ê¸ˆìœµì—…ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™',
            r'[ê°€-í£]+ê²€ì‚¬ë°[ê°€-í£]+ì—ê´€í•œê·œì •',
            r'[ê°€-í£]+ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™'
        ]
        
        for pattern in special_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                normalized = match
                normalized = re.sub(r'ê²€ì‚¬ë°', 'ê²€ì‚¬ ë° ', normalized)
                normalized = re.sub(r'ì—ê´€í•œ', 'ì— ê´€í•œ ', normalized)
                normalized = re.sub(r'ì—…ë¬´ì‹œí–‰', 'ì—…ë¬´ ì‹œí–‰', normalized)
                
                attached_laws.add(self._clean_law_name(normalized))
        
        return attached_laws
    
    def _clean_law_name(self, law_name: str) -> str:
        """ë²•ë ¹ëª… ì •ì œ"""
        if not isinstance(law_name, str):
            law_name = str(law_name)
        
        # ì‹œí–‰ ì •ë³´ ì œê±°
        law_name = re.sub(r'\s*\[ì‹œí–‰[^\]]+\]', '', law_name)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        law_name = law_name.strip()
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        law_name = ' '.join(law_name.split())
        
        # ë¶™ì–´ìˆëŠ” í˜•íƒœ ì •ê·œí™”
        law_name = re.sub(r'ê²€ì‚¬ë°', 'ê²€ì‚¬ ë° ', law_name)
        law_name = re.sub(r'ì—ê´€í•œ', 'ì— ê´€í•œ ', law_name)
        
        return law_name
    
    def _is_valid_law_name(self, law_name: str) -> bool:
        """ìœ íš¨í•œ ë²•ë ¹ëª…ì¸ì§€ ê²€ì¦"""
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
        if law_name in self.exclude_keywords:
            return False
        
        # ë„ˆë¬´ ì§§ì€ ê²ƒ ì œì™¸
        if len(law_name) < 3:
            return False
        
        # ìµœì†Œ 2ê¸€ì ì´ìƒì˜ í•œê¸€ì´ ìˆì–´ì•¼ í•¨
        korean_chars = re.findall(r'[ê°€-í£]+', law_name)
        if not korean_chars or max(len(k) for k in korean_chars) < 2:
            return False
        
        # ë²•ë ¹ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨
        law_keywords = ['ë²•', 'ë ¹', 'ê·œì¹™', 'ê·œì •', 'ê³ ì‹œ', 'í›ˆë ¹', 'ì˜ˆê·œ', 'ì§€ì¹¨', 'ì„¸ì¹™', 'ë¶„ë¥˜', 'ì—…ë¬´ê·œì •', 'ê°ë…ê·œì •']
        if not any(keyword in law_name for keyword in law_keywords):
            return False
        
        return True
    
    def _extract_compound_laws(self, text: str) -> Set[str]:
        """í•©ì„± ë²•ë ¹ëª… ì¶”ì¶œ"""
        compound_laws = set()
        
        base_law_pattern = r'([ê°€-í£]+(?:\s+[ê°€-í£]+)*ë²•(?:ë¥ )?)\s*(?:\[ì‹œí–‰[^\]]+\])?\s*\n'
        
        matches = re.finditer(base_law_pattern, text)
        for match in matches:
            base_law = self._clean_law_name(match.group(1))
            
            next_text = text[match.end():match.end() + 200]
            
            if f"{base_law} ì‹œí–‰ë ¹" in next_text or f"{base_law}ì‹œí–‰ë ¹" in next_text:
                compound_laws.add(f"{base_law} ì‹œí–‰ë ¹")
            
            if f"{base_law} ì‹œí–‰ê·œì¹™" in next_text or f"{base_law}ì‹œí–‰ê·œì¹™" in next_text:
                compound_laws.add(f"{base_law} ì‹œí–‰ê·œì¹™")
        
        return compound_laws
    
    def _remove_duplicates_and_substrings(self, laws: Set[str]) -> Set[str]:
        """ì¤‘ë³µ ë° ë¶€ë¶„ ë¬¸ìì—´ ì œê±°"""
        laws_list = sorted(list(laws), key=len, reverse=True)
        final_laws = []
        
        for law in laws_list:
            is_substring = False
            for existing_law in final_laws:
                if law in existing_law and law != existing_law:
                    is_substring = True
                    break
            
            if len(law) < 5 and law in ['ê·œì •', 'ì„¸ì¹™', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™']:
                continue
                
            if not is_substring:
                final_laws.append(law)
        
        return set(final_laws)
    
    def extract_from_excel(self, file) -> List[str]:
        """Excel íŒŒì¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        laws = set()
        
        try:
            excel_file = pd.ExcelFile(file)
            
            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(file, sheet_name=sheet_name)
                
                all_text = ""
                for column in df.columns:
                    for value in df[column].dropna():
                        if isinstance(value, str):
                            all_text += value + "\n"
                
                # ê¸°ë³¸ ì¶”ì¶œ
                sheet_laws = self._extract_laws_from_text(all_text)
                
                # AI ê°•í™” (ì„¤ì •ëœ ê²½ìš°)
                if self.use_ai and self.api_key and sheet_laws:
                    sheet_laws = self._enhance_with_chatgpt(all_text, sheet_laws)
                
                laws.update(sheet_laws)
                
        except Exception as e:
            st.error(f"Excel ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        
        return sorted(list(laws))
    
    def extract_from_markdown(self, file) -> List[str]:
        """Markdown íŒŒì¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        laws = set()
        
        try:
            content = file.read().decode('utf-8')
            laws = self._extract_laws_from_text(content)
            
            # AI ê°•í™”
            if self.use_ai and self.api_key and laws:
                laws = self._enhance_with_chatgpt(content, laws)
                
        except Exception as e:
            st.error(f"Markdown ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        
        return sorted(list(laws))
    
    def extract_from_text(self, file) -> List[str]:
        """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë²•ë ¹ëª… ì¶”ì¶œ"""
        laws = set()
        
        try:
            content = file.read().decode('utf-8')
            laws = self._extract_laws_from_text(content)
            
            # AI ê°•í™”
            if self.use_ai and self.api_key and laws:
                laws = self._enhance_with_chatgpt(content, laws)
                
        except Exception as e:
            st.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
        
        return sorted(list(laws))


class LawCollectorAPI:
    """ë²•ë ¹ ìˆ˜ì§‘ API í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.law_search_url = "http://www.law.go.kr/DRF/lawSearch.do"
        self.law_detail_url = "http://www.law.go.kr/DRF/lawService.do"
        self.delay = 0.5  # API í˜¸ì¶œ ê°„ê²©
        
    def search_law(self, oc_code: str, law_name: str) -> List[Dict[str, Any]]:
        """ë²•ë ¹ ê²€ìƒ‰ - ê°œì„ ëœ ë²„ì „"""
        params = {
            'OC': oc_code,
            'target': 'law',
            'type': 'XML',
            'query': law_name,
            'display': '100',
            'page': '1'
        }
        
        try:
            response = requests.get(
                self.law_search_url, 
                params=params, 
                timeout=10,
                verify=False
            )
            
            if response.status_code != 200:
                st.warning(f"API ì‘ë‹µ ì½”ë“œ: {response.status_code}")
                return []
            
            response.encoding = 'utf-8'
            content = response.text
            
            if content.startswith('\ufeff'):
                content = content[1:]
            
            try:
                if not content.strip().startswith('<?xml'):
                    content = '<?xml version="1.0" encoding="UTF-8"?>\n' + content
                
                content = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', content)
                
                root = ET.fromstring(content.encode('utf-8'))
            except ET.ParseError as e:
                st.error(f"XML íŒŒì‹± ì˜¤ë¥˜: {str(e)[:100]}")
                if '<html>' in content.lower():
                    st.error("APIê°€ HTMLì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ê¸°ê´€ì½”ë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return []
            
            laws = []
            
            for law_elem in root.findall('.//law'):
                law_id = law_elem.findtext('ë²•ë ¹ID', '')
                law_name_full = law_elem.findtext('ë²•ë ¹ëª…í•œê¸€', '')
                law_msn = law_elem.findtext('ë²•ë ¹ì¼ë ¨ë²ˆí˜¸', '')
                
                if law_id and law_name_full:
                    law_info = {
                        'law_id': law_id,
                        'law_msn': law_msn,
                        'law_name': law_name_full,
                        'law_type': law_elem.findtext('ë²•ì¢…êµ¬ë¶„', ''),
                        'promulgation_date': law_elem.findtext('ê³µí¬ì¼ì', ''),
                        'enforcement_date': law_elem.findtext('ì‹œí–‰ì¼ì', ''),
                    }
                    laws.append(law_info)
            
            return laws
            
        except requests.exceptions.Timeout:
            st.error("API ìš”ì²­ ì‹œê°„ ì´ˆê³¼ - ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")
            return []
        except requests.exceptions.ConnectionError:
            st.error("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜ - ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”")
            return []
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def get_law_detail_with_full_content(self, oc_code: str, law_id: str, law_msn: str, law_name: str) -> Optional[Dict[str, Any]]:
        """ë²•ë ¹ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ - ì¡°ë¬¸, ë¶€ì¹™, ë³„í‘œ ëª¨ë‘ í¬í•¨"""
        params = {
            'OC': oc_code,
            'target': 'law',
            'type': 'XML',
            'MST': law_msn,
            'mobileYn': 'N'
        }
        
        try:
            response = requests.get(
                self.law_detail_url,
                params=params,
                timeout=30,
                verify=False
            )
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                st.warning(f"{law_name} ìƒì„¸ ì •ë³´ ì ‘ê·¼ ì‹¤íŒ¨")
                return None
            
            content = response.text
            
            if content.startswith('\ufeff'):
                content = content[1:]
            
            try:
                root = ET.fromstring(content.encode('utf-8'))
            except ET.ParseError:
                st.warning(f"{law_name} XML íŒŒì‹± ì˜¤ë¥˜")
                return None
            
            law_detail = {
                'law_id': law_id,
                'law_msn': law_msn,
                'law_name': law_name,
                'law_type': '',
                'promulgation_date': '',
                'enforcement_date': '',
                'articles': [],
                'supplementary_provisions': [],
                'attachments': [],
                'raw_content': '',
            }
            
            basic_info = root.find('.//ê¸°ë³¸ì •ë³´')
            if basic_info is not None:
                law_detail['law_type'] = basic_info.findtext('ë²•ì¢…êµ¬ë¶„ëª…', '')
                law_detail['promulgation_date'] = basic_info.findtext('ê³µí¬ì¼ì', '')
                law_detail['enforcement_date'] = basic_info.findtext('ì‹œí–‰ì¼ì', '')
            
            self._extract_all_articles(root, law_detail)
            self._extract_supplementary_provisions(root, law_detail)
            self._extract_attachments(root, law_detail)
            
            if not law_detail['articles']:
                law_detail['raw_content'] = self._extract_full_text(root)
            
            return law_detail
            
        except Exception as e:
            st.warning(f"{law_name} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _extract_all_articles(self, root: ET.Element, law_detail: Dict[str, Any]) -> None:
        """ëª¨ë“  ì¡°ë¬¸ ì¶”ì¶œ"""
        articles_section = root.find('.//ì¡°ë¬¸')
        if articles_section is not None:
            for article_unit in articles_section.findall('.//ì¡°ë¬¸ë‹¨ìœ„'):
                article_info = self._parse_article_unit(article_unit)
                if article_info:
                    law_detail['articles'].append(article_info)
        
        if not law_detail['articles']:
            for article_content in root.findall('.//ì¡°ë¬¸ë‚´ìš©'):
                if article_content.text:
                    article_info = self._parse_article_text(article_content.text)
                    if article_info:
                        law_detail['articles'].append(article_info)
        
        if not law_detail['articles']:
            article_elements = []
            for elem in root.iter():
                if elem.tag in ['ì¡°', 'ì¡°ë¬¸', 'article', 'ì¡°ë¬¸ë‹¨ìœ„']:
                    article_elements.append(elem)
            
            for elem in article_elements:
                article_info = self._extract_article_from_element(elem)
                if article_info and article_info['content']:
                    law_detail['articles'].append(article_info)
    
    def _parse_article_unit(self, article_elem: ET.Element) -> Optional[Dict[str, Any]]:
        """ì¡°ë¬¸ë‹¨ìœ„ íŒŒì‹±"""
        article_info = {
            'number': '',
            'title': '',
            'content': '',
            'paragraphs': []
        }
        
        article_num = article_elem.findtext('ì¡°ë¬¸ë²ˆí˜¸', '')
        if article_num:
            article_info['number'] = f"ì œ{article_num}ì¡°"
        
        article_info['title'] = article_elem.findtext('ì¡°ë¬¸ì œëª©', '')
        
        article_content = article_elem.findtext('ì¡°ë¬¸ë‚´ìš©', '')
        if not article_content:
            article_content = self._get_all_text(article_elem)
        
        article_info['content'] = article_content
        
        for para in article_elem.findall('.//í•­'):
            para_info = {
                'number': para.findtext('í•­ë²ˆí˜¸', ''),
                'content': para.findtext('í•­ë‚´ìš©', '')
            }
            if para_info['content']:
                article_info['paragraphs'].append(para_info)
        
        return article_info if (article_info['number'] or article_info['content']) else None
    
    def _parse_article_text(self, text: str) -> Optional[Dict[str, Any]]:
        """ì¡°ë¬¸ í…ìŠ¤íŠ¸ íŒŒì‹±"""
        pattern = r'(ì œ\d+ì¡°(?:ì˜\d+)?)\s*(?:\((.*?)\))?\s*(.*?)(?=ì œ\d+ì¡°|$)'
        matches = re.findall(pattern, text, re.DOTALL)
        
        if not matches:
            return None
            
        match = matches[0]
        article_info = {
            'number': match[0],
            'title': match[1],
            'content': match[2].strip(),
            'paragraphs': []
        }
        
        para_pattern = r'([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]+)\s*(.*?)(?=[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]|$)'
        para_matches = re.findall(para_pattern, article_info['content'], re.DOTALL)
        
        for para_match in para_matches:
            article_info['paragraphs'].append({
                'number': para_match[0],
                'content': para_match[1].strip()
            })
        
        return article_info
    
    def _extract_article_from_element(self, elem: ET.Element) -> Dict[str, Any]:
        """ìš”ì†Œì—ì„œ ì¡°ë¬¸ ì •ë³´ ì¶”ì¶œ"""
        article_info = {
            'number': '',
            'title': '',
            'content': '',
            'paragraphs': []
        }
        
        for tag in ['ì¡°ë¬¸ë²ˆí˜¸', 'ì¡°ë²ˆí˜¸', 'ë²ˆí˜¸']:
            num = elem.findtext(tag, '')
            if num:
                article_info['number'] = f"ì œ{num}ì¡°" if not num.startswith('ì œ') else num
                break
        
        for tag in ['ì¡°ë¬¸ì œëª©', 'ì¡°ì œëª©', 'ì œëª©']:
            title = elem.findtext(tag, '')
            if title:
                article_info['title'] = title
                break
        
        for tag in ['ì¡°ë¬¸ë‚´ìš©', 'ì¡°ë‚´ìš©', 'ë‚´ìš©']:
            content = elem.findtext(tag, '')
            if content:
                article_info['content'] = content
                break
        
        if not article_info['content']:
            article_info['content'] = self._get_all_text(elem)
        
        return article_info
    
    def _extract_supplementary_provisions(self, root: ET.Element, law_detail: Dict[str, Any]) -> None:
        """ë¶€ì¹™ ì¶”ì¶œ"""
        for addendum in root.findall('.//ë¶€ì¹™'):
            addendum_info = {
                'number': addendum.findtext('ë¶€ì¹™ë²ˆí˜¸', ''),
                'promulgation_date': addendum.findtext('ë¶€ì¹™ê³µí¬ì¼ì', ''),
                'content': self._get_all_text(addendum)
            }
            if addendum_info['content']:
                law_detail['supplementary_provisions'].append(addendum_info)
        
        if not law_detail['supplementary_provisions']:
            for elem in root.iter():
                if elem.tag == 'ë¶€ì¹™ë‚´ìš©' and elem.text:
                    law_detail['supplementary_provisions'].append({
                        'number': '',
                        'promulgation_date': '',
                        'content': elem.text
                    })
    
    def _extract_attachments(self, root: ET.Element, law_detail: Dict[str, Any]) -> None:
        """ë³„í‘œ/ë³„ì²¨ ì¶”ì¶œ"""
        for table in root.findall('.//ë³„í‘œ'):
            table_info = {
                'type': 'ë³„í‘œ',
                'number': table.findtext('ë³„í‘œë²ˆí˜¸', ''),
                'title': table.findtext('ë³„í‘œì œëª©', ''),
                'content': self._get_all_text(table)
            }
            if table_info['content'] or table_info['title']:
                law_detail['attachments'].append(table_info)
        
        for form in root.findall('.//ë³„ì§€'):
            form_info = {
                'type': 'ë³„ì§€',
                'number': form.findtext('ë³„ì§€ë²ˆí˜¸', ''),
                'title': form.findtext('ë³„ì§€ì œëª©', ''),
                'content': self._get_all_text(form)
            }
            if form_info['content'] or form_info['title']:
                law_detail['attachments'].append(form_info)
        
        for format_elem in root.findall('.//ì„œì‹'):
            format_info = {
                'type': 'ì„œì‹',
                'number': format_elem.findtext('ì„œì‹ë²ˆí˜¸', ''),
                'title': format_elem.findtext('ì„œì‹ì œëª©', ''),
                'content': self._get_all_text(format_elem)
            }
            if format_info['content'] or format_info['title']:
                law_detail['attachments'].append(format_info)
    
    def _extract_full_text(self, root: ET.Element) -> str:
        """ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í´ë°±)"""
        return self._get_all_text(root)
    
    def _get_all_text(self, elem: ET.Element) -> str:
        """ìš”ì†Œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        texts = []
        
        if elem.text and elem.text.strip():
            texts.append(elem.text.strip())
        
        for child in elem:
            child_text = self._get_all_text(child)
            if child_text:
                texts.append(child_text)
            
            if child.tail and child.tail.strip():
                texts.append(child.tail.strip())
        
        return ' '.join(texts)
    
    def export_to_zip(self, laws_dict: Dict[str, Dict[str, Any]]) -> bytes:
        """ìˆ˜ì§‘ëœ ë²•ë ¹ì„ ZIPìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        zip_buffer = BytesIO()
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            all_data = {
                'collection_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_laws': len(laws_dict),
                'laws': laws_dict
            }
            
            zip_file.writestr(
                'all_laws.json',
                json.dumps(all_data, ensure_ascii=False, indent=2)
            )
            
            all_laws_md = self._create_all_laws_markdown(laws_dict)
            zip_file.writestr('all_laws.md', all_laws_md)
            
            for law_id, law in laws_dict.items():
                safe_name = re.sub(r'[\\/*?:"<>|]', '_', law['law_name'])
                
                zip_file.writestr(
                    f'laws/{safe_name}.json',
                    json.dumps(law, ensure_ascii=False, indent=2)
                )
                
                text_content = self._format_law_full_text(law)
                zip_file.writestr(
                    f'laws/{safe_name}.txt',
                    text_content
                )
                
                md_content = self._format_law_markdown(law)
                zip_file.writestr(
                    f'laws/{safe_name}.md',
                    md_content
                )
            
            readme = self._create_readme(laws_dict)
            zip_file.writestr('README.md', readme)
        
        zip_buffer.seek(0)
        return zip_buffer.getvalue()
    
    def _format_law_markdown(self, law: Dict[str, Any]) -> str:
        """ê°œë³„ ë²•ë ¹ì„ Markdownìœ¼ë¡œ í¬ë§·"""
        lines = []
        
        lines.append(f"# {law['law_name']}\n")
        
        lines.append("## ğŸ“‹ ê¸°ë³¸ ì •ë³´\n")
        lines.append(f"- **ë²•ì¢…êµ¬ë¶„**: {law.get('law_type', '')}")
        lines.append(f"- **ê³µí¬ì¼ì**: {law.get('promulgation_date', '')}")
        lines.append(f"- **ì‹œí–‰ì¼ì**: {law.get('enforcement_date', '')}")
        lines.append(f"- **ë²•ë ¹ID**: {law.get('law_id', '')}")
        lines.append("")
        
        if law.get('articles'):
            lines.append("## ğŸ“– ì¡°ë¬¸\n")
            for article in law['articles']:
                lines.append(f"### {article['number']}")
                if article.get('title'):
                    lines.append(f"**{article['title']}**\n")
                
                lines.append(article['content'])
                
                if article.get('paragraphs'):
                    for para in article['paragraphs']:
                        lines.append(f"\n> {para['number']} {para['content']}")
                
                lines.append("")
        
        if law.get('supplementary_provisions'):
            lines.append("\n## ğŸ“Œ ë¶€ì¹™\n")
            for idx, supp in enumerate(law['supplementary_provisions'], 1):
                if supp.get('promulgation_date'):
                    lines.append(f"### ë¶€ì¹™ <{supp['promulgation_date']}>")
                else:
                    lines.append(f"### ë¶€ì¹™ {idx}")
                lines.append(f"\n{supp['content']}\n")
        
        if law.get('attachments'):
            lines.append("\n## ğŸ“ ë³„í‘œ/ë³„ì²¨\n")
            for attach in law['attachments']:
                lines.append(f"### [{attach['type']}] {attach.get('title', '')}")
                lines.append(f"\n{attach['content']}\n")
        
        if not law.get('articles') and law.get('raw_content'):
            lines.append("\n## ğŸ“„ ì›ë¬¸\n")
            lines.append(law['raw_content'])
        
        return '\n'.join(lines)
    
    def _create_all_laws_markdown(self, laws_dict: Dict[str, Dict[str, Any]]) -> str:
        """ì „ì²´ ë²•ë ¹ì„ í•˜ë‚˜ì˜ Markdownìœ¼ë¡œ ìƒì„±"""
        lines = []
        
        lines.append("# ğŸ“š ë²•ë ¹ ìˆ˜ì§‘ ê²°ê³¼ (ì „ì²´)\n")
        lines.append(f"**ìˆ˜ì§‘ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"**ì´ ë²•ë ¹ ìˆ˜**: {len(laws_dict)}ê°œ\n")
        
        lines.append("## ğŸ“‘ ëª©ì°¨\n")
        for idx, (law_id, law) in enumerate(laws_dict.items(), 1):
            anchor = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', law['law_name'])
            lines.append(f"{idx}. [{law['law_name']}](#{anchor})")
        lines.append("")
        
        lines.append("---\n")
        
        for law_id, law in laws_dict.items():
            anchor = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', law['law_name'])
            lines.append(f'<div id="{anchor}"></div>\n')
            
            lines.append(self._format_law_markdown(law))
            lines.append("\n---\n")
        
        return '\n'.join(lines)
    
    def export_single_file(self, laws_dict: Dict[str, Dict[str, Any]], format: str = 'json') -> str:
        """ì„ íƒí•œ ë²•ë ¹ë“¤ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if format == 'json':
            data = {
                'collection_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_laws': len(laws_dict),
                'laws': laws_dict
            }
            return json.dumps(data, ensure_ascii=False, indent=2)
        
        elif format == 'markdown':
            return self._create_all_laws_markdown(laws_dict)
        
        elif format == 'text':
            lines = []
            lines.append(f"ë²•ë ¹ ìˆ˜ì§‘ ê²°ê³¼")
            lines.append(f"ìˆ˜ì§‘ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            lines.append(f"ì´ ë²•ë ¹ ìˆ˜: {len(laws_dict)}ê°œ")
            lines.append("="*80 + "\n")
            
            for law_id, law in laws_dict.items():
                lines.append(self._format_law_full_text(law))
                lines.append("\n" + "="*80 + "\n")
            
            return '\n'.join(lines)
    
    def _format_law_full_text(self, law: Dict[str, Any]) -> str:
        """ë²•ë ¹ ì „ì²´ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·"""
        lines = []
        
        lines.append(f"{'=' * 80}")
        lines.append(f"ë²•ë ¹ëª…: {law['law_name']}")
        lines.append(f"ë²•ì¢…êµ¬ë¶„: {law.get('law_type', '')}")
        lines.append(f"ê³µí¬ì¼ì: {law.get('promulgation_date', '')}")
        lines.append(f"ì‹œí–‰ì¼ì: {law.get('enforcement_date', '')}")
        lines.append(f"{'=' * 80}\n")
        
        if law.get('articles'):
            lines.append("ã€ ì¡° ë¬¸ ã€‘\n")
            for article in law['articles']:
                lines.append(f"\n{article['number']} {article.get('title', '')}")
                lines.append(article['content'])
                
                if article.get('paragraphs'):
                    for para in article['paragraphs']:
                        lines.append(f"\n  {para['number']} {para['content']}")
                
                lines.append("")
        
        if law.get('supplementary_provisions'):
            lines.append("\n\nã€ ë¶€ ì¹™ ã€‘\n")
            for idx, supp in enumerate(law['supplementary_provisions'], 1):
                if supp.get('promulgation_date'):
                    lines.append(f"\në¶€ì¹™ <{supp['promulgation_date']}>")
                else:
                    lines.append(f"\në¶€ì¹™ {idx}")
                lines.append(supp['content'])
        
        if law.get('attachments'):
            lines.append("\n\nã€ ë³„í‘œ/ë³„ì²¨ ã€‘\n")
            for attach in law['attachments']:
                lines.append(f"\n[{attach['type']}] {attach.get('title', '')}")
                lines.append(attach['content'])
                lines.append("")
        
        if not law.get('articles') and law.get('raw_content'):
            lines.append("\n\nã€ ì› ë¬¸ ã€‘\n")
            lines.append(law['raw_content'])
        
        return '\n'.join(lines)
    
    def _create_readme(self, laws_dict: Dict[str, Dict[str, Any]]) -> str:
        """README ìƒì„±"""
        content = f"""# ë²•ë ¹ ìˆ˜ì§‘ ê²°ê³¼

ìˆ˜ì§‘ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ì´ ë²•ë ¹ ìˆ˜: {len(laws_dict)}ê°œ

## ğŸ“ íŒŒì¼ êµ¬ì¡°

- `all_laws.json`: ì „ì²´ ë²•ë ¹ ë°ì´í„° (JSON)
- `all_laws.md`: ì „ì²´ ë²•ë ¹ í†µí•© ë¬¸ì„œ (Markdown)
- `laws/`: ê°œë³„ ë²•ë ¹ íŒŒì¼ ë””ë ‰í† ë¦¬
  - `*.json`: ë²•ë ¹ë³„ ìƒì„¸ ë°ì´í„°
  - `*.txt`: ë²•ë ¹ë³„ ì „ì²´ í…ìŠ¤íŠ¸
  - `*.md`: ë²•ë ¹ë³„ Markdown ë¬¸ì„œ
- `README.md`: ì´ íŒŒì¼

## ğŸ“Š ìˆ˜ì§‘ í†µê³„

"""
        total_articles = 0
        total_provisions = 0
        total_attachments = 0
        
        for law in laws_dict.values():
            total_articles += len(law.get('articles', []))
            total_provisions += len(law.get('supplementary_provisions', []))
            total_attachments += len(law.get('attachments', []))
        
        content += f"- ì´ ì¡°ë¬¸ ìˆ˜: {total_articles:,}ê°œ\n"
        content += f"- ì´ ë¶€ì¹™ ìˆ˜: {total_provisions}ê°œ\n"
        content += f"- ì´ ë³„í‘œ/ë³„ì²¨ ìˆ˜: {total_attachments}ê°œ\n"
        
        content += "\n## ğŸ“– ìˆ˜ì§‘ëœ ë²•ë ¹ ëª©ë¡\n\n"
        
        for law_id, law in laws_dict.items():
            article_count = len(law.get('articles', []))
            content += f"### {law['law_name']}\n"
            content += f"- ë²•ì¢…êµ¬ë¶„: {law.get('law_type', '')}\n"
            content += f"- ì‹œí–‰ì¼ì: {law.get('enforcement_date', '')}\n"
            content += f"- ì¡°ë¬¸: {article_count}ê°œ\n"
            content += f"- ë¶€ì¹™: {len(law.get('supplementary_provisions', []))}ê°œ\n"
            content += f"- ë³„í‘œ/ë³„ì²¨: {len(law.get('attachments', []))}ê°œ\n\n"
        
        return content


def generate_search_variations(law_name: str) -> List[str]:
    """ë²•ë ¹ëª…ì˜ ë‹¤ì–‘í•œ ë³€í˜• ìƒì„± - ê²€ìƒ‰ ì„±ê³µë¥  í–¥ìƒ"""
    variations = [law_name]
    
    # ë„ì–´ì“°ê¸° ì¶”ê°€ ë²„ì „
    spaced = law_name
    spaced = re.sub(r'([ê°€-í£]+)ë°([ê°€-í£]+)', r'\1 ë° \2', spaced)
    spaced = re.sub(r'([ê°€-í£]+)ì—ê´€í•œ([ê°€-í£]+)', r'\1ì— ê´€í•œ \2', spaced)
    spaced = re.sub(r'([ê°€-í£]+)ì—ê´€í•œ', r'\1ì— ê´€í•œ ', spaced)
    if spaced != law_name:
        variations.append(spaced)
    
    # ë„ì–´ì“°ê¸° ì œê±° ë²„ì „
    no_space = law_name.replace(' ', '')
    if no_space != law_name:
        variations.append(no_space)
    
    # "ì—ê´€í•œ" â†” "ì— ê´€í•œ" ë³€í™˜
    if 'ì—ê´€í•œ' in law_name:
        variations.append(law_name.replace('ì—ê´€í•œ', 'ì— ê´€í•œ'))
    if 'ì— ê´€í•œ' in law_name:
        variations.append(law_name.replace('ì— ê´€í•œ', 'ì—ê´€í•œ'))
    
    # ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ ë¶„ë¦¬
    if ' ì‹œí–‰ë ¹' in law_name:
        base = law_name.replace(' ì‹œí–‰ë ¹', '')
        variations.append(base)
        variations.append(f"{base}ì‹œí–‰ë ¹")
    
    if ' ì‹œí–‰ê·œì¹™' in law_name:
        base = law_name.replace(' ì‹œí–‰ê·œì¹™', '')
        variations.append(base)
        variations.append(f"{base}ì‹œí–‰ê·œì¹™")
    
    # ê´„í˜¸ ì œê±°
    if '(' in law_name or ')' in law_name:
        no_paren = re.sub(r'[()]', '', law_name).strip()
        variations.append(no_paren)
    
    # ì£¼ìš” í‚¤ì›Œë“œë§Œ
    words = law_name.split()
    if len(words) > 3:
        variations.append(' '.join(words[:2]))
        if words[-1] in ['ë²•', 'ë ¹', 'ê·œì¹™', 'ê·œì •', 'ì„¸ì¹™']:
            variations.append(' '.join(words[:-1]))
    
    return list(dict.fromkeys(variations))


def is_matching_law(query: str, result_name: str) -> bool:
    """ìœ ì—°í•œ ë²•ë ¹ëª… ë§¤ì¹­"""
    def normalize(text):
        text = re.sub(r'\s+', '', text)
        text = re.sub(r'[^\wê°€-í£]', '', text)
        return text.lower()
    
    query_norm = normalize(query)
    result_norm = normalize(result_name)
    
    # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ì™„ì „ ì¼ì¹˜
    if query_norm == result_norm:
        return True
    
    # í¬í•¨ ê´€ê³„
    if query_norm in result_norm or result_norm in query_norm:
        return True
    
    # ì£¼ìš” í‚¤ì›Œë“œ ë§¤ì¹­
    law_types = ['ë²•ë¥ ', 'ë²•', 'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™', 'ê·œì •', 'ê·œì¹™', 'ì„¸ì¹™', 'ê³ ì‹œ', 'í›ˆë ¹', 'ì˜ˆê·œ']
    
    query_type = None
    result_type = None
    
    for ltype in law_types:
        if ltype in query:
            query_type = ltype
        if ltype in result_name:
            result_type = ltype
    
    if query_type and result_type and query_type == result_type:
        query_core = query.replace(query_type, '').strip()
        result_core = result_name.replace(result_type, '').strip()
        
        if normalize(query_core) in normalize(result_core) or normalize(result_core) in normalize(query_core):
            return True
    
    # ê³µí†µ ë¬¸ì ë¹„ìœ¨
    common_chars = set(query_norm) & set(result_norm)
    if len(query_norm) > 0:
        similarity = len(common_chars) / len(set(query_norm))
        if similarity >= 0.7:
            return True
    
    return False


# ë©”ì¸ UI
def main():
    st.title("ğŸ“š ë²•ì œì²˜ ë²•ë ¹ ìˆ˜ì§‘ê¸°")
    st.markdown("ë²•ì œì²˜ Open APIë¥¼ í™œìš©í•œ ë²•ë ¹ ìˆ˜ì§‘ ë„êµ¬")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # ê¸°ê´€ì½”ë“œ ì…ë ¥
        oc_code = st.text_input(
            "ê¸°ê´€ì½”ë“œ (OC)",
            placeholder="ì´ë©”ì¼ @ ì•ë¶€ë¶„",
            help="ì˜ˆ: test@korea.kr â†’ test"
        )
        
        st.divider()
        
        # AI ì„¤ì • ì„¹ì…˜ ì¶”ê°€
        with st.expander("ğŸ¤– AI ì„¤ì • (ì„ íƒì‚¬í•­)", expanded=False):
            st.markdown("**ChatGPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë²•ë ¹ëª… ì¶”ì¶œ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤**")
            
            api_key = st.text_input(
                "OpenAI API Key",
                type="password",
                value=st.session_state.get('openai_api_key', ''),
                help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. https://platform.openai.com/api-keys ì—ì„œ ë°œê¸‰ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
            
            if api_key:
                st.session_state.openai_api_key = api_key
                st.session_state.use_ai = True
                st.success("âœ… API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # API í‚¤ í…ŒìŠ¤íŠ¸ ë²„íŠ¼
                if st.button("ğŸ” API í‚¤ í…ŒìŠ¤íŠ¸", type="secondary"):
                    try:
                        from openai import OpenAI
                        client = OpenAI(api_key=api_key)
                        
                        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
                        response = client.chat.completions.create(
                            model="gpt-3.5-turbo",
                            messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
                            max_tokens=10
                        )
                        st.success("âœ… API í‚¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
                    except ImportError:
                        st.error("âŒ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        st.code("pip install openai", language="bash")
                    except Exception as e:
                        st.error(f"âŒ API í‚¤ ì˜¤ë¥˜: {str(e)}")
            else:
                st.session_state.use_ai = False
                st.info("ğŸ’¡ API í‚¤ë¥¼ ì…ë ¥í•˜ë©´ ë” ì •í™•í•œ ë²•ë ¹ëª… ì¶”ì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        st.divider()
        
        # ëª¨ë“œ ì„ íƒ
        st.subheader("ğŸ¯ ìˆ˜ì§‘ ë°©ì‹")
        mode = st.radio(
            "ë°©ì‹ ì„ íƒ",
            ["ì§ì ‘ ê²€ìƒ‰", "íŒŒì¼ ì—…ë¡œë“œ"],
            help="ì§ì ‘ ê²€ìƒ‰: ë²•ë ¹ëª…ì„ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰\níŒŒì¼ ì—…ë¡œë“œ: PDF/Excel/MD íŒŒì¼ì—ì„œ ë²•ë ¹ ì¶”ì¶œ"
        )
        st.session_state.mode = 'direct' if mode == "ì§ì ‘ ê²€ìƒ‰" else 'file'
        
        # ì§ì ‘ ê²€ìƒ‰ ëª¨ë“œ
        if st.session_state.mode == 'direct':
            law_name = st.text_input(
                "ë²•ë ¹ëª…",
                placeholder="ì˜ˆ: ë¯¼ë²•, ìƒë²•, í˜•ë²•",
                help="ê²€ìƒ‰í•  ë²•ë ¹ëª…ì„ ì…ë ¥í•˜ì„¸ìš”"
            )
            
            search_btn = st.button("ğŸ” ê²€ìƒ‰", type="primary", use_container_width=True)
        
        # íŒŒì¼ ì—…ë¡œë“œ ëª¨ë“œ
        else:
            st.subheader("ğŸ“„ ë²•ë ¹ì²´ê³„ë„ íŒŒì¼ ì—…ë¡œë“œ")
            uploaded_file = st.file_uploader(
                "íŒŒì¼ ì„ íƒ",
                type=['pdf', 'xlsx', 'xls', 'md', 'txt'],
                help="PDF, Excel, Markdown, í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤"
            )
            
            if uploaded_file:
                st.success(f"âœ… {uploaded_file.name} ì—…ë¡œë“œë¨")
                file_type = uploaded_file.name.split('.')[-1].lower()
                st.info(f"íŒŒì¼ í˜•ì‹: {file_type.upper()}")
        
        # ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ”„ ì´ˆê¸°í™”", type="secondary", use_container_width=True):
            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (API í‚¤ëŠ” ìœ ì§€)
            keys_to_keep = ['mode', 'openai_api_key', 'use_ai']
            for key in list(st.session_state.keys()):
                if key not in keys_to_keep:
                    del st.session_state[key]
            st.rerun()
    
    # ë©”ì¸ ì»¨í…ì¸ 
    collector = LawCollectorAPI()
    
    # ì§ì ‘ ê²€ìƒ‰ ëª¨ë“œ
    if st.session_state.mode == 'direct':
        st.header("ğŸ” ì§ì ‘ ê²€ìƒ‰ ëª¨ë“œ")
        
        if 'search_btn' in locals() and search_btn:
            if not oc_code:
                st.error("ê¸°ê´€ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            elif not law_name:
                st.error("ë²•ë ¹ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            else:
                with st.spinner(f"'{law_name}' ê²€ìƒ‰ ì¤‘..."):
                    results = collector.search_law(oc_code, law_name)
                    
                    if results:
                        st.success(f"{len(results)}ê°œì˜ ë²•ë ¹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                        st.session_state.search_results = results
                    else:
                        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        st.session_state.search_results = []
        
        # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
        if st.session_state.search_results:
            display_search_results_and_collect(collector, oc_code)
    
    # íŒŒì¼ ì—…ë¡œë“œ ëª¨ë“œ
    else:
        st.header("ğŸ“„ íŒŒì¼ ì—…ë¡œë“œ ëª¨ë“œ")
        
        # AI ì„¤ì • ìƒíƒœ í‘œì‹œ
        if st.session_state.use_ai:
            st.info("ğŸ¤– AI ê°•í™” ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
        else:
            st.info("ğŸ’¡ AI ì„¤ì •ì„ í†µí•´ ë²•ë ¹ëª… ì¶”ì¶œ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        extractor = EnhancedLawFileExtractor()
        
        # íŒŒì¼ì—ì„œ ë²•ë ¹ ì¶”ì¶œ
        if uploaded_file and not st.session_state.file_processed:
            st.subheader("ğŸ“‹ STEP 1: ë²•ë ¹ëª… ì¶”ì¶œ")
            
            with st.spinner("íŒŒì¼ì—ì„œ ë²•ë ¹ëª…ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                file_type = uploaded_file.name.split('.')[-1].lower()
                
                # íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬
                if file_type == 'pdf':
                    extracted_laws = extractor.extract_from_pdf(uploaded_file)
                elif file_type in ['xlsx', 'xls']:
                    extracted_laws = extractor.extract_from_excel(uploaded_file)
                elif file_type == 'md':
                    extracted_laws = extractor.extract_from_markdown(uploaded_file)
                elif file_type == 'txt':
                    extracted_laws = extractor.extract_from_text(uploaded_file)
                else:
                    st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤")
                    extracted_laws = []
                
                if extracted_laws:
                    st.success(f"âœ… {len(extracted_laws)}ê°œì˜ ë²•ë ¹ëª…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                    st.session_state.extracted_laws = extracted_laws
                    st.session_state.file_processed = True
                else:
                    st.warning("íŒŒì¼ì—ì„œ ë²•ë ¹ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì¶”ì¶œëœ ë²•ë ¹ í‘œì‹œ ë° í¸ì§‘
        if st.session_state.extracted_laws:
            st.subheader("âœï¸ STEP 2: ë²•ë ¹ëª… í™•ì¸ ë° í¸ì§‘")
            st.info("ì¶”ì¶œëœ ë²•ë ¹ëª…ì„ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            # ì¶”ì¶œëœ ë²•ë ¹ ëª©ë¡ í‘œì‹œ
            st.write("**ì¶”ì¶œëœ ë²•ë ¹ëª…:**")
            for idx, law in enumerate(st.session_state.extracted_laws, 1):
                st.write(f"{idx}. {law}")
            
            # ë²•ë ¹ëª… í¸ì§‘ ì˜ì—­
            edited_laws = []
            
            st.write("\n**ë²•ë ¹ëª… í¸ì§‘:**")
            for idx, law_name in enumerate(st.session_state.extracted_laws):
                col1, col2 = st.columns([4, 1])
                with col1:
                    edited_name = st.text_input(
                        f"ë²•ë ¹ {idx+1}",
                        value=law_name,
                        key=f"edit_{idx}"
                    )
                    if edited_name:
                        edited_laws.append(edited_name)
                with col2:
                    if st.button("ì‚­ì œ", key=f"del_{idx}"):
                        st.session_state.extracted_laws.pop(idx)
                        st.rerun()
            
            # ë²•ë ¹ëª… ì¶”ê°€
            st.subheader("ë²•ë ¹ëª… ì¶”ê°€")
            new_law = st.text_input("ìƒˆ ë²•ë ¹ëª… ì…ë ¥", key="new_law_input")
            if st.button("â• ì¶”ê°€") and new_law:
                st.session_state.extracted_laws.append(new_law)
                st.rerun()
            
            # ë²•ë ¹ ê²€ìƒ‰ ë²„íŠ¼
            if st.button("ğŸ” ë²•ë ¹ ê²€ìƒ‰", type="primary", use_container_width=True):
                if not oc_code:
                    st.error("ê¸°ê´€ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
                else:
                    # ê²€ìƒ‰ ì‹œì‘
                    search_results = []
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # ìˆ˜ì •ëœ ë²•ë ¹ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                    if edited_laws:
                        st.session_state.extracted_laws = edited_laws
                    
                    total = len(st.session_state.extracted_laws)
                    no_result_laws = []
                    
                    for idx, law_name in enumerate(st.session_state.extracted_laws):
                        progress = (idx + 1) / total
                        progress_bar.progress(progress)
                        status_text.text(f"ê²€ìƒ‰ ì¤‘: {law_name}")
                        
                        # ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ê²€ìƒ‰ ì‹œë„
                        search_variations_list = generate_search_variations(law_name)
                        found = False
                        
                        for variation in search_variations_list:
                            results = collector.search_law(oc_code, variation)
                            
                            if results:
                                for result in results:
                                    if is_matching_law(law_name, result['law_name']):
                                        result['search_query'] = law_name
                                        search_results.append(result)
                                        found = True
                                        break
                                
                                if found:
                                    break
                        
                        if not found:
                            no_result_laws.append(law_name)
                        
                        time.sleep(collector.delay)
                    
                    progress_bar.progress(1.0)
                    status_text.text("ê²€ìƒ‰ ì™„ë£Œ!")
                    
                    # ê²°ê³¼ í‘œì‹œ
                    if search_results:
                        st.success(f"âœ… ì´ {len(search_results)}ê°œì˜ ë²•ë ¹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                        st.session_state.search_results = search_results
                    else:
                        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                    
                    # ê²€ìƒ‰ ì‹¤íŒ¨í•œ ë²•ë ¹ ëª©ë¡ í‘œì‹œ
                    if no_result_laws:
                        with st.expander(f"âŒ ê²€ìƒ‰ë˜ì§€ ì•Šì€ ë²•ë ¹ ({len(no_result_laws)}ê°œ)"):
                            for law in no_result_laws:
                                st.write(f"- {law}")
                            st.info("ğŸ’¡ Tip: ê¸°ê´€ì½”ë“œë¥¼ í™•ì¸í•˜ê±°ë‚˜, ë²•ë ¹ëª…ì„ ìˆ˜ì •í•´ë³´ì„¸ìš”.")
        
        # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
        if st.session_state.search_results:
            display_search_results_and_collect(collector, oc_code, is_file_mode=True)


def display_search_results_and_collect(collector: LawCollectorAPI, oc_code: str, is_file_mode: bool = False) -> None:
    """ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ ë° ìˆ˜ì§‘ - ê³µí†µ í•¨ìˆ˜"""
    st.subheader("ğŸ“‘ ê²€ìƒ‰ ê²°ê³¼")
    
    # ì „ì²´ ì„ íƒ
    select_all = st.checkbox("ì „ì²´ ì„ íƒ", key="select_all_results")
    
    # í…Œì´ë¸” í—¤ë”
    if is_file_mode:
        col1, col2, col3, col4, col5 = st.columns([1, 3, 2, 2, 2])
        with col5:
            st.markdown("**ê²€ìƒ‰ì–´**")
    else:
        col1, col2, col3, col4 = st.columns([1, 3, 2, 2])
    
    with col1:
        st.markdown("**ì„ íƒ**")
    with col2:
        st.markdown("**ë²•ë ¹ëª…**")
    with col3:
        st.markdown("**ë²•ì¢…êµ¬ë¶„**")
    with col4:
        st.markdown("**ì‹œí–‰ì¼ì**")
    
    st.divider()
    
    # ì„ íƒëœ ë²•ë ¹
    selected_indices = []
    
    for idx, law in enumerate(st.session_state.search_results):
        if is_file_mode:
            col1, col2, col3, col4, col5 = st.columns([1, 3, 2, 2, 2])
        else:
            col1, col2, col3, col4 = st.columns([1, 3, 2, 2])
        
        with col1:
            is_selected = st.checkbox(
                "ì„ íƒ", 
                key=f"sel_{idx}", 
                value=select_all,
                label_visibility="collapsed"
            )
            if is_selected:
                selected_indices.append(idx)
        
        with col2:
            st.write(law['law_name'])
        
        with col3:
            st.write(law.get('law_type', ''))
        
        with col4:
            st.write(law.get('enforcement_date', ''))
        
        if is_file_mode:
            with col5:
                st.write(law.get('search_query', ''))
    
    # ì„ íƒëœ ë²•ë ¹ ì €ì¥
    st.session_state.selected_laws = [
        st.session_state.search_results[i] for i in selected_indices
    ]
    
    if st.session_state.selected_laws:
        st.success(f"{len(st.session_state.selected_laws)}ê°œ ë²•ë ¹ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤")
        
        # ìˆ˜ì§‘ ë²„íŠ¼
        if st.button("ğŸ“¥ ì„ íƒí•œ ë²•ë ¹ ìˆ˜ì§‘", type="primary", use_container_width=True):
            collected_laws = {}
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            total = len(st.session_state.selected_laws)
            success_count = 0
            
            for idx, law in enumerate(st.session_state.selected_laws):
                progress = (idx + 1) / total
                progress_bar.progress(progress)
                status_text.text(f"ìˆ˜ì§‘ ì¤‘ ({idx + 1}/{total}): {law['law_name']}")
                
                # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                law_detail = collector.get_law_detail_with_full_content(
                    oc_code,
                    law['law_id'],
                    law.get('law_msn', ''),
                    law['law_name']
                )
                
                if law_detail:
                    collected_laws[law['law_id']] = law_detail
                    success_count += 1
                
                time.sleep(collector.delay)
            
            progress_bar.progress(1.0)
            status_text.text(f"ìˆ˜ì§‘ ì™„ë£Œ! (ì„±ê³µ: {success_count}/{total})")
            
            st.session_state.collected_laws = collected_laws
            
            # í†µê³„ í‘œì‹œ
            total_articles = sum(len(law.get('articles', [])) for law in collected_laws.values())
            total_provisions = sum(len(law.get('supplementary_provisions', [])) for law in collected_laws.values())
            total_attachments = sum(len(law.get('attachments', [])) for law in collected_laws.values())
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ì¡°ë¬¸", f"{total_articles:,}ê°œ")
            with col2:
                st.metric("ì´ ë¶€ì¹™", f"{total_provisions}ê°œ")
            with col3:
                st.metric("ì´ ë³„í‘œ/ë³„ì²¨", f"{total_attachments}ê°œ")
    
    # ë‹¤ìš´ë¡œë“œ ì„¹ì…˜
    if st.session_state.collected_laws:
        st.header("ğŸ’¾ ë‹¤ìš´ë¡œë“œ")
        
        # ë‹¤ìš´ë¡œë“œ ì˜µì…˜ ì„ íƒ
        st.subheader("ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì˜µì…˜")
        download_option = st.radio(
            "ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì„ íƒ",
            ["ê°œë³„ íŒŒì¼ (ZIP)", "í†µí•© íŒŒì¼ (ë‹¨ì¼)"],
            help="ê°œë³„ íŒŒì¼: ê° ë²•ë ¹ë³„ë¡œ íŒŒì¼ ìƒì„±\ní†µí•© íŒŒì¼: ëª¨ë“  ë²•ë ¹ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ"
        )
        
        if download_option == "ê°œë³„ íŒŒì¼ (ZIP)":
            col1, col2 = st.columns(2)
            
            with col1:
                # JSON ë‹¤ìš´ë¡œë“œ
                json_data = {
                    'collection_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'mode': st.session_state.mode,
                    'ai_enhanced': st.session_state.use_ai,
                    'laws': st.session_state.collected_laws
                }
                json_str = json.dumps(json_data, ensure_ascii=False, indent=2)
                
                st.download_button(
                    label="ğŸ“„ JSON ë‹¤ìš´ë¡œë“œ",
                    data=json_str,
                    file_name=f"laws_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json",
                    use_container_width=True
                )
            
            with col2:
                # ZIP ë‹¤ìš´ë¡œë“œ
                zip_data = collector.export_to_zip(st.session_state.collected_laws)
                
                st.download_button(
                    label="ğŸ“¦ ZIP ë‹¤ìš´ë¡œë“œ (JSON+TXT+MD)",
                    data=zip_data,
                    file_name=f"laws_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                    mime="application/zip",
                    use_container_width=True
                )
        
        else:  # í†µí•© íŒŒì¼
            file_format = st.selectbox(
                "íŒŒì¼ í˜•ì‹ ì„ íƒ",
                ["JSON", "Markdown", "Text"],
                help="ëª¨ë“  ë²•ë ¹ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ í†µí•©í•©ë‹ˆë‹¤"
            )
            
            if file_format == "JSON":
                content = collector.export_single_file(st.session_state.collected_laws, 'json')
                mime = "application/json"
                extension = "json"
            elif file_format == "Markdown":
                content = collector.export_single_file(st.session_state.collected_laws, 'markdown')
                mime = "text/markdown"
                extension = "md"
            else:  # Text
                content = collector.export_single_file(st.session_state.collected_laws, 'text')
                mime = "text/plain"
                extension = "txt"
            
            st.download_button(
                label=f"ğŸ’¾ {file_format} í†µí•© íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data=content,
                file_name=f"all_laws_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{extension}",
                mime=mime,
                use_container_width=True
            )
        
        # ìˆ˜ì§‘ ê²°ê³¼ ìƒì„¸
        with st.expander("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìƒì„¸"):
            for law_id, law in st.session_state.collected_laws.items():
                st.subheader(law['law_name'])
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.write(f"ì¡°ë¬¸: {len(law.get('articles', []))}ê°œ")
                with col2:
                    st.write(f"ë¶€ì¹™: {len(law.get('supplementary_provisions', []))}ê°œ")
                with col3:
                    st.write(f"ë³„í‘œ: {len(law.get('attachments', []))}ê°œ")
                
                # ìƒ˜í”Œ ì¡°ë¬¸ í‘œì‹œ
                if law.get('articles'):
                    st.write("**ìƒ˜í”Œ ì¡°ë¬¸:**")
                    sample = law['articles'][0]
                    st.text(f"{sample['number']} {sample.get('title', '')}")
                    content_preview = sample['content'][:200] + "..." if len(sample['content']) > 200 else sample['content']
                    st.text(content_preview)


if __name__ == "__main__":
    main()
